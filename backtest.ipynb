{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9a1c200c-dc1e-49ec-96bf-566a30e6191f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Reproducible backtest harness:\n",
    "- Loads outputs/live_dataset.csv (or synthesizes a small mock).\n",
    "- Uses time-based 70/30 split.\n",
    "- Evaluates current forecasters if available; otherwise uses a trivial baseline.\n",
    "- Exports outputs/backtest/backtest_report.md + saves calibration charts (reuse your functions).\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os, json, math\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3cac27b6-7e3b-4855-ac7e-02d064c357cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT = Path(os.getenv(\"OUT\", \"outputs\"))\n",
    "OUT.mkdir(parents=True, exist_ok=True)\n",
    "LIVE_CSV = Path(os.getenv(\"LIVE_CSV\", OUT / \"live_dataset.csv\"))\n",
    "BK_DIR = OUT / \"backtest\"\n",
    "BK_DIR.mkdir(parents=True, exist_ok=True)\n",
    "REPORT = BK_DIR / \"backtest_report.md\"\n",
    "SEED = int(os.getenv(\"SEED\", \"42\"))\n",
    "rng = np.random.default_rng(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db2f0167-e93a-44fc-9fdc-95d2c8a9c8bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_live():\n",
    "    if LIVE_CSV.exists():\n",
    "        return pd.read_csv(LIVE_CSV, parse_dates=[\"ts\"])\n",
    "    ts = pd.date_range(\"2025-01-01\", periods=400, freq=\"min\", tz=\"UTC\")\n",
    "    rows = []\n",
    "    for p in [\"USDC/USDT-uni\", \"DAI/USDC-curve\"]:\n",
    "        dev = rng.normal(0, 0.0008, len(ts))\n",
    "        dev[300:305] += 0.006  \n",
    "        rows += [{\"ts\": t, \"pool\": p, \"dev\": float(d), \"anom_fused\": 0.2, \"feeds_fresh\": True} for t, d in zip(ts, dev)]\n",
    "    df = pd.DataFrame(rows)\n",
    "    df.to_csv(LIVE_CSV, index=False)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5979e667-f845-4aa6-8fb2-00bc26a22a0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _time_split_idx(ts, frac=0.70):\n",
    "    ts = pd.to_datetime(ts, utc=True, errors=\"coerce\")\n",
    "    cut = ts.quantile(frac)\n",
    "    tr = (ts <= cut).to_numpy().nonzero()[0]\n",
    "    te = (ts >  cut).to_numpy().nonzero()[0]\n",
    "    return tr, te\n",
    "def _labels(df: pd.DataFrame, horizon=10, thr=0.005):\n",
    "    y = []\n",
    "    for p, g in df.sort_values([\"pool\",\"ts\"]).groupby(\"pool\"):\n",
    "        v = g[\"dev\"].abs().fillna(0).to_numpy()\n",
    "        lab = np.zeros_like(v, dtype=int)\n",
    "        for i in range(len(v)):\n",
    "            lab[i] = 1 if (v[i+1:i+1+horizon] >= thr).any() else 0\n",
    "        y += list(lab)\n",
    "    return np.array(y, dtype=int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3f4efb08-2422-4b81-9747-a01b893ba281",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _predict(df, use_cols, model_loader=None):\n",
    "    X = df[use_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0).to_numpy()\n",
    "    if model_loader:\n",
    "        clf, calib = model_loader()\n",
    "        if calib is not None:\n",
    "            return calib.predict_proba(X)[:,1]\n",
    "        if clf is not None and hasattr(clf, \"predict_proba\"):\n",
    "            return clf.predict_proba(X)[:,1]\n",
    "    z = (df[\"dev\"].fillna(0) / (df[\"dev\"].std() or 1)).clip(-5, 5)\n",
    "    return 1/(1+np.exp(-z))\n",
    "def _metric_ap(y_true, p):\n",
    "    try:\n",
    "        from sklearn.metrics import average_precision_score as AP\n",
    "        return float(AP(y_true, p))\n",
    "    except Exception:\n",
    "        thr = np.linspace(0, 1, 11)\n",
    "        prec = []\n",
    "        for t in thr:\n",
    "            pred = (p >= t).astype(int)\n",
    "            tp = ((pred==1)&(y_true==1)).sum()\n",
    "            fp = ((pred==1)&(y_true==0)).sum()\n",
    "            fn = ((pred==0)&(y_true==1)).sum()\n",
    "            prec.append(tp/(tp+fp+1e-9))\n",
    "        return float(np.mean(prec))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fcd67ae2-198e-4b5d-a083-2002eed486bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _metric_brier(y_true, p):\n",
    "    return float(np.mean((p - y_true)**2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e0737388-e0e6-46d4-82ce-04fd73dc217f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_11988\\2578782990.py:3: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  return pd.read_csv(LIVE_CSV, parse_dates=[\"ts\"])\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_11988\\108063518.py:2: UserWarning: Could not infer format, so each element will be parsed individually, falling back to `dateutil`. To ensure parsing is consistent and as-expected, please specify a format.\n",
      "  ts = pd.to_datetime(ts, utc=True, errors=\"coerce\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[backtest] wrote outputs\\backtest\\backtest_report.md\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n",
      "C:\\Anaconda3\\Lib\\site-packages\\sklearn\\metrics\\_ranking.py:1046: UserWarning: No positive class found in y_true, recall is set to one for all thresholds.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    df = _ensure_live()\n",
    "    use_cols = [c for c in [\n",
    "        \"dev\",\"dev_roll_std\",\"tvl_outflow_rate\",\"spot_twap_gap_bps\",\n",
    "        \"oracle_ratio\",\"anom_fused\",\"neighbor_max_dev\",\"neighbor_avg_anom\",\"corr_best\",\"lead_lag_best\"\n",
    "    ] if c in df.columns] or [\"dev\"]\n",
    "    y10 = _labels(df, horizon=10, thr=0.005)\n",
    "    y30 = _labels(df, horizon=30, thr=0.005)\n",
    "    tr, te = _time_split_idx(df[\"ts\"], 0.70)\n",
    "    try:\n",
    "        from sentinel_runtime import _load_forecaster, _load_forecaster_30m\n",
    "        p10 = _predict(df.iloc[te], use_cols, model_loader=_load_forecaster)\n",
    "        p30 = _predict(df.iloc[te], use_cols, model_loader=_load_forecaster_30m)\n",
    "    except Exception:\n",
    "        p10 = _predict(df.iloc[te], use_cols, model_loader=None)\n",
    "        p30 = _predict(df.iloc[te], use_cols, model_loader=None)\n",
    "    ap10 = _metric_ap(y10[te], p10)\n",
    "    ap30 = _metric_ap(y30[te], p30)\n",
    "    br10 = _metric_brier(y10[te], p10)\n",
    "    br30 = _metric_brier(y30[te], p30)\n",
    "    try:\n",
    "        from sentinel_runtime import save_all_calibration_artifacts\n",
    "        save_all_calibration_artifacts()\n",
    "    except Exception:\n",
    "        pass\n",
    "    md = []\n",
    "    md.append(\"# Backtest Report\\n\")\n",
    "    md.append(f\"- Generated: {datetime.now(timezone.utc).isoformat(timespec='seconds')}\\n\")\n",
    "    md.append(\"## Holdout Metrics (30%)\\n\")\n",
    "    md.append(f\"- **10m**  AP={ap10:.3f}, Brier={br10:.4f}\")\n",
    "    md.append(f\"- **30m**  AP={ap30:.3f}, Brier={br30:.4f}\")\n",
    "    md.append(\"\\nArtifacts:\\n\")\n",
    "    for a in [\"artifacts/calibration_10m.png\", \"artifacts/calibration_30m.png\"]:\n",
    "        if (OUT / a).exists():\n",
    "            md.append(f\"- {a}\")\n",
    "    REPORT.write_text(\"\\n\".join(md))\n",
    "    print(f\"[backtest] wrote {REPORT}\")\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1ee8221-03e5-4da3-9ce9-ce486f642bc0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
