{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6a9b4ac-ecea-4cd2-8c34-457ccb41adb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pip in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (25.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install -U pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a79953-20e9-4254-a3e1-db3670ab7055",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in c:\\anaconda3\\lib\\site-packages (2.3.1)\n",
      "Requirement already satisfied: numpy in c:\\anaconda3\\lib\\site-packages (2.1.3)\n",
      "Requirement already satisfied: scikit-learn in c:\\anaconda3\\lib\\site-packages (1.7.1)\n",
      "Requirement already satisfied: xgboost in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (3.0.4)\n",
      "Requirement already satisfied: lightgbm in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (4.6.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
      "Requirement already satisfied: scipy>=1.8.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn) (1.16.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn) (3.6.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas numpy scikit-learn xgboost lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6754031a-bb5e-4049-8ce5-2b5d64d5d052",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cpu\n",
      "Requirement already satisfied: torch in c:\\anaconda3\\lib\\site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in c:\\anaconda3\\lib\\site-packages (from torch) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\anaconda3\\lib\\site-packages (from torch) (4.14.1)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda3\\lib\\site-packages (from torch) (78.1.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\anaconda3\\lib\\site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\anaconda3\\lib\\site-packages (from torch) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda3\\lib\\site-packages (from torch) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\anaconda3\\lib\\site-packages (from torch) (2025.7.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\anaconda3\\lib\\site-packages (from jinja2->torch) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install torch --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8db5514a-c4db-472c-894e-b0467e3ad392",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: shap in c:\\anaconda3\\lib\\site-packages (0.48.0)\n",
      "Requirement already satisfied: web3 in c:\\anaconda3\\lib\\site-packages (7.13.0)\n",
      "Requirement already satisfied: requests in c:\\anaconda3\\lib\\site-packages (2.32.4)\n",
      "Requirement already satisfied: tenacity in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (9.1.2)\n",
      "Requirement already satisfied: sentence-transformers in c:\\anaconda3\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: duckdb in c:\\anaconda3\\lib\\site-packages (1.3.2)\n",
      "Requirement already satisfied: numpy in c:\\anaconda3\\lib\\site-packages (from shap) (2.1.3)\n",
      "Requirement already satisfied: scipy in c:\\anaconda3\\lib\\site-packages (from shap) (1.16.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\anaconda3\\lib\\site-packages (from shap) (1.7.1)\n",
      "Requirement already satisfied: pandas in c:\\anaconda3\\lib\\site-packages (from shap) (2.3.1)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\anaconda3\\lib\\site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\anaconda3\\lib\\site-packages (from shap) (25.0)\n",
      "Requirement already satisfied: slicer==0.0.8 in c:\\anaconda3\\lib\\site-packages (from shap) (0.0.8)\n",
      "Requirement already satisfied: numba>=0.54 in c:\\anaconda3\\lib\\site-packages (from shap) (0.61.0)\n",
      "Requirement already satisfied: cloudpickle in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from shap) (3.1.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\anaconda3\\lib\\site-packages (from shap) (4.14.1)\n",
      "Requirement already satisfied: eth-abi>=5.0.1 in c:\\anaconda3\\lib\\site-packages (from web3) (5.2.0)\n",
      "Requirement already satisfied: eth-account>=0.13.6 in c:\\anaconda3\\lib\\site-packages (from web3) (0.13.7)\n",
      "Requirement already satisfied: eth-hash>=0.5.1 in c:\\anaconda3\\lib\\site-packages (from eth-hash[pycryptodome]>=0.5.1->web3) (0.7.1)\n",
      "Requirement already satisfied: eth-typing>=5.0.0 in c:\\anaconda3\\lib\\site-packages (from web3) (5.2.1)\n",
      "Requirement already satisfied: eth-utils>=5.0.0 in c:\\anaconda3\\lib\\site-packages (from web3) (5.3.0)\n",
      "Requirement already satisfied: hexbytes>=1.2.0 in c:\\anaconda3\\lib\\site-packages (from web3) (1.3.1)\n",
      "Requirement already satisfied: aiohttp>=3.7.4.post0 in c:\\anaconda3\\lib\\site-packages (from web3) (3.11.10)\n",
      "Requirement already satisfied: pydantic>=2.4.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from web3) (2.11.7)\n",
      "Requirement already satisfied: pywin32>=223 in c:\\anaconda3\\lib\\site-packages (from web3) (308)\n",
      "Requirement already satisfied: types-requests>=2.0.0 in c:\\anaconda3\\lib\\site-packages (from web3) (2.32.4.20250809)\n",
      "Requirement already satisfied: websockets<16.0.0,>=10.0.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from web3) (15.0.1)\n",
      "Requirement already satisfied: pyunormalize>=15.0.0 in c:\\anaconda3\\lib\\site-packages (from web3) (16.0.0)\n",
      "Requirement already satisfied: transformers<5.0.0,>=4.41.0 in c:\\anaconda3\\lib\\site-packages (from sentence-transformers) (4.55.4)\n",
      "Requirement already satisfied: torch>=1.11.0 in c:\\anaconda3\\lib\\site-packages (from sentence-transformers) (2.5.1)\n",
      "Requirement already satisfied: huggingface-hub>=0.20.0 in c:\\anaconda3\\lib\\site-packages (from sentence-transformers) (0.34.4)\n",
      "Requirement already satisfied: Pillow in c:\\anaconda3\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: filelock in c:\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (3.19.1)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (2025.7.34)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\anaconda3\\lib\\site-packages (from transformers<5.0.0,>=4.41.0->sentence-transformers) (0.6.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\anaconda3\\lib\\site-packages (from huggingface-hub>=0.20.0->sentence-transformers) (2025.7.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in c:\\anaconda3\\lib\\site-packages (from aiohttp>=3.7.4.post0->web3) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\anaconda3\\lib\\site-packages (from aiohttp>=3.7.4.post0->web3) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\anaconda3\\lib\\site-packages (from aiohttp>=3.7.4.post0->web3) (24.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\anaconda3\\lib\\site-packages (from aiohttp>=3.7.4.post0->web3) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\anaconda3\\lib\\site-packages (from aiohttp>=3.7.4.post0->web3) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in c:\\anaconda3\\lib\\site-packages (from aiohttp>=3.7.4.post0->web3) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in c:\\anaconda3\\lib\\site-packages (from aiohttp>=3.7.4.post0->web3) (1.18.0)\n",
      "Requirement already satisfied: idna>=2.0 in c:\\anaconda3\\lib\\site-packages (from yarl<2.0,>=1.17.0->aiohttp>=3.7.4.post0->web3) (3.10)\n",
      "Requirement already satisfied: parsimonious<0.11.0,>=0.10.0 in c:\\anaconda3\\lib\\site-packages (from eth-abi>=5.0.1->web3) (0.10.0)\n",
      "Requirement already satisfied: bitarray>=2.4.0 in c:\\anaconda3\\lib\\site-packages (from eth-account>=0.13.6->web3) (3.6.1)\n",
      "Requirement already satisfied: eth-keyfile<0.9.0,>=0.7.0 in c:\\anaconda3\\lib\\site-packages (from eth-account>=0.13.6->web3) (0.8.1)\n",
      "Requirement already satisfied: eth-keys>=0.4.0 in c:\\anaconda3\\lib\\site-packages (from eth-account>=0.13.6->web3) (0.7.0)\n",
      "Requirement already satisfied: eth-rlp>=2.1.0 in c:\\anaconda3\\lib\\site-packages (from eth-account>=0.13.6->web3) (2.2.0)\n",
      "Requirement already satisfied: rlp>=1.0.0 in c:\\anaconda3\\lib\\site-packages (from eth-account>=0.13.6->web3) (4.1.0)\n",
      "Requirement already satisfied: ckzg>=2.0.0 in c:\\anaconda3\\lib\\site-packages (from eth-account>=0.13.6->web3) (2.1.1)\n",
      "Requirement already satisfied: pycryptodome<4,>=3.6.6 in c:\\anaconda3\\lib\\site-packages (from eth-keyfile<0.9.0,>=0.7.0->eth-account>=0.13.6->web3) (3.23.0)\n",
      "Requirement already satisfied: cytoolz>=0.10.1 in c:\\anaconda3\\lib\\site-packages (from eth-utils>=5.0.0->web3) (1.0.1)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from pydantic>=2.4.0->web3) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from pydantic>=2.4.0->web3) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from pydantic>=2.4.0->web3) (0.4.1)\n",
      "Requirement already satisfied: toolz>=0.8.0 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from cytoolz>=0.10.1->eth-utils>=5.0.0->web3) (1.0.0)\n",
      "Requirement already satisfied: llvmlite<0.45,>=0.44.0dev0 in c:\\anaconda3\\lib\\site-packages (from numba>=0.54->shap) (0.44.0)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (78.1.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda3\\lib\\site-packages (from torch>=1.11.0->sentence-transformers) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=1.11.0->sentence-transformers) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\anaconda3\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: urllib3>=2 in c:\\anaconda3\\lib\\site-packages (from types-requests>=2.0.0->web3) (2.5.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\anaconda3\\lib\\site-packages (from jinja2->torch>=1.11.0->sentence-transformers) (3.0.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\anaconda3\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\anaconda3\\lib\\site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\anaconda3\\lib\\site-packages (from pandas->shap) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn->shap) (3.6.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install shap web3 requests tenacity sentence-transformers duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92ebcf55-70c1-4f6e-8bcb-93bb14112db9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in c:\\anaconda3\\lib\\site-packages (3.10.5)\n",
      "Requirement already satisfied: seaborn in c:\\anaconda3\\lib\\site-packages (0.13.2)\n",
      "Requirement already satisfied: plotly in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (6.2.0)\n",
      "Requirement already satisfied: weasyprint in c:\\anaconda3\\lib\\site-packages (66.0)\n",
      "Requirement already satisfied: fpdf2 in c:\\anaconda3\\lib\\site-packages (2.8.3)\n",
      "Requirement already satisfied: pyarrow in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (21.0.0)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\anaconda3\\lib\\site-packages (from matplotlib) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\anaconda3\\lib\\site-packages (from matplotlib) (4.55.3)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in c:\\anaconda3\\lib\\site-packages (from matplotlib) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\anaconda3\\lib\\site-packages (from matplotlib) (25.0)\n",
      "Requirement already satisfied: pillow>=8 in c:\\anaconda3\\lib\\site-packages (from matplotlib) (11.3.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\anaconda3\\lib\\site-packages (from matplotlib) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\anaconda3\\lib\\site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: pandas>=1.2 in c:\\anaconda3\\lib\\site-packages (from seaborn) (2.3.1)\n",
      "Requirement already satisfied: narwhals>=1.15.1 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from plotly) (2.1.0)\n",
      "Requirement already satisfied: pydyf>=0.11.0 in c:\\anaconda3\\lib\\site-packages (from weasyprint) (0.11.0)\n",
      "Requirement already satisfied: cffi>=0.6 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from weasyprint) (1.17.1)\n",
      "Requirement already satisfied: tinyhtml5>=2.0.0b1 in c:\\anaconda3\\lib\\site-packages (from weasyprint) (2.0.0)\n",
      "Requirement already satisfied: tinycss2>=1.4.0 in c:\\anaconda3\\lib\\site-packages (from weasyprint) (1.4.0)\n",
      "Requirement already satisfied: cssselect2>=0.8.0 in c:\\anaconda3\\lib\\site-packages (from weasyprint) (0.8.0)\n",
      "Requirement already satisfied: Pyphen>=0.9.1 in c:\\anaconda3\\lib\\site-packages (from weasyprint) (0.17.2)\n",
      "Requirement already satisfied: defusedxml in c:\\anaconda3\\lib\\site-packages (from fpdf2) (0.7.1)\n",
      "Requirement already satisfied: pycparser in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from cffi>=0.6->weasyprint) (2.22)\n",
      "Requirement already satisfied: webencodings in c:\\anaconda3\\lib\\site-packages (from cssselect2>=0.8.0->weasyprint) (0.5.1)\n",
      "Requirement already satisfied: zopfli>=0.1.4 in c:\\anaconda3\\lib\\site-packages (from fonttools[woff]>=4.0.0->weasyprint) (0.2.3.post1)\n",
      "Requirement already satisfied: brotli>=1.0.1 in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from fonttools[woff]>=4.0.0->weasyprint) (1.1.0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\anaconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\anaconda3\\lib\\site-packages (from pandas>=1.2->seaborn) (2025.2)\n",
      "Requirement already satisfied: six>=1.5 in c:\\anaconda3\\lib\\site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib seaborn plotly weasyprint fpdf2 pyarrow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cfc83b81-59b0-4d16-a491-216b54024d19",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\anaconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\anaconda3\\lib\\site-packages (from faiss-cpu) (2.1.3)\n",
      "Requirement already satisfied: packaging in c:\\anaconda3\\lib\\site-packages (from faiss-cpu) (25.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "dd7c4028-6d04-4f45-ad0e-0decc9348df8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, sys, json, math, time, hashlib, unicodedata, warnings, random, re\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Tuple, Sequence, Dict\n",
    "from datetime import datetime, timezone\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f1343500-eedf-4ceb-9086-d88414284aea",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "SEED = 42\n",
    "np.random.seed(SEED)\n",
    "random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c4d4f60a-2404-478e-b6f0-c8926f09a992",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import xgboost as xgb\n",
    "    _XGB_OK = True\n",
    "except Exception as e:\n",
    "    _XGB_OK = False\n",
    "    print(f\"[warn] xgboost unavailable: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bbfc97e1-8e7d-466c-90fe-e6bf09ea3f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import torch\n",
    "    import torch.nn as nn\n",
    "    from torch.utils.data import DataLoader, TensorDataset\n",
    "    _TORCH_OK = True\n",
    "    torch.manual_seed(SEED)\n",
    "except Exception as e:\n",
    "    _TORCH_OK = False\n",
    "    print(f\"[note] torch unavailable: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c79148ce-276d-42a5-9aa7-6c5bada6bdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from web3 import Web3, __version__ as _w3v\n",
    "    _WEB3_OK = True\n",
    "except Exception as e:\n",
    "    _WEB3_OK = False\n",
    "    print(f\"[note] web3 unavailable: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c4eb3938-e685-4a9b-ac05-c85d2c0815ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from fpdf import FPDF, HTMLMixin\n",
    "    _FPDF_OK = True\n",
    "except Exception as e:\n",
    "    _FPDF_OK = False\n",
    "    print(f\"[warn] fpdf2 unavailable: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6099ed1a-fb1b-4354-9571-39a5f8b95dfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from sklearn.ensemble import IsolationForest\n",
    "    from sklearn.neighbors import LocalOutlierFactor\n",
    "    from sklearn.svm import OneClassSVM\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    from sklearn.calibration import CalibratedClassifierCV\n",
    "    from sklearn.metrics import average_precision_score, brier_score_loss\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    from joblib import dump, load\n",
    "    _SK_OK = True\n",
    "except Exception as e:\n",
    "    _SK_OK = False\n",
    "    print(f\"[warn] scikit-learn unavailable: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "95086f82-323e-4d16-8232-54826699f1f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUT_BASE = Path(r\"C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\")\n",
    "OUTPUT_BASE.mkdir(parents=True, exist_ok=True)\n",
    "OUT = OUTPUT_BASE\n",
    "OUT_MODEL = OUT / \"model\"\n",
    "OUT_ARCHIVE = OUT / \"archive\"\n",
    "OUT_MODEL.mkdir(exist_ok=True, parents=True)\n",
    "OUT_ARCHIVE.mkdir(exist_ok=True, parents=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1f4d5392-33fb-4502-aec9-b60dc1c80a23",
   "metadata": {},
   "outputs": [],
   "source": [
    "LIVE_CSV = OUT / \"live_dataset.csv\"\n",
    "FORECAST_10M_PATH = OUT_MODEL / \"forecast_10m_xgb.joblib\"\n",
    "CALIB_10M_PATH    = OUT_MODEL / \"forecast_10m_calib.joblib\"\n",
    "FORECAST_10M_PARQUET = OUT / \"forecast_10m.parquet\"\n",
    "EXPLAIN_JSON = OUT / \"explain.json\"\n",
    "EVENTS_JSON  = OUT / \"events.json\"\n",
    "STALE_SEC = 20 * 60"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6f8b66e6-183f-42ec-9589-d47b4ef88060",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config:\n",
    "    eth_rpc: str = os.getenv(\"ETH_RPC\", \"https://cloudflare-eth.com\")\n",
    "    chain_id: int = 1\n",
    "    mock_mode: bool = True  \n",
    "    lookback: int = 20\n",
    "    outdir: str = str(OUT)\n",
    "    chainlink_feeds = {\n",
    "        \"USDC/USD\": \"0x8fFfFfd4AfB6115b954Bd326cbe7B4BA576818f6\",\n",
    "        \"USDT/USD\": \"0x3E7d1eAB13ad0104d2750B8863b489D65364e32D\",\n",
    "        \"DAI/USD\":  \"0xAed0c38402a5d19df6E4c03F4E2DceD6e29c1ee9\",\n",
    "    }\n",
    "    pools = {\n",
    "        \"USDC/USDT_univ3\": {\"address\": \"0x3416cf6c708da44db2624d63ea0aaef7113527c6\", \"type\": \"uniswap_v3\", \"symbol\": \"USDC/USDT\"},\n",
    "        \"DAI/USDC_univ3\":  {\"address\": \"0x6c6bc977e13df9b0de53b251522280bb72383700\",  \"type\": \"uniswap_v3\", \"symbol\": \"DAI/USDC\"},\n",
    "        \"3pool_curve\":     {\"address\": \"0xbebc44782c7db0a1a60cb6fe97d0b483032ff1c7\",  \"type\": \"curve\",      \"symbol\": \"USDT/USD\"},\n",
    "    }\n",
    "    models = {\"emb\": \"sentence-transformers/all-MiniLM-L6-v2\"}\n",
    "CFG = Config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b7c11406-137f-468d-a180-b608f2d1df96",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "def _iso_str(s) -> str:\n",
    "    ts = pd.to_datetime(s, errors=\"coerce\", utc=True)\n",
    "    if isinstance(ts, pd.Series):\n",
    "        return ts.dt.strftime(\"%Y-%m-%d %H:%M:%S%z\").fillna(\"\")\n",
    "    return ts.strftime(\"%Y-%m-%d %H:%M:%S%z\") if not pd.isna(ts) else \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "19910d6f-9328-48dc-8eaa-74ac787bc937",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rpc_health_check(rpc: str) -> bool:\n",
    "    if not (_WEB3_OK and rpc):\n",
    "        return False\n",
    "    try:\n",
    "        w3 = Web3(Web3.HTTPProvider(rpc, request_kwargs={\"timeout\": 6}))\n",
    "        ok = w3.is_connected() if hasattr(w3, \"is_connected\") else w3.isConnected()\n",
    "        return bool(ok)\n",
    "    except Exception:\n",
    "        return False\n",
    "if os.getenv(\"ETH_RPC\") and \"<YOUR_KEY>\" not in os.getenv(\"ETH_RPC\") and rpc_health_check(os.getenv(\"ETH_RPC\")):\n",
    "    CFG.eth_rpc = os.getenv(\"ETH_RPC\")\n",
    "    CFG.mock_mode = False\n",
    "else:\n",
    "    CFG.mock_mode = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47655167-7795-4494-82fb-d7a196859059",
   "metadata": {},
   "outputs": [],
   "source": [
    "_FONT_CANDIDATES = [\n",
    "    r\"C:\\Windows\\Fonts\\DejaVuSans.ttf\",\n",
    "    r\"C:\\Windows\\Fonts\\DejaVuSansCondensed.ttf\",\n",
    "    r\"C:\\Windows\\Fonts\\seguiemj.ttf\",\n",
    "    r\"C:\\Windows\\Fonts\\seguisym.ttf\",\n",
    "    r\"C:\\Windows\\Fonts\\arialuni.ttf\",\n",
    "    r\"C:\\Windows\\Fonts\\SegoeUI.ttf\",\n",
    "    \"/usr/share/fonts/truetype/dejavu/DejaVuSans.ttf\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "e7c0e6d1-75aa-4f07-a679-49e9453f30e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _find_unicode_font() -> Optional[str]:\n",
    "    for p in _FONT_CANDIDATES:\n",
    "        if os.path.exists(p):\n",
    "            return p\n",
    "    return None\n",
    "UNICODE_FONT_PATH = _find_unicode_font()\n",
    "UNICODE_FAMILY = \"DepegUnicode\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "490ccb0f-c63d-4ab2-9a3f-4a29a2282f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sanitize_text(s: str) -> str:\n",
    "    if s is None:\n",
    "        return \"\"\n",
    "    repl = {\"—\": \"-\", \"–\": \"-\", \"…\": \"...\", \"•\": \"-\", \"“\": '\"', \"”\": '\"', \"’\": \"'\", \"‘\": \"'\",\n",
    "            \"→\": \"->\", \"↑\": \"^\", \"↓\": \"v\", \"±\": \"+/-\", \"×\": \"x\", \"·\": \".\", \"™\": \"(TM)\", \"®\": \"(R)\", \"©\": \"(C)\"}\n",
    "    for k, v in repl.items(): s = s.replace(k, v)\n",
    "    return unicodedata.normalize(\"NFKD\", s).encode(\"ascii\", \"ignore\").decode(\"ascii\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a7a26263-79e1-4153-b303-988e6c2e6d05",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _FPDF_OK:\n",
    "    class PDF(FPDF, HTMLMixin): pass\n",
    "    def _set_font_unicode(pdf: FPDF, size: int = 10, style: str = \"\"):\n",
    "        if UNICODE_FONT_PATH:\n",
    "            try:\n",
    "                if UNICODE_FAMILY not in pdf.fonts:\n",
    "                    pdf.add_font(UNICODE_FAMILY, \"\", UNICODE_FONT_PATH, uni=True)\n",
    "                    pdf.add_font(UNICODE_FAMILY, \"B\", UNICODE_FONT_PATH, uni=True)\n",
    "                pdf.set_font(UNICODE_FAMILY, style=style, size=size); return\n",
    "            except Exception as e:\n",
    "                print(f\"[warn] Unicode font failed: {e}\")\n",
    "        pdf.set_font(\"Helvetica\", style=style, size=size)\n",
    "    def _init_pdf(title: str = \"Depeg Sentinel Report\") -> FPDF:\n",
    "        pdf = PDF(); pdf.set_auto_page_break(auto=True, margin=15); pdf.add_page()\n",
    "        _set_font_unicode(pdf, size=16, style=\"B\")\n",
    "        pdf.cell(0, 10, title if UNICODE_FONT_PATH else _sanitize_text(title), ln=1)\n",
    "        _set_font_unicode(pdf, size=9); pdf.cell(0, 6, f\"Generated: {_now_iso()}\", ln=1); pdf.ln(2)\n",
    "        return pdf\n",
    "    def _h(pdf: FPDF, text: str, level: int = 2):\n",
    "        sizes = {1: 14, 2: 12, 3: 11}\n",
    "        _set_font_unicode(pdf, size=sizes.get(level, 11), style=\"B\")\n",
    "        pdf.multi_cell(0, 6, text if UNICODE_FONT_PATH else _sanitize_text(text)); pdf.ln(1)\n",
    "    def _p(pdf: FPDF, text: str):\n",
    "        _set_font_unicode(pdf, size=10, style=\"\")\n",
    "        pdf.multi_cell(0, 5, text if UNICODE_FONT_PATH else _sanitize_text(text)); pdf.ln(1)\n",
    "    def _kv_table(pdf: FPDF, rows: List[Tuple[str, str]]):\n",
    "        _set_font_unicode(pdf, size=10, style=\"\")\n",
    "        col_w = [45, pdf.w - pdf.l_margin - pdf.r_margin - 45]; th = 6\n",
    "        for k, v in rows:\n",
    "            _set_font_unicode(pdf, size=10, style=\"B\"); pdf.cell(col_w[0], th, str(k if UNICODE_FONT_PATH else _sanitize_text(k)))\n",
    "            _set_font_unicode(pdf, size=10, style=\"\"); pdf.multi_cell(col_w[1], th, str(v if UNICODE_FONT_PATH else _sanitize_text(v)))\n",
    "        pdf.ln(1)\n",
    "    def export_analyst_note_pdf(note_text: str, risk_now=None, risk_10m=None, risk_30m=None,\n",
    "                                contributors: Optional[List[str]] = None, freshness: Optional[str] = None,\n",
    "                                confidence: Optional[str] = None,\n",
    "                                out_path: Path = OUT / \"analyst_note.pdf\", title: str = \"Analyst Note\") -> Path:\n",
    "        pdf = _init_pdf(title=title)\n",
    "        metrics = []\n",
    "        if risk_now is not None: metrics.append((\"Risk Score (now)\", f\"{risk_now:.2f}\"))\n",
    "        if risk_10m is not None: metrics.append((\"Risk Forecast (10m)\", f\"{risk_10m:.2f}\"))\n",
    "        if risk_30m is not None: metrics.append((\"Risk Forecast (30m)\", f\"{risk_30m:.2f}\"))\n",
    "        if freshness: metrics.append((\"Freshness\", freshness))\n",
    "        if confidence: metrics.append((\"Confidence\", confidence))\n",
    "        if contributors: metrics.append((\"Top Contributors\", \"; \".join(contributors[:3])))\n",
    "        if metrics: _kv_table(pdf, metrics)\n",
    "        _h(pdf, \"Analyst Note\", 2); _p(pdf, note_text)\n",
    "        pdf.output(str(out_path)); print(f\"[ok] Analyst Note exported → {out_path}\")\n",
    "        return out_path\n",
    "    def export_markdown_pdf(md_text: str, out_path: Path = OUT / \"report.pdf\", title: str = \"Depeg Sentinel Report\") -> Path:\n",
    "        pdf = _init_pdf(title=title)\n",
    "        for raw in md_text.splitlines():\n",
    "            line = raw.rstrip(\"\\n\")\n",
    "            if   line.startswith(\"### \"): _h(pdf, line[4:], 3)\n",
    "            elif line.startswith(\"## \"):  _h(pdf, line[3:], 2)\n",
    "            elif line.startswith(\"# \"):   _h(pdf, line[2:], 1)\n",
    "            elif line.strip() == \"\": pdf.ln(1)\n",
    "            else: _p(pdf, line)\n",
    "        pdf.output(str(out_path)); print(f\"[ok] Markdown PDF exported → {out_path}\")\n",
    "        return out_path\n",
    "else:\n",
    "    def export_analyst_note_pdf(*args, **kwargs):\n",
    "        path = kwargs.get(\"out_path\", OUT / \"analyst_note.txt\")\n",
    "        Path(path).write_text(kwargs.get(\"note_text\", \"\")); print(f\"[ok] Analyst Note (plain) → {path}\"); return path\n",
    "    def export_markdown_pdf(md_text: str, out_path: Path = OUT / \"report.txt\", title: str = \"Depeg Sentinel Report\") -> Path:\n",
    "        Path(out_path).write_text(md_text); print(f\"[ok] Report (plain) → {out_path}\"); return out_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fc5bc808-90f4-4aba-99d9-b38e11bf5003",
   "metadata": {},
   "outputs": [],
   "source": [
    "CANON_COLS = [\n",
    "    \"ts\",\"pool\",\"dex_spot\",\"dex_twap\",\"oracle_ratio\",\"dev\",\"dev_roll_std\",\n",
    "    \"tvl_outflow_rate\",\"virtual_price\",\"spot_twap_gap_bps\",\"r0\",\"r1\",\"r0_delta\",\"r1_delta\",\n",
    "    \"event_severity_max_24h\",\"event_count_24h\",\"feeds_fresh\",\"run_quality_pass\",\"model_version\",\"block\",\n",
    "    \"z_if\",\"z_lof\",\"z_ocsvm\",\"z_cusum\",\"z_ae\",\"anom_fused\",\n",
    "    \"y_10m\",\n",
    "    \"neighbor_max_dev\",\"neighbor_avg_anom\",\"lead_lag_best\",\"corr_best\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e29ae107-df9c-4940-8e21-025ea6874f92",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_live_schema(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for col in CANON_COLS:\n",
    "        if col not in df.columns: df[col] = np.nan\n",
    "    return df[CANON_COLS]\n",
    "def write_live(df: pd.DataFrame) -> Path:\n",
    "    df2 = ensure_live_schema(df)\n",
    "    df2[\"ts\"] = _iso_str(df2[\"ts\"])\n",
    "    df2.to_csv(LIVE_CSV, index=False); return LIVE_CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "4ea05229-610c-49bf-b554-cf05a8ed5638",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_live(n: int | None = None) -> pd.DataFrame:\n",
    "    if not LIVE_CSV.exists(): return pd.DataFrame(columns=CANON_COLS)\n",
    "    df = pd.read_csv(LIVE_CSV, dtype=str)\n",
    "    num_cols = [c for c in CANON_COLS if c not in (\"ts\",\"pool\",\"model_version\",\"block\")]\n",
    "    for c in num_cols:\n",
    "        if c in df.columns: df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    if \"ts\" in df.columns:\n",
    "        df[\"ts\"] = pd.to_datetime(df[\"ts\"], utc=True, errors=\"coerce\")\n",
    "    df = ensure_live_schema(df)\n",
    "    return df.tail(n) if n else df\n",
    "def _pct(a, b):\n",
    "    if pd.isna(a) or pd.isna(b) or b == 0: return np.nan\n",
    "    return (a - b) / b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "8bc91c21-9e68-40cc-97ab-eec94cd18b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class OnchainResult:\n",
    "    ok: bool\n",
    "    ts: str\n",
    "    block: Optional[int]\n",
    "    data: dict\n",
    "    err: Optional[str] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "3cad2022-446d-4cb5-8499-d53fd92e2152",
   "metadata": {},
   "outputs": [],
   "source": [
    "ABI_AGG = json.loads(\"\"\"[\n",
    " {\"inputs\":[],\"name\":\"decimals\",\"outputs\":[{\"internalType\":\"uint8\",\"name\":\"\",\"type\":\"uint8\"}],\"stateMutability\":\"view\",\"type\":\"function\"},\n",
    " {\"inputs\":[],\"name\":\"latestRoundData\",\"outputs\":[\n",
    "  {\"internalType\":\"uint80\",\"name\":\"roundId\",\"type\":\"uint80\"},\n",
    "  {\"internalType\":\"int256\",\"name\":\"answer\",\"type\":\"int256\"},\n",
    "  {\"internalType\":\"uint256\",\"name\":\"startedAt\",\"type\":\"uint256\"},\n",
    "  {\"internalType\":\"uint256\",\"name\":\"updatedAt\",\"type\":\"uint256\"},\n",
    "  {\"internalType\":\"uint80\",\"name\":\"answeredInRound\",\"type\":\"uint80\"}],\n",
    " \"stateMutability\":\"view\",\"type\":\"function\"}]\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "00c05ecf-9bd2-4220-b2ea-2083de01ab28",
   "metadata": {},
   "outputs": [],
   "source": [
    "class OnChainTool:\n",
    "    def __init__(self, rpc: str, mock: bool = False):\n",
    "        self.mock = mock or not _WEB3_OK\n",
    "        self.w3 = None\n",
    "        if not self.mock and _WEB3_OK and rpc:\n",
    "            try:\n",
    "                self.w3 = Web3(Web3.HTTPProvider(rpc, request_kwargs={\"timeout\": 8}))\n",
    "                connected = self.w3.is_connected() if hasattr(self.w3, \"is_connected\") else self.w3.isConnected()\n",
    "                if not connected:\n",
    "                    print(\"[onchain] RPC not reachable → mock_mode\"); self.mock = True\n",
    "                else:\n",
    "                    try:\n",
    "                        net_id = self.w3.eth.chain_id\n",
    "                        print(f\"[onchain] connected. chain_id={net_id}\")\n",
    "                    except Exception:\n",
    "                        print(\"[onchain] connected (chain_id check failed)\")\n",
    "            except Exception as e:\n",
    "                print(f\"[onchain] init error: {e} → mock_mode\"); self.mock = True\n",
    "        else:\n",
    "            print(\"[onchain] mock_mode enabled\")\n",
    "    def _call(self, fn):\n",
    "        return fn()\n",
    "    def get_oracle_price(self, feed_addr: str) -> OnchainResult:\n",
    "        ts = _now_iso()\n",
    "        if self.mock or not feed_addr or feed_addr.lower().startswith(\"0x0000\"):\n",
    "            px = float(1.0 + np.random.normal(0, 1e-4))\n",
    "            return OnchainResult(True, ts, None, {\"price\": px, \"decimals\": 8, \"source\": \"mock\"})\n",
    "        try:\n",
    "            feed = Web3.to_checksum_address(feed_addr)\n",
    "            c = self.w3.eth.contract(address=feed, abi=ABI_AGG)\n",
    "            decimals = int(self._call(lambda: c.functions.decimals().call()))\n",
    "            rd = self._call(lambda: c.functions.latestRoundData().call())\n",
    "            ans, upd = rd[1], int(rd[3])\n",
    "            price = float(ans) / (10 ** decimals)\n",
    "            block = self.w3.eth.block_number\n",
    "            return OnchainResult(True, ts, block, {\"price\": price, \"decimals\": decimals, \"updatedAt\": upd, \"source\": \"chainlink\"})\n",
    "        except Exception as e:\n",
    "            return OnchainResult(False, ts, None, {}, str(e))\n",
    "    def get_univ3_price(self, pool_addr: str) -> OnchainResult:\n",
    "        ts = _now_iso()\n",
    "        if self.mock or not pool_addr or pool_addr.lower().startswith(\"0x0000\"):\n",
    "            base = 1.0 + np.random.normal(0, 3e-4)\n",
    "            return OnchainResult(True, ts, None, {\"price\": float(base), \"sqrtPriceX96\": None, \"source\": \"mock\"})\n",
    "        try:\n",
    "            _ABI_UNIV3_POOL = [\n",
    "                {\"name\":\"slot0\",\"inputs\":[],\"outputs\":[\n",
    "                    {\"type\":\"uint160\",\"name\":\"sqrtPriceX96\"},\n",
    "                    {\"type\":\"int24\",\"name\":\"tick\"},\n",
    "                    {\"type\":\"uint16\",\"name\":\"observationIndex\"},\n",
    "                    {\"type\":\"uint16\",\"name\":\"observationCardinality\"},\n",
    "                    {\"type\":\"uint16\",\"name\":\"observationCardinalityNext\"},\n",
    "                    {\"type\":\"uint8\",\"name\":\"feeProtocol\"},\n",
    "                    {\"type\":\"bool\",\"name\":\"unlocked\"}],\n",
    "                 \"stateMutability\":\"view\",\"type\":\"function\"},\n",
    "                {\"name\":\"token0\",\"inputs\":[],\"outputs\":[{\"type\":\"address\"}], \"stateMutability\":\"view\",\"type\":\"function\"},\n",
    "                {\"name\":\"token1\",\"inputs\":[],\"outputs\":[{\"type\":\"address\"}], \"stateMutability\":\"view\",\"type\":\"function\"},\n",
    "            ]\n",
    "            _ABI_ERC20_DEC = [\n",
    "                {\"name\":\"decimals\",\"inputs\":[],\"outputs\":[{\"type\":\"uint8\"}], \"stateMutability\":\"view\",\"type\":\"function\"},\n",
    "                {\"name\":\"symbol\",\"inputs\":[],\"outputs\":[{\"type\":\"string\"}],   \"stateMutability\":\"view\",\"type\":\"function\"},\n",
    "            ]\n",
    "            pool = Web3.to_checksum_address(pool_addr)\n",
    "            c = self.w3.eth.contract(address=pool, abi=_ABI_UNIV3_POOL)\n",
    "            slot0 = self._call(lambda: c.functions.slot0().call()); sqrtp = slot0[0]\n",
    "            t0 = self._call(lambda: c.functions.token0().call()); t1 = self._call(lambda: c.functions.token1().call())\n",
    "            t0c = self.w3.eth.contract(address=t0, abi=_ABI_ERC20_DEC)\n",
    "            t1c = self.w3.eth.contract(address=t1, abi=_ABI_ERC20_DEC)\n",
    "            d0 = int(self._call(lambda: t0c.functions.decimals().call()))\n",
    "            d1 = int(self._call(lambda: t1c.functions.decimals().call()))\n",
    "            p_raw = (sqrtp / (2**96))**2\n",
    "            price = float(p_raw * (10 ** (d0 - d1)))\n",
    "            block = self.w3.eth.block_number\n",
    "            return OnchainResult(True, ts, block, {\n",
    "                \"price\": price, \"sqrtPriceX96\": int(sqrtp),\n",
    "                \"token0\": t0, \"token1\": t1, \"decimals0\": d0, \"decimals1\": d1, \"source\": \"uniswap_v3\"\n",
    "            })\n",
    "        except Exception as e:\n",
    "            return OnchainResult(False, ts, None, {}, str(e))\n",
    "    def get_curve_virtual_price(self, pool_addr: str) -> OnchainResult:\n",
    "        ts = _now_iso()\n",
    "        if self.mock or not pool_addr or pool_addr.lower().startswith(\"0x0000\"):\n",
    "            vp = 1.0 + np.random.normal(0, 2e-4)\n",
    "            return OnchainResult(True, ts, None, {\"virtual_price\": float(vp), \"source\": \"mock\"})\n",
    "        try:\n",
    "            abi = [\n",
    "                {\"name\":\"get_virtual_price\",\"inputs\":[],\"outputs\":[{\"type\":\"uint256\"}],\"stateMutability\":\"view\",\"type\":\"function\"},\n",
    "                {\"name\":\"virtual_price\",\"inputs\":[],\"outputs\":[{\"type\":\"uint256\"}],\"stateMutability\":\"view\",\"type\":\"function\"},\n",
    "            ]\n",
    "            pool = Web3.to_checksum_address(pool_addr)\n",
    "            c = self.w3.eth.contract(address=pool, abi=abi)\n",
    "            def _try_vp():\n",
    "                try: return int(c.functions.get_virtual_price().call())\n",
    "                except Exception: return int(c.functions.virtual_price().call())\n",
    "            raw = self._call(_try_vp)\n",
    "            vp = raw / 1e18 if raw > 10**9 else raw\n",
    "            block = self.w3.eth.block_number\n",
    "            return OnchainResult(True, ts, block, {\"virtual_price\": float(vp), \"source\": \"curve\"})\n",
    "        except Exception as e:\n",
    "            return OnchainResult(False, ts, None, {}, str(e))\n",
    "    def get_reserves_min(self, pool_addr: str) -> OnchainResult:\n",
    "        ts = _now_iso()\n",
    "        if self.mock or not pool_addr or pool_addr.lower().startswith(\"0x0000\"):\n",
    "            r0 = float(1e9 + np.random.randint(-5e6, 5e6))\n",
    "            r1 = float(1e9 + np.random.randint(-5e6, 5e6))\n",
    "            return OnchainResult(True, ts, None, {\"r0\": r0, \"r1\": r1, \"source\": \"mock\"})\n",
    "        try:\n",
    "            abi = [{\"name\":\"getReserves\",\"inputs\":[],\"outputs\":[\n",
    "                {\"type\":\"uint112\",\"name\":\"_reserve0\"},{\"type\":\"uint112\",\"name\":\"_reserve1\"},{\"type\":\"uint32\",\"name\":\"_blockTimestampLast\"}],\n",
    "                \"stateMutability\":\"view\",\"type\":\"function\"}]\n",
    "            pool = Web3.to_checksum_address(pool_addr)\n",
    "            c = self.w3.eth.contract(address=pool, abi=abi)\n",
    "            r0, r1, ts_last = self._call(lambda: c.functions.getReserves().call())\n",
    "            block = self.w3.eth.block_number\n",
    "            return OnchainResult(True, ts, block, {\"r0\": float(r0), \"r1\": float(r1), \"ts_last\": int(ts_last), \"source\": \"getReserves\"})\n",
    "        except Exception as e:\n",
    "            return OnchainResult(False, ts, None, {}, str(e))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "ba077bc2-11d7-46ad-a704-381849d73408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[onchain] mock_mode enabled\n"
     ]
    }
   ],
   "source": [
    "onchain = OnChainTool(CFG.eth_rpc, mock=CFG.mock_mode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "2376fafd-c562-408d-835e-1483e9c0fac7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _TWAPCache:\n",
    "    def __init__(self, alpha: float = 0.2): self.alpha = alpha; self.state: Dict[str, float] = {}\n",
    "    def update(self, pool: str, spot: float) -> float:\n",
    "        if np.isnan(spot): return np.nan\n",
    "        prev = self.state.get(pool, spot); twap = self.alpha*spot + (1-self.alpha)*prev\n",
    "        self.state[pool] = twap; return twap\n",
    "_TWAP = _TWAPCache(alpha=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7ab1ec19-8dcc-42b0-988d-8828cbc5d31c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _last_per_pool(col: str) -> dict[str, float]:\n",
    "    df = load_live(5000)\n",
    "    if df.empty or col not in df.columns: return {}\n",
    "    return df.sort_values(\"ts\").groupby(\"pool\")[col].last().to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e05b834-d1cd-4585-847a-93b562190a3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(raw_rows: List[dict]) -> pd.DataFrame:\n",
    "    df = pd.DataFrame(raw_rows)\n",
    "    if df.empty: return df\n",
    "    if \"dex_spot\" in df.columns and \"virtual_price\" in df.columns:\n",
    "        df[\"dex_spot\"] = pd.to_numeric(df[\"dex_spot\"], errors=\"coerce\")\n",
    "        df[\"virtual_price\"] = pd.to_numeric(df[\"virtual_price\"], errors=\"coerce\")\n",
    "        df[\"dex_spot\"] = df[\"dex_spot\"].where(pd.notna(df[\"dex_spot\"]), df[\"virtual_price\"])\n",
    "    df[\"oracle_px\"] = pd.to_numeric(df[\"oracle_px\"], errors=\"coerce\")\n",
    "    df[\"oracle_ratio\"] = df[\"dex_spot\"] / df[\"oracle_px\"]\n",
    "    df[\"dev\"]          = (df[\"dex_spot\"] - df[\"oracle_px\"]) / df[\"oracle_px\"]\n",
    "    df[\"dex_twap\"] = [ _TWAP.update(p, float(s) if pd.notna(s) else np.nan) for p, s in zip(df[\"pool\"], df[\"dex_spot\"]) ]\n",
    "    df[\"spot_twap_gap_bps\"] = (df[\"dex_spot\"] - df[\"dex_twap\"]) / df[\"dex_twap\"].replace(0,np.nan) * 1e4\n",
    "    df[\"spot_twap_gap_bps\"] = df[\"spot_twap_gap_bps\"].replace([np.inf,-np.inf], np.nan)\n",
    "    last_r0 = _last_per_pool(\"r0\"); last_r1 = _last_per_pool(\"r1\")\n",
    "    df[\"r0_delta\"] = [np.nan if np.isnan(r) else (r - last_r0.get(p, r)) for p, r in zip(df[\"pool\"], df[\"r0\"])]\n",
    "    df[\"r1_delta\"] = [np.nan if np.isnan(r) else (r - last_r1.get(p, r)) for p, r in zip(df[\"pool\"], df[\"r1\"])]\n",
    "    df[\"tvl_outflow_rate\"] = [\n",
    "        _pct((r0 or 0)+(r1 or 0), (last_r0.get(p, r0) or 0)+(last_r1.get(p, r1) or 0))\n",
    "        for p, r0, r1 in zip(df[\"pool\"], df[\"r0\"], df[\"r1\"])\n",
    "    ]\n",
    "    hist = load_live(1000)\n",
    "    if not hist.empty:\n",
    "        hist = pd.concat([hist, df], ignore_index=True).sort_values([\"pool\",\"ts\"])\n",
    "        hist[\"dev_roll_std\"] = hist.groupby(\"pool\")[\"dev\"].rolling(20, min_periods=5).std().reset_index(level=0, drop=True)\n",
    "        df = df.merge(hist.loc[hist[\"ts\"].isin(df[\"ts\"])][[\"ts\",\"pool\",\"dev_roll_std\"]], on=[\"ts\",\"pool\"], how=\"left\")\n",
    "    else:\n",
    "        df[\"dev_roll_std\"] = np.nan\n",
    "    df[\"event_severity_max_24h\"] = df.get(\"event_severity_max_24h\", pd.Series([0]*len(df)))\n",
    "    df[\"event_count_24h\"]        = df.get(\"event_count_24h\", pd.Series([0]*len(df)))\n",
    "    df[\"feeds_fresh\"]            = True\n",
    "    df[\"run_quality_pass\"]       = df[[\"dex_spot\",\"oracle_px\",\"dev\"]].notna().all(axis=1)\n",
    "    df[\"model_version\"]          = \"v2.0.0\"\n",
    "    if \"block\" not in df.columns: df[\"block\"] = None\n",
    "    return ensure_live_schema(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d08c5b62-c47b-46cc-b087-127d3b92f30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_once() -> pd.DataFrame:\n",
    "    rows = []\n",
    "    ts_now = _now_iso()\n",
    "    for pool_name, meta in CFG.pools.items():\n",
    "        sym  = meta.get(\"symbol\", \"USDC/USD\"); addr = meta[\"address\"]; typ  = meta[\"type\"]\n",
    "        uni = onchain.get_univ3_price(addr) if typ==\"uniswap_v3\" else None\n",
    "        cur = onchain.get_curve_virtual_price(addr) if typ==\"curve\" else None\n",
    "        ora = onchain.get_oracle_price(CFG.chainlink_feeds.get(sym, \"\"))\n",
    "        dex_spot   = (uni.data.get(\"price\") if uni and uni.ok else np.nan) if typ==\"uniswap_v3\" else np.nan\n",
    "        virt_price = (cur.data.get(\"virtual_price\") if cur and cur.ok else np.nan) if typ==\"curve\" else np.nan\n",
    "        oracle_px  = (ora.data.get(\"price\") if ora and ora.ok else np.nan)\n",
    "        res = onchain.get_reserves_min(addr)\n",
    "        r0 = float(res.data.get(\"r0\")) if res and res.ok else np.nan\n",
    "        r1 = float(res.data.get(\"r1\")) if res and res.ok else np.nan\n",
    "        rows.append({\"ts\": ts_now, \"pool\": pool_name, \"dex_spot\": float(dex_spot) if pd.notna(dex_spot) else np.nan,\n",
    "                     \"oracle_px\": float(oracle_px) if pd.notna(oracle_px) else np.nan, \"virtual_price\": float(virt_price) if pd.notna(virt_price) else np.nan,\n",
    "                     \"r0\": r0, \"r1\": r1, \"block\": None})\n",
    "    feat = engineer_features(rows); append_live(feat); return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c4210f9b-038a-4552-899c-ee592c070dc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _scale_01(x: np.ndarray) -> np.ndarray:\n",
    "    a = np.nanmin(x); b = np.nanmax(x)\n",
    "    if not np.isfinite(a) or not np.isfinite(b) or b-a == 0: return np.zeros_like(x)\n",
    "    return (x - a) / (b - a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3dbc476f-da6d-447e-ae52-c1b6d7d27166",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cusum_score(dev: pd.Series, w: int = 30, k: float = 0.0) -> pd.Series:\n",
    "    s = dev.fillna(0).astype(float).to_numpy()\n",
    "    g_pos = np.zeros_like(s, dtype=float); g_neg = np.zeros_like(s, dtype=float)\n",
    "    for i in range(1, len(s)):\n",
    "        g_pos[i] = max(0.0, g_pos[i-1] + (s[i] - k))\n",
    "        g_neg[i] = min(0.0, g_neg[i-1] + (s[i] + k))\n",
    "    mag = np.abs(g_pos) + np.abs(g_neg)\n",
    "    out = pd.Series(mag, index=dev.index)\n",
    "    out_rolled = out.rolling(w, min_periods=5).apply(lambda z: z[-1] / (np.nanmax(np.abs(z))+1e-9), raw=True)\n",
    "    return out_rolled.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c3847530-297b-4388-9901-9427ee0ab205",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _autoencoder_scores(df_feat: pd.DataFrame, X: np.ndarray) -> np.ndarray:\n",
    "    if not _TORCH_OK: return np.zeros(len(df_feat))\n",
    "    inp = X.astype(np.float32); d = inp.shape[1]\n",
    "    class AE(nn.Module):\n",
    "        def __init__(self, d):\n",
    "            super().__init__()\n",
    "            self.enc = nn.Sequential(nn.Linear(d, max(4, d//2)), nn.ReLU(), nn.Linear(max(4, d//2), max(2, d//4)))\n",
    "            self.dec = nn.Sequential(nn.Linear(max(2, d//4), max(4, d//2)), nn.ReLU(), nn.Linear(max(4, d//2), d))\n",
    "        def forward(self, x): return self.dec(self.enc(x))\n",
    "    model = AE(d); opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    ds = TensorDataset(torch.from_numpy(inp)); dl = DataLoader(ds, batch_size=64, shuffle=True)\n",
    "    model.train()\n",
    "    for _ in range(5):\n",
    "        for (xb,) in dl:\n",
    "            opt.zero_grad(); loss = ((model(xb) - xb)**2).mean(); loss.backward(); opt.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad(): rec = model(torch.from_numpy(inp)).numpy()\n",
    "    return _scale_01(np.mean((rec - inp)**2, axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "1ae5152f-2d9b-4303-acbe-cfca4645faf3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_anomaly_zoo_update_live(features_cols: List[str] | None = None) -> pd.DataFrame:\n",
    "    df = load_live()\n",
    "    if df.empty or df[\"dev\"].isna().all():\n",
    "        print(\"[zoo] live dataset empty; sample first.\"); return df\n",
    "    if not features_cols:\n",
    "        features_cols = [\"dev\", \"dev_roll_std\", \"tvl_outflow_rate\", \"spot_twap_gap_bps\", \"oracle_ratio\"]\n",
    "    use_cols = [c for c in features_cols if c in df.columns]\n",
    "    if not use_cols: raise ValueError(\"No usable feature columns found in live dataset.\")\n",
    "    df_proc = df.copy()\n",
    "    X = df_proc[use_cols].astype(float).replace([np.inf, -np.inf], np.nan).fillna(0.0).to_numpy()\n",
    "    scaler = StandardScaler() if _SK_OK else None; Xs = scaler.fit_transform(X) if scaler is not None else X\n",
    "    z_if = np.zeros(len(df_proc)); z_lof = np.zeros(len(df_proc)); z_ocsvm = np.zeros(len(df_proc))\n",
    "    if _SK_OK:\n",
    "        z_if = _scale_01(-IsolationForest(n_estimators=200, contamination=\"auto\", random_state=SEED).fit(Xs).score_samples(Xs))\n",
    "        try:\n",
    "            lof2 = LocalOutlierFactor(n_neighbors=20, novelty=True).fit(Xs)\n",
    "            z_lof = _scale_01(-lof2.score_samples(Xs))\n",
    "        except Exception:\n",
    "            lof = LocalOutlierFactor(n_neighbors=20, novelty=False)\n",
    "            z_lof = _scale_01((-lof.fit_predict(Xs)).astype(float))\n",
    "        try:\n",
    "            oc = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.05)\n",
    "            z_ocsvm = _scale_01(-oc.fit(Xs).decision_function(Xs))\n",
    "        except Exception:\n",
    "            z_ocsvm = np.zeros(len(df_proc))\n",
    "    z_cusum = []\n",
    "    for _, g in df_proc.sort_values([\"pool\",\"ts\"]).groupby(\"pool\", sort=False):\n",
    "        z_cusum.append(_cusum_score(g[\"dev\"]))\n",
    "    z_cusum = _scale_01(pd.concat(z_cusum).reindex(df_proc.index).fillna(0.0).to_numpy())\n",
    "    try:\n",
    "        z_ae = _autoencoder_scores(df_proc, Xs)\n",
    "    except Exception:\n",
    "        z_ae = np.zeros(len(df_proc))\n",
    "    fused = np.nanmax(np.vstack([z_if, z_lof, z_ocsvm, z_cusum, z_ae]), axis=0)\n",
    "    for k, arr in [(\"z_if\", z_if), (\"z_lof\", z_lof), (\"z_ocsvm\", z_ocsvm), (\"z_cusum\", z_cusum), (\"z_ae\", z_ae), (\"anom_fused\", fused)]:\n",
    "        df_proc[k] = arr\n",
    "    write_live(df_proc); print(\"[zoo] anomaly scores updated\"); return df_proc.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6671d286-411c-4938-8054-bc3634e92127",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _time_split_idx(ts: pd.Series, frac_train: float = 0.70):\n",
    "    order = np.argsort(ts.fillna(pd.Timestamp.utcnow()).values)\n",
    "    n = len(order); cut = int(n*frac_train)\n",
    "    return order[:cut], order[cut:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b742c763-13ae-4a33-95f4-a1f561834b1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_targets(df: pd.DataFrame, horizon: int = 10, dev_thr: float = 0.005,\n",
    "                  fused_col: str | None = None, fused_thr: float = 0.90) -> pd.Series:\n",
    "    df = df.sort_values([\"pool\",\"ts\"]).reset_index(drop=True)\n",
    "    y = pd.Series(0, index=df.index)\n",
    "    by = df.groupby(\"pool\")\n",
    "    for pool, g in by:\n",
    "        g_idx = g.index.to_list()\n",
    "        g_dev = g[\"dev\"].abs().to_numpy()\n",
    "        fused = g[fused_col].to_numpy() if fused_col and fused_col in g.columns else None\n",
    "        n = len(g_idx)\n",
    "        for i in range(n):\n",
    "            j1, j2 = i+1, min(n, i+1+horizon)\n",
    "            if j1 >= j2: continue\n",
    "            cond_dev = np.nanmax(g_dev[j1:j2])\n",
    "            cond_fused = np.nanmax(fused[j1:j2]) if fused is not None else 0.0\n",
    "            if (cond_dev >= dev_thr) or (fused is not None and cond_fused >= fused_thr):\n",
    "                y.iloc[g_idx[i]] = 1\n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "baf591dc-e31a-44db-8305-c44905670615",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _ensure_labels(df: pd.DataFrame, base_thr=0.005) -> pd.Series:\n",
    "    y = label_targets(df, horizon=10, dev_thr=base_thr, fused_col=\"anom_fused\" if \"anom_fused\" in df.columns else None)\n",
    "    if y.nunique() > 1: return y\n",
    "    y2 = label_targets(df, horizon=10, dev_thr=max(0.001, base_thr/5), fused_col=\"anom_fused\" if \"anom_fused\" in df.columns else None)\n",
    "    if y2.nunique() > 1: return y2\n",
    "    af = df[\"anom_fused\"].replace([np.inf,-np.inf], np.nan).fillna(0.0)\n",
    "    k = max(1, int(0.05*len(af))); thr = np.partition(af.values, -k)[-k]\n",
    "    return (af >= thr).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "ce75c218-1979-4e06-a200-3b71a031de64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_forecaster_10m(feature_cols: List[str] | None = None, label_col: str = \"y_10m\"):\n",
    "    if not _XGB_OK or not _SK_OK: raise RuntimeError(\"xgboost + scikit-learn required to train forecaster.\")\n",
    "    df = load_live().copy()\n",
    "    if df.empty: raise ValueError(\"live_dataset is empty. sample more rows first.\")\n",
    "    if label_col not in df.columns or df[label_col].nunique(dropna=True) < 2:\n",
    "        df[label_col] = _ensure_labels(df); write_live(df)\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [\"dev\",\"dev_roll_std\",\"tvl_outflow_rate\",\"spot_twap_gap_bps\",\"oracle_ratio\",\"anom_fused\",\"r0_delta\",\"r1_delta\",\"event_severity_max_24h\",\"event_count_24h\"]\n",
    "    use_cols = [c for c in feature_cols if c in df.columns]\n",
    "    X = df[use_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0).to_numpy()\n",
    "    y = pd.to_numeric(df[label_col], errors=\"coerce\").fillna(0).astype(int).to_numpy()\n",
    "    if len(np.unique(y)) < 2:\n",
    "        df[label_col] = _ensure_labels(df); write_live(df)\n",
    "        y = pd.to_numeric(df[label_col], errors=\"coerce\").fillna(0).astype(int).to_numpy()\n",
    "    ts = df[\"ts\"]; tr_idx, te_idx = _time_split_idx(ts, 0.70)\n",
    "    Xtr, Xte = X[tr_idx], X[te_idx]; ytr, yte = y[tr_idx], y[te_idx]\n",
    "    clf = xgb.XGBClassifier(n_estimators=250, max_depth=4, learning_rate=0.06,\n",
    "                            subsample=0.9, colsample_bytree=0.8,\n",
    "                            objective=\"binary:logistic\", eval_metric=\"logloss\",\n",
    "                            random_state=SEED, n_jobs=0)\n",
    "    clf.fit(Xtr, ytr)\n",
    "    calib = None\n",
    "    unique, counts = np.unique(ytr, return_counts=True)\n",
    "    min_cls = min(counts) if len(counts)==2 else 0\n",
    "    if min_cls >= 3:\n",
    "        try:\n",
    "            calib = CalibratedClassifierCV(clf, method=\"isotonic\", cv=3); calib.fit(Xtr, ytr)\n",
    "        except Exception: calib = None\n",
    "    if calib is None and min_cls >= 2:\n",
    "        try:\n",
    "            calib = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=2); calib.fit(Xtr, ytr)\n",
    "        except Exception: calib = None\n",
    "    def _proba(model, X_): return (model or clf).predict_proba(X_)[:,1]\n",
    "    ap, bs = float(\"nan\"), float(\"nan\")\n",
    "    if len(np.unique(yte))>1:\n",
    "        p_te = _proba(calib, Xte)\n",
    "        try: ap = average_precision_score(yte, p_te)\n",
    "        except Exception: pass\n",
    "        try: bs = float(brier_score_loss(yte, p_te))\n",
    "        except Exception: bs = float(\"nan\")\n",
    "    print(f\"[forecast10m] AP={ap if pd.notna(ap) else float('nan'):.3f}  Brier={bs if pd.notna(bs) else float('nan'):.3f}  n_te={len(yte)}  calib={'iso3' if (calib and getattr(calib,'method','')=='isotonic') else ('sig2' if calib else 'none')}\")\n",
    "    dump(clf, FORECAST_10M_PATH); \n",
    "    if calib: dump(calib, CALIB_10M_PATH)\n",
    "    elif CALIB_10M_PATH.exists(): CALIB_10M_PATH.unlink()\n",
    "    tail = df.tail(6).copy()\n",
    "    X_tail = tail[use_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0).to_numpy()\n",
    "    tail[\"risk_forecast_10m\"] = _proba(calib, X_tail)\n",
    "    return tail[[\"ts\",\"pool\",\"anom_fused\",\"risk_forecast_10m\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "def49aa2-2ecc-478d-8e89-d5ab10b16f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_forecaster():\n",
    "    clf = load(FORECAST_10M_PATH) if FORECAST_10M_PATH.exists() else None\n",
    "    calib = load(CALIB_10M_PATH) if CALIB_10M_PATH.exists() else None\n",
    "    return clf, calib\n",
    "def _proba_model(clf, calib, X: np.ndarray) -> np.ndarray:\n",
    "    model = calib if calib is not None else clf\n",
    "    if model is None: raise RuntimeError(\"No 10m forecaster found. Run train_forecaster_10m() first.\")\n",
    "    return model.predict_proba(X)[:, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "e13f7a24-5b39-41f6-a521-261033fb60c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_latest_10m(n_tail: int = 60, feature_cols: Sequence[str] | None = None, write_parquet: bool = True) -> pd.DataFrame:\n",
    "    df = load_live()\n",
    "    if df.empty: raise ValueError(\"live_dataset is empty. Run sample_once() first.\")\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [\"dev\",\"dev_roll_std\",\"tvl_outflow_rate\",\"spot_twap_gap_bps\",\"oracle_ratio\",\"anom_fused\",\"r0_delta\",\"r1_delta\",\"event_severity_max_24h\",\"event_count_24h\"]\n",
    "    use_cols = [c for c in feature_cols if c in df.columns]\n",
    "    X_tail = df.tail(n_tail)[use_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0).to_numpy()\n",
    "    clf, calib = _load_forecaster(); p = _proba_model(clf, calib, X_tail)\n",
    "    out = df.tail(n_tail).copy(); out[\"risk_forecast_10m\"] = p\n",
    "    out = out[[\"ts\",\"pool\",\"anom_fused\",\"risk_forecast_10m\"]]\n",
    "    if write_parquet:\n",
    "        try:\n",
    "            out.to_parquet(FORECAST_10M_PARQUET, index=False); print(f\"[forecast10m] wrote {FORECAST_10M_PARQUET}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] parquet write failed: {e}\")\n",
    "    return out.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "7b30b281-e5f2-4cde-86c3-7d12dd9500b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_forecast_10m(feature_cols: Sequence[str] | None = None, n_repeats: int = 8) -> dict:\n",
    "    if not _SK_OK:\n",
    "        payload = {\"ts\": _now_iso(), \"top_contributors\": [], \"all_features\": []}\n",
    "        EXPLAIN_JSON.write_text(json.dumps(payload, indent=2)); print(f\"[explain] wrote {EXPLAIN_JSON}\"); return payload\n",
    "    df = load_live()\n",
    "    if df.empty: raise ValueError(\"live_dataset is empty.\")\n",
    "    if \"y_10m\" not in df.columns or df[\"y_10m\"].nunique() < 2:\n",
    "        df[\"y_10m\"] = _ensure_labels(df); write_live(df)\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [\"dev\",\"dev_roll_std\",\"tvl_outflow_rate\",\"spot_twap_gap_bps\",\"oracle_ratio\",\"anom_fused\",\"r0_delta\",\"r1_delta\",\"event_severity_max_24h\",\"event_count_24h\"]\n",
    "    use_cols = [c for c in feature_cols if c in df.columns]\n",
    "    X = df[use_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0).to_numpy()\n",
    "    y = df[\"y_10m\"].astype(int).to_numpy()\n",
    "    tr_idx, te_idx = _time_split_idx(df[\"ts\"], 0.70)\n",
    "    Xte, yte = X[te_idx], y[te_idx]\n",
    "    clf, calib = _load_forecaster()\n",
    "    if clf is None and calib is None:\n",
    "        _ = train_forecaster_10m(feature_cols=use_cols); clf, calib = _load_forecaster()\n",
    "    model = calib if calib is not None else clf\n",
    "    if len(np.unique(yte)) < 2:\n",
    "        payload = {\"ts\": _now_iso(), \"top_contributors\": [], \"all_features\": []}\n",
    "    else:\n",
    "        r = permutation_importance(model, Xte, yte, n_repeats=n_repeats, scoring=\"average_precision\", random_state=SEED)\n",
    "        imp = sorted(zip(use_cols, r.importances_mean, r.importances_std), key=lambda z: z[1], reverse=True)\n",
    "        top3 = [f\"{name} (+{mean:.4f}±{std:.4f} AP)\" for name, mean, std in imp[:3]]\n",
    "        payload = {\"ts\": _now_iso(), \"top_contributors\": top3, \"all_features\": [{\"feature\": n, \"mean\": float(m), \"std\": float(s)} for n,m,s in imp]}\n",
    "    EXPLAIN_JSON.write_text(json.dumps(payload, indent=2)); print(f\"[explain] wrote {EXPLAIN_JSON}\"); return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "48be7128-db8a-41d2-ba52-dc894c10bd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class PolicyDecision:\n",
    "    level: str\n",
    "    actions: list\n",
    "    rationale: str\n",
    "    requires_ack: bool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "5301872b-598f-436d-bf78-8444ee6acf9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SeverityModel:\n",
    "    def __init__(self, emb_model_name: Optional[str] = None, device: Optional[str] = None):\n",
    "        self.model = None; self.emb = None; self.dim = 128\n",
    "        self.emb_name = emb_model_name or \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "        try:\n",
    "            from sklearn.linear_model import LogisticRegression as _LR\n",
    "            self._LR = _LR; self._sk_ok = True\n",
    "        except Exception as e:\n",
    "            self._LR = None; self._sk_ok = False; warnings.warn(f\"[note] scikit-learn unavailable: {e}. Using prior-only severity.\")\n",
    "        try:\n",
    "            from sentence_transformers import SentenceTransformer as _SB\n",
    "            kwargs = {}; \n",
    "            if device: kwargs[\"device\"] = device\n",
    "            self.emb = _SB(self.emb_name, **kwargs)\n",
    "            try: self.dim = int(self.emb.get_sentence_embedding_dimension())\n",
    "            except Exception: pass\n",
    "        except Exception as e:\n",
    "            self.emb = None; warnings.warn(f\"[note] sentence-transformers unavailable: {e}. Using zero embeddings.\")\n",
    "    def _embed(self, texts: List[str]) -> np.ndarray:\n",
    "        texts = [t if isinstance(t, str) else \"\" for t in texts]\n",
    "        if self.emb is None: return np.zeros((len(texts), self.dim), dtype=np.float32)\n",
    "        try: return self.emb.encode(texts, normalize_embeddings=True)\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"[note] embedding failed: {e}. Returning zeros.\")\n",
    "            return np.zeros((len(texts), self.dim), dtype=np.float32)\n",
    "    def fit(self, texts: List[str], labels: List[int]):\n",
    "        if not texts: warnings.warn(\"[warn] fit called with empty texts; skipping.\"); return self\n",
    "        if len(texts) != len(labels): raise ValueError(\"texts and labels must have the same length\")\n",
    "        y = np.asarray(labels, dtype=int)\n",
    "        if self.emb is None:\n",
    "            p = float(np.mean(y)) if y.size else 0.0; self.model = {\"prior\": max(0.0, min(1.0, p))}\n",
    "            warnings.warn(\"[note] sentence-transformers missing; using prior-only severity.\"); return self\n",
    "        uniq = np.unique(y)\n",
    "        if not getattr(self, \"_sk_ok\", False) or uniq.size < 2:\n",
    "            p = float(np.mean(y)) if y.size else 0.0; self.model = {\"prior\": max(0.0, min(1.0, p))}\n",
    "            if not getattr(self, \"_sk_ok\", False): warnings.warn(\"[note] sklearn missing; using prior-only severity.\")\n",
    "            elif uniq.size < 2: warnings.warn(\"[note] one-class labels; using prior-only severity.\")\n",
    "            return self\n",
    "        X = self._embed(texts)\n",
    "        try:\n",
    "            clf = self._LR(max_iter=200); clf.fit(X, y); self.model = clf\n",
    "        except Exception as e:\n",
    "            p = float(np.mean(y)); self.model = {\"prior\": max(0.0, min(1.0, p))}\n",
    "            warnings.warn(f\"[note] LR fit failed: {e}. Using prior-only severity.\")\n",
    "        return self\n",
    "    def predict_proba(self, texts: List[str]) -> List[float]:\n",
    "        if self.model is None: return [0.0]*len(texts)\n",
    "        if isinstance(self.model, dict) and \"prior\" in self.model:\n",
    "            p = float(self.model[\"prior\"]); return [p]*len(texts)\n",
    "        X = self._embed(texts)\n",
    "        try:\n",
    "            proba = self.model.predict_proba(X)[:, -1]; return np.clip(proba, 0.0, 1.0).tolist()\n",
    "        except Exception as e:\n",
    "            warnings.warn(f\"[note] predict_proba failed: {e}. Returning zeros.\"); return [0.0]*len(texts)\n",
    "    def predict(self, texts: List[str], threshold: float = 0.5) -> List[int]:\n",
    "        p = self.predict_proba(texts); return [int(x >= float(threshold)) for x in p]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "e0842ca5-961e-455c-9130-8ef895c5b71f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_events() -> list[dict]:\n",
    "    try:\n",
    "        if EVENTS_JSON.exists():\n",
    "            return json.loads(EVENTS_JSON.read_text())\n",
    "    except Exception as e:\n",
    "        print(f\"[events] load failed: {e}\")\n",
    "    return []\n",
    "def _save_events(ev: list[dict]) -> None:\n",
    "    try:\n",
    "        EVENTS_JSON.write_text(json.dumps(ev, indent=2))\n",
    "    except Exception as e:\n",
    "        print(f\"[events] save failed: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "832acf7a-f68c-4ab5-a0c2-6979f22b263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _http_get_json(url: str, timeout: int = 10) -> dict | list | None:\n",
    "    try:\n",
    "        import requests\n",
    "        r = requests.get(url, timeout=timeout)\n",
    "        if r.status_code == 200:\n",
    "            try:\n",
    "                return r.json()\n",
    "            except Exception:\n",
    "                return {\"text\": r.text}\n",
    "        else:\n",
    "            print(f\"[offchain] non-200 from {url}: {r.status_code}\")\n",
    "    except Exception as e:\n",
    "        print(f\"[offchain] fetch error {url}: {e}\")\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3e9cc04a-9b89-46d8-90eb-0f95d5cfbc47",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_curve_status_min() -> list[dict]:\n",
    "    \"\"\"Minimal status fetch from Curve status page → single normalized event.\"\"\"\n",
    "    data = _http_get_json(\"https://status.curve.fi/api/v2/status.json\")\n",
    "    out = []\n",
    "    now = _now_iso()\n",
    "    if isinstance(data, dict) and \"status\" in data:\n",
    "        summary = data[\"status\"].get(\"description\") or data[\"status\"].get(\"indicator\", \"\")\n",
    "        out.append({\n",
    "            \"type\": \"status_update\",\n",
    "            \"severity\": 2,\n",
    "            \"ts\": now,\n",
    "            \"summary\": f\"Curve status: {summary}\",\n",
    "            \"source\": \"https://status.curve.fi\"\n",
    "        })\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "c348f810-64c8-436e-840b-4761bd4e3509",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fetch_curve_forum_min() -> list[dict]:\n",
    "    \"\"\"Placeholder/governance ping (keep it simple; real crawler can replace).\"\"\"\n",
    "    return [{\n",
    "        \"type\": \"governance_vote\",\n",
    "        \"severity\": 1,\n",
    "        \"ts\": _now_iso(),\n",
    "        \"summary\": \"Forum chatter: parameter tweak discussion (placeholder).\",\n",
    "        \"source\": \"https://gov.curve.fi/\"\n",
    "    }]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "8eab6b16-422d-4de5-bd3d-fa787a9bc322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_events_from_sources() -> list[dict]:\n",
    "    \"\"\"Fetch events from a few off-chain sources, dedupe, persist to EVENTS_JSON.\"\"\"\n",
    "    ev = _load_events()\n",
    "    try:\n",
    "        ev += fetch_curve_status_min()\n",
    "    except Exception as e:\n",
    "        print(f\"[offchain] curve status skipped: {e}\")\n",
    "    try:\n",
    "        ev += fetch_curve_forum_min()\n",
    "    except Exception as e:\n",
    "        print(f\"[offchain] curve forum skipped: {e}\")\n",
    "    uniq = {}\n",
    "    for e in ev:\n",
    "        key = (str(e.get(\"summary\", \"\"))[:120], str(e.get(\"source\", \"\")), (str(e.get(\"ts\", \"\")) or \"\")[:16])\n",
    "        if key not in uniq:\n",
    "            uniq[key] = {\n",
    "                \"type\": str(e.get(\"type\", \"information\")),\n",
    "                \"severity\": int(e.get(\"severity\", 1)),\n",
    "                \"ts\": str(e.get(\"ts\") or _now_iso()),\n",
    "                \"summary\": str(e.get(\"summary\", \"\"))[:280],\n",
    "                \"source\": str(e.get(\"source\", \"\")),\n",
    "            }\n",
    "    ev2 = list(uniq.values())\n",
    "    _save_events(ev2)\n",
    "    print(f\"[offchain] events.json updated: {len(ev2)} events\")\n",
    "    return ev2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "7cc39646-804c-41dd-bfe3-130e88d09db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Anaconda3\\Lib\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    SEV_MODEL  \n",
    "except NameError:\n",
    "    try:\n",
    "        SEV_MODEL = SeverityModel()\n",
    "    except Exception:\n",
    "        class _DummySev:\n",
    "            def fit(self, *_, **__): return self\n",
    "            def predict(self, texts, threshold: float = 0.5): return [1 if t else 0 for t in texts]\n",
    "        SEV_MODEL = _DummySev()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "93c376a9-b93e-481f-a502-a38eb07a173a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enrich_events_and_aggregate():\n",
    "    \"\"\"\n",
    "    - Loads EVENTS_JSON (creates a tiny mock if empty)\n",
    "    - Fits severity model and writes back 'severity' labels\n",
    "    - Updates live tail rows per pool with event_count_24h & event_severity_max_24h\n",
    "    \"\"\"\n",
    "    ev = _load_events()\n",
    "    if not ev:\n",
    "        ev = [{\n",
    "            \"type\": \"status_update\",\n",
    "            \"severity\": 2,\n",
    "            \"ts\": _now_iso(),\n",
    "            \"summary\": \"noisy markets on stables; monitoring.\",\n",
    "            \"source\": \"mock://status\"\n",
    "        }]\n",
    "        _save_events(ev)\n",
    "    texts = [e.get(\"summary\", \"\") for e in ev]\n",
    "    y_boot = [int(e.get(\"severity\", 2)) for e in ev]  \n",
    "    try:\n",
    "        SEV_MODEL.fit(texts, y_boot)\n",
    "        sev_pred = SEV_MODEL.predict(texts)\n",
    "    except Exception as e:\n",
    "        print(f\"[note] severity fit skipped: {e}\")\n",
    "        sev_pred = [int(bool(y)) for y in y_boot]\n",
    "    for i, s in enumerate(sev_pred):\n",
    "        try:\n",
    "            ev[i][\"severity\"] = int(s)\n",
    "        except Exception:\n",
    "            ev[i][\"severity\"] = int(y_boot[i]) if i < len(y_boot) else 1\n",
    "    _save_events(ev)\n",
    "    df = load_live()\n",
    "    if df.empty:\n",
    "        print(\"[enrich] live empty; nothing to write\")\n",
    "        return None\n",
    "    max_sev = int(max([e.get(\"severity\", 0) for e in ev]) if ev else 0)\n",
    "    cnt = int(len(ev))\n",
    "    df_latest = df.sort_values(\"ts\").groupby(\"pool\", as_index=False).tail(1).copy()\n",
    "    df_latest[\"event_severity_max_24h\"] = max_sev\n",
    "    df_latest[\"event_count_24h\"] = cnt\n",
    "    live = load_live()\n",
    "    live[\"ts_str\"] = _iso_str(live[\"ts\"])\n",
    "    df_latest[\"ts_str\"] = _iso_str(df_latest[\"ts\"])\n",
    "    live_idx = live.set_index([\"pool\", \"ts_str\"])\n",
    "    upd_idx  = df_latest.set_index([\"pool\", \"ts_str\"])[[\"event_severity_max_24h\", \"event_count_24h\"]]\n",
    "    live_idx.update(upd_idx)\n",
    "    live_updated = live_idx.reset_index().drop(columns=[\"ts_str\"])\n",
    "    write_live(live_updated)\n",
    "    print(\"[enrich] wrote event aggregates to live tail rows\")\n",
    "    return df_latest[[\"ts\",\"pool\",\"event_severity_max_24h\",\"event_count_24h\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05040d96-20c7-4f47-8d02-407f0808079b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_analyst_note_v2() -> dict:\n",
    "    \"\"\"\n",
    "    Creates an analyst note PDF with:\n",
    "      - current fused anomaly (risk_now)\n",
    "      - 10-min forecast probability\n",
    "      - top contributors from explain.json\n",
    "      - cross-pool propagation cue\n",
    "      - freshness + confidence band\n",
    "    \"\"\"\n",
    "    tail = load_live().sort_values(\"ts\").groupby(\"pool\").tail(1)\n",
    "    if tail.empty:\n",
    "        return {\"ok\": False, \"reason\": \"no data\"}\n",
    "    try:\n",
    "        explain = json.loads(EXPLAIN_JSON.read_text()) if EXPLAIN_JSON.exists() else explain_forecast_10m()\n",
    "        top3 = explain.get(\"top_contributors\", [])[:3]\n",
    "    except Exception:\n",
    "        top3 = []\n",
    "    cue = None\n",
    "    try:\n",
    "        net = compute_network_features()\n",
    "        if net is not None and not net.empty:\n",
    "            nrow = net.sort_values(\"neighbor_avg_anom\", ascending=False).iloc[0]\n",
    "            lead_str = \"leads\" if nrow[\"lead_lag_best\"] > 0 else (\"lags\" if nrow[\"lead_lag_best\"] < 0 else \"co-moves\")\n",
    "            cue = f\"{nrow['pool']} {lead_str} peers (corr={nrow['corr_best']:.2f}).\"\n",
    "    except Exception as e:\n",
    "        print(f\"[note] network features skipped in note: {e}\")\n",
    "    risk_now, p10 = 0.0, 0.0\n",
    "    try:\n",
    "        scored = score_latest_10m(n_tail=len(tail), write_parquet=False)\n",
    "        if not scored.empty:\n",
    "            rmax = scored.sort_values(\"risk_forecast_10m\", ascending=False).iloc[0]\n",
    "            risk_now = float(rmax.get(\"anom_fused\", 0.0))\n",
    "            p10 = float(rmax.get(\"risk_forecast_10m\", 0.0))\n",
    "    except Exception as e:\n",
    "        print(f\"[note] forecast in note skipped: {e}\")\n",
    "    band = \"High\" if p10 >= 0.60 or risk_now >= 0.90 else (\"Medium\" if p10 >= 0.35 or risk_now >= 0.70 else \"Low\")\n",
    "    freshness = \"Fresh\" if bool(tail[\"feeds_fresh\"].astype(bool).iloc[-1]) else \"Stale\"\n",
    "    note_lines = [\n",
    "        f\"Fused anomaly now={risk_now:.2f}; 10-min risk={p10:.2f}. Confidence {band}.\",\n",
    "    ]\n",
    "    if top3:\n",
    "        note_lines.append(\"Top drivers: \" + \"; \".join(top3))\n",
    "    if cue:\n",
    "        note_lines.append(\"Propagation: \" + cue)\n",
    "    note_lines.append(\"Action: monitor in 5m; if risk rises above 0.60 or 3 consecutive reds, engage mitigation (widen slippage / reroute).\")\n",
    "    note = \" \".join(note_lines)[:900]\n",
    "    pdf_path = export_analyst_note_pdf(\n",
    "        note_text=note,\n",
    "        risk_now=risk_now,\n",
    "        risk_10m=p10,\n",
    "        risk_30m=None,\n",
    "        contributors=top3,\n",
    "        freshness=freshness,\n",
    "        confidence=band,\n",
    "        out_path=OUT / \"analyst_note.pdf\",\n",
    "        title=\"Analyst Note v2\"\n",
    "    )\n",
    "    return {\"ok\": True, \"note\": note, \"pdf\": str(pdf_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "d5b41198-7311-48b9-9fdc-8c2479c12f93",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nightly_report() -> dict:\n",
    "    \"\"\"\n",
    "    Nightly Markdown report:\n",
    "      - winner detector by PR-AUC proxy\n",
    "      - 10m forecast calibration bins\n",
    "      - incident (red) summary\n",
    "    Exports to PDF as well.\n",
    "    \"\"\"\n",
    "    df = load_live()\n",
    "    if df.empty:\n",
    "        return {\"ok\": False, \"markdown\": \"[report] no data\"}\n",
    "    thr = 0.003\n",
    "    labels = (df[\"dev\"].abs() >= thr).astype(int).values\n",
    "    scores = {c: df[c].fillna(0.0).values for c in [\"z_if\",\"z_lof\",\"z_ocsvm\",\"z_cusum\",\"z_ae\",\"anom_fused\"] if c in df.columns}\n",
    "    ap = {}\n",
    "    if _SK_OK and labels.sum() > 0 and labels.sum() < len(labels):\n",
    "        try:\n",
    "            from sklearn.metrics import average_precision_score as AP\n",
    "            for k, s in scores.items():\n",
    "                ap[k] = AP(labels, s)\n",
    "        except Exception as e:\n",
    "            print(f\"[report] AP calc skipped: {e}\")\n",
    "    winner = max(ap.items(), key=lambda x: x[1])[0] if ap else \"anom_fused\"\n",
    "    if FORECAST_10M_PARQUET.exists():\n",
    "        try:\n",
    "            f = pd.read_parquet(FORECAST_10M_PARQUET)\n",
    "            if \"y_10m\" not in f.columns:\n",
    "                f = f.merge(df[[\"ts\",\"pool\",\"y_10m\"]] if \"y_10m\" in df.columns else pd.DataFrame(), on=[\"ts\",\"pool\"], how=\"left\")\n",
    "        except Exception as e:\n",
    "            print(f\"[report] parquet read failed: {e}\")\n",
    "            f = score_latest_10m(write_parquet=False)\n",
    "            f[\"y_10m\"] = _ensure_labels(df)\n",
    "    else:\n",
    "        f = score_latest_10m(write_parquet=False)\n",
    "        if \"y_10m\" not in df.columns:\n",
    "            df[\"y_10m\"] = _ensure_labels(df); write_live(df)\n",
    "        f[\"y_10m\"] = _ensure_labels(df)\n",
    "    bins = np.linspace(0, 1, 6)\n",
    "    f[\"bin\"] = np.digitize(f[\"risk_forecast_10m\"].fillna(0.0), bins)\n",
    "    calib = f.groupby(\"bin\", dropna=False).agg(\n",
    "        p=(\"risk_forecast_10m\", \"mean\"),\n",
    "        y=(\"y_10m\", \"mean\"),\n",
    "        n=(\"y_10m\", \"size\")\n",
    "    ).reset_index(drop=True)\n",
    "    try:\n",
    "        recent = df.sort_values(\"ts\").groupby(\"pool\").tail(min(100, len(df)))\n",
    "        dec = decide_latest(n_tail=min(100, len(recent)))\n",
    "        reds = dec[dec[\"level\"] == \"red\"] if isinstance(dec, pd.DataFrame) else pd.DataFrame()\n",
    "    except Exception as e:\n",
    "        print(f\"[report] decide_latest failed: {e}\")\n",
    "        reds = pd.DataFrame()\n",
    "    md = []\n",
    "    md.append(\"# Nightly Model Report\")\n",
    "    md.append(f\"Generated: {_now_iso()}\\n\")\n",
    "    md.append(\"## Winner Detector Today\")\n",
    "    md.append(f\"- **Winner:** `{winner}`  (by AP proxy on |dev|≥0.3%)\\n\")\n",
    "    if ap:\n",
    "        md.append(\"AP scores:\")\n",
    "        for k, v in sorted(ap.items(), key=lambda z: z[1], reverse=True):\n",
    "            md.append(f\"- {k}: {v:.3f}\")\n",
    "        md.append(\"\")\n",
    "    md.append(\"## Forecast Calibration (10m)\")\n",
    "    for _, r in calib.iterrows():\n",
    "        p = r[\"p\"] if pd.notna(r[\"p\"]) else 0.0\n",
    "        y = r[\"y\"] if pd.notna(r[\"y\"]) else 0.0\n",
    "        md.append(f\"- bin≈{p:.2f}: observed {y:.2f} (n={int(r['n'])})\")\n",
    "    md.append(\"\")\n",
    "    md.append(\"## Incidents (last window)\")\n",
    "    if isinstance(reds, pd.DataFrame) and not reds.empty:\n",
    "        for _, r in reds.iterrows():\n",
    "            an = r.get(\"anom_fused\", np.nan)\n",
    "            p10 = r.get(\"risk_forecast_10m\", np.nan)\n",
    "            md.append(f\"- {r['ts']} | {r['pool']} | fused={an:.2f} | p10={p10:.2f}\")\n",
    "    else:\n",
    "        md.append(\"- None\")\n",
    "    md_text = \"\\n\".join(md)\n",
    "    pdf_path = export_markdown_pdf(md_text, OUT / \"report.pdf\", title=\"Depeg Sentinel — Nightly Report\")\n",
    "    return {\"ok\": True, \"markdown\": md_text[:800] + \"...\", \"pdf\": str(pdf_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "75291a6a-1275-4cde-bebb-8179d23026a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_demo():\n",
    "    print(f\"[env] Output base: {OUT}\")\n",
    "    print(f\"[chain] mock_mode={CFG.mock_mode} rpc={CFG.eth_rpc}\")\n",
    "    sample_once()\n",
    "    run_anomaly_zoo_update_live()\n",
    "    df = load_live()\n",
    "    if \"y_10m\" not in df.columns or df[\"y_10m\"].nunique() < 2:\n",
    "        df[\"y_10m\"] = _ensure_labels(df); write_live(df)\n",
    "    if _XGB_OK and _SK_OK:\n",
    "        try:\n",
    "            train_forecaster_10m()\n",
    "            score_latest_10m()\n",
    "            explain_forecast_10m()\n",
    "        except Exception as e:\n",
    "            print(f\"[note] forecaster step skipped: {e}\")\n",
    "    else:\n",
    "        print(\"[note] skipping forecaster (xgboost or sklearn missing)\")\n",
    "    compute_network_features()\n",
    "    enrich_events_and_aggregate()\n",
    "    try:\n",
    "        dec = decide_latest(12); print(dec.tail(3))\n",
    "    except Exception as e:\n",
    "        print(f\"[note] decide_latest skipped: {e}\")\n",
    "    print(build_analyst_note_v2())\n",
    "    print(nightly_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "2596ae5d-7e9f-4c96-a4b1-2b3741c93703",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_run_meta(extra: dict | None = None, path: Path = OUT / \"RUN_META.json\") -> Path:\n",
    "    meta = {\n",
    "        \"ts\": _now_iso(),\n",
    "        \"versions\": {\"python\": sys.version.split()[0],\n",
    "                     \"numpy\": getattr(np, \"__version__\", None),\n",
    "                     \"pandas\": getattr(pd, \"__version__\", None),\n",
    "                     \"sklearn\": None, \"xgboost\": None, \"torch\": None, \"web3\": None},\n",
    "        \"config\": {\"eth_rpc\": CFG.eth_rpc, \"mock_mode\": CFG.mock_mode, \"lookback\": CFG.lookback, \"pools\": list(CFG.pools.keys())},\n",
    "        \"artifacts\": {\"live_csv\": str(LIVE_CSV), \"forecast_parquet\": str(FORECAST_10M_PARQUET), \"explain_json\": str(EXPLAIN_JSON), \"events_json\": str(EVENTS_JSON)},\n",
    "        \"counts\": {},\n",
    "    }\n",
    "    try: import sklearn; meta[\"versions\"][\"sklearn\"] = sklearn.__version__\n",
    "    except Exception: pass\n",
    "    try: import xgboost as _x; meta[\"versions\"][\"xgboost\"] = _x.__version__\n",
    "    except Exception: pass\n",
    "    try: import torch as _t; meta[\"versions\"][\"torch\"] = _t.__version__\n",
    "    except Exception: pass\n",
    "    try: from web3 import __version__ as _w3v; meta[\"versions\"][\"web3\"] = _w3v\n",
    "    except Exception: pass\n",
    "    df = load_live(); meta[\"counts\"][\"live_rows\"] = int(len(df)); meta[\"counts\"][\"pools\"] = int(df[\"pool\"].nunique()) if not df.empty else 0\n",
    "    if extra: meta.update(extra)\n",
    "    Path(path).write_text(json.dumps(meta, indent=2)); print(f\"[ok] RUN_META.json → {path}\"); return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f4b63728-dd06-4fa4-8781-c6d844c99be1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def main_demo_next_steps(webhook_url: str | None = None, use_rag: bool = False):\n",
    "    update_events_from_sources()\n",
    "    enrich_events_and_aggregate()\n",
    "    write_run_meta()\n",
    "    if webhook_url:\n",
    "        try:\n",
    "            trigger_alerts_if_needed(webhook_url)\n",
    "        except Exception as e:\n",
    "            print(f\"[alert] skipped: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "cbf56b6d-e3f7-4e09-b218-463011ec78c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "cafdcf44-d201-4689-a458-ed1f64521695",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[secrets] HF_TOKEN set for this session.\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"HF_TOKEN\"] = \"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\"\n",
    "print(\"[secrets] HF_TOKEN set for this session.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "5d18abf6-e9e4-4440-b6ad-e2bafb3a835d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Note: Environment variable`HF_TOKEN` is set and is the current active token independently from the token you've just configured.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[secrets] Hugging Face client logged in.\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import login\n",
    "login(token=os.environ[\"HF_TOKEN\"], add_to_git_credential=False)\n",
    "print(\"[secrets] Hugging Face client logged in.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "c671cc6b-09ac-402f-b896-c5d7b1103fd1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[secrets] HF_TOKEN loaded from environment.\n"
     ]
    }
   ],
   "source": [
    "HF_TOKEN = os.getenv(\"HF_TOKEN\") \n",
    "if HF_TOKEN:\n",
    "    print(\"[secrets] HF_TOKEN loaded from environment.\")\n",
    "else:\n",
    "    print(\"[secrets] HF_TOKEN not set (features that need HF may be disabled).\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "434477e1-50c7-4f24-b5fc-c5ce5ab5fe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Any"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "1d9df9e7-167a-47d1-86be-f7453a05bf33",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rpc_health_check(rpc: str) -> bool:\n",
    "    if not (_WEB3_OK and rpc):\n",
    "        return False\n",
    "    try:\n",
    "        w3 = Web3(Web3.HTTPProvider(rpc, request_kwargs={\"timeout\": 6}))\n",
    "        ok = w3.is_connected() if hasattr(w3, \"is_connected\") else w3.isConnected()\n",
    "        return bool(ok)\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "539aa7fb-b238-4048-a66b-2a41c98248b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ConfigProxy:\n",
    "    \"\"\"Guards ETH RPC config after init so you can't flip real/mock mid-run by accident.\"\"\"\n",
    "    def __init__(self, cfg_obj: Any):\n",
    "        object.__setattr__(self, \"_cfg\", cfg_obj)\n",
    "        object.__setattr__(self, \"_locked\", False)\n",
    "    def lock(self):  object.__setattr__(self, \"_locked\", True)\n",
    "    def unlock(self): object.__setattr__(self, \"_locked\", False)\n",
    "    def __getattr__(self, k): return getattr(object.__getattribute__(self, \"_cfg\"), k)\n",
    "    def __setattr__(self, k, v):\n",
    "        if object.__getattribute__(self, \"_locked\") and k in (\"eth_rpc\", \"mock_mode\"):\n",
    "            raise RuntimeError(f\"[rpc] Config is locked. Refusing to set {k}. Restart kernel or call CFG.unlock().\")\n",
    "        setattr(object.__getattribute__(self, \"_cfg\"), k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "39963a01-3de7-415e-bb5c-f91de456927b",
   "metadata": {},
   "outputs": [],
   "source": [
    "if not isinstance(CFG, _ConfigProxy):\n",
    "    CFG = _ConfigProxy(CFG)\n",
    "_RPC_INIT_DONE = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "eb2f43cc-92ba-47b7-8a7f-c5ba645ca832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def init_onchain_locked():\n",
    "    \"\"\"Initialize OnChainTool once, with health check, then lock RPC config.\"\"\"\n",
    "    global onchain, _RPC_INIT_DONE\n",
    "    if _RPC_INIT_DONE:\n",
    "        print(\"[rpc] Already initialized; not switching modes.\")\n",
    "        return onchain\n",
    "    env_rpc = os.getenv(\"ETH_RPC\")\n",
    "    if env_rpc and \"<YOUR_KEY>\" not in env_rpc and rpc_health_check(env_rpc):\n",
    "        CFG.eth_rpc = env_rpc\n",
    "        CFG.mock_mode = False\n",
    "        print(f\"[rpc] Using real RPC: {CFG.eth_rpc}\")\n",
    "    else:\n",
    "        CFG.mock_mode = True\n",
    "        print(\"[rpc] Real RPC not available/healthy → mock mode.\")\n",
    "    onchain = OnChainTool(CFG.eth_rpc, mock=CFG.mock_mode)\n",
    "    CFG.lock()\n",
    "    _RPC_INIT_DONE = True\n",
    "    return onchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "9e817c52-8fa4-4c3e-b35f-fd01f50b3029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rpc] Real RPC not available/healthy → mock mode.\n",
      "[onchain] mock_mode enabled\n"
     ]
    }
   ],
   "source": [
    "onchain = init_onchain_locked()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "a7a60a16-7866-4a6f-969d-397dbb3964c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _define_once(flag_name: str) -> bool:\n",
    "    \"\"\"Return True if we should define/patch now; False if already defined.\"\"\"\n",
    "    if globals().get(flag_name, False):\n",
    "        print(f\"[guard] {flag_name} already set — skipping redefinition.\")\n",
    "        return False\n",
    "    globals()[flag_name] = True\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "00041a6b-0f6f-490e-ac3e-16c79bd5adb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _define_once(\"_CUSUM_SCORE_DEFINED\"):\n",
    "    def _cusum_score(dev: pd.Series, w: int = 30, k: float = 0.0) -> pd.Series:\n",
    "        s = dev.fillna(0).astype(float).to_numpy()\n",
    "        g_pos = np.zeros_like(s, dtype=float); g_neg = np.zeros_like(s, dtype=float)\n",
    "        for i in range(1, len(s)):\n",
    "            g_pos[i] = max(0.0, g_pos[i-1] + (s[i] - k))\n",
    "            g_neg[i] = min(0.0, g_neg[i-1] + (s[i] + k))\n",
    "        mag = np.abs(g_pos) + np.abs(g_neg)\n",
    "        out = pd.Series(mag, index=dev.index)\n",
    "        out_rolled = out.rolling(w, min_periods=5).apply(\n",
    "            lambda z: z[-1] / (np.nanmax(np.abs(z)) + 1e-9),\n",
    "            raw=True\n",
    "        )\n",
    "        return out_rolled.fillna(0.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "381ee0db-a9ad-4fca-ba3d-60a3b18f34b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _define_once(\"_GET_UNIV3_PRICE_PATCHED\"):\n",
    "    def _get_univ3_price_precise(self, pool_addr: str) -> OnchainResult:\n",
    "        ts = _now_iso()\n",
    "        if self.mock or not pool_addr or pool_addr.lower().startswith(\"0x0000\"):\n",
    "            base = 1.0 + np.random.normal(0, 3e-4)\n",
    "            return OnchainResult(True, ts, None, {\"price\": float(base), \"sqrtPriceX96\": None, \"source\": \"mock\"})\n",
    "        try:\n",
    "            _ABI_UNIV3_POOL = [\n",
    "                {\"name\":\"slot0\",\"inputs\":[],\"outputs\":[\n",
    "                    {\"type\":\"uint160\",\"name\":\"sqrtPriceX96\"},\n",
    "                    {\"type\":\"int24\",\"name\":\"tick\"},\n",
    "                    {\"type\":\"uint16\",\"name\":\"observationIndex\"},\n",
    "                    {\"type\":\"uint16\",\"name\":\"observationCardinality\"},\n",
    "                    {\"type\":\"uint16\",\"name\":\"observationCardinalityNext\"},\n",
    "                    {\"type\":\"uint8\",\"name\":\"feeProtocol\"},\n",
    "                    {\"type\":\"bool\",\"name\":\"unlocked\"}],\n",
    "                 \"stateMutability\":\"view\",\"type\":\"function\"},\n",
    "                {\"name\":\"token0\",\"inputs\":[],\"outputs\":[{\"type\":\"address\"}], \"stateMutability\":\"view\",\"type\":\"function\"},\n",
    "                {\"name\":\"token1\",\"inputs\":[],\"outputs\":[{\"type\":\"address\"}], \"stateMutability\":\"view\",\"type\":\"function\"},\n",
    "            ]\n",
    "            _ABI_ERC20_DEC = [\n",
    "                {\"name\":\"decimals\",\"inputs\":[],\"outputs\":[{\"type\":\"uint8\"}], \"stateMutability\":\"view\",\"type\":\"function\"},\n",
    "                {\"name\":\"symbol\",\"inputs\":[],\"outputs\":[{\"type\":\"string\"}],   \"stateMutability\":\"view\",\"type\":\"function\"},\n",
    "            ]\n",
    "            pool = Web3.to_checksum_address(pool_addr)\n",
    "            c = self.w3.eth.contract(address=pool, abi=_ABI_UNIV3_POOL)\n",
    "            slot0 = self._call(lambda: c.functions.slot0().call()); sqrtp = slot0[0]\n",
    "            t0 = self._call(lambda: c.functions.token0().call()); t1 = self._call(lambda: c.functions.token1().call())\n",
    "            t0c = self.w3.eth.contract(address=t0, abi=_ABI_ERC20_DEC)\n",
    "            t1c = self.w3.eth.contract(address=t1, abi=_ABI_ERC20_DEC)\n",
    "            d0 = int(self._call(lambda: t0c.functions.decimals().call()))\n",
    "            d1 = int(self._call(lambda: t1c.functions.decimals().call()))\n",
    "            p_raw = (sqrtp / (2**96))**2\n",
    "            price = float(p_raw * (10 ** (d0 - d1)))\n",
    "            block = self.w3.eth.block_number\n",
    "            return OnchainResult(True, ts, block, {\n",
    "                \"price\": price, \"sqrtPriceX96\": int(sqrtp),\n",
    "                \"token0\": t0, \"token1\": t1, \"decimals0\": d0, \"decimals1\": d1, \"source\": \"uniswap_v3\"\n",
    "            })\n",
    "        except Exception as e:\n",
    "            return OnchainResult(False, ts, None, {}, str(e))\n",
    "    OnChainTool.get_univ3_price = _get_univ3_price_precise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "842fca11-9d8a-406e-8c58-6fa20d822a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _define_once(\"_MAIN_DEMO_DEFINED\"):\n",
    "    def main_demo():\n",
    "        print(f\"[env] Output base: {OUT}\")\n",
    "        print(f\"[chain] mock_mode={CFG.mock_mode} rpc={CFG.eth_rpc}\")\n",
    "        sample_once()\n",
    "        run_anomaly_zoo_update_live()\n",
    "        df = load_live()\n",
    "        if \"y_10m\" not in df.columns or df[\"y_10m\"].nunique() < 2:\n",
    "            df[\"y_10m\"] = _ensure_labels(df); write_live(df)\n",
    "        if _XGB_OK and _SK_OK:\n",
    "            try:\n",
    "                train_forecaster_10m()\n",
    "                score_latest_10m()\n",
    "                explain_forecast_10m()\n",
    "            except Exception as e:\n",
    "                print(f\"[note] forecaster step skipped: {e}\")\n",
    "        else:\n",
    "            print(\"[note] skipping forecaster (xgboost or sklearn missing)\")\n",
    "        compute_network_features()\n",
    "        enrich_events_and_aggregate()\n",
    "        try:\n",
    "            dec = decide_latest(12); print(dec.tail(3))\n",
    "        except Exception as e:\n",
    "            print(f\"[note] decide_latest skipped: {e}\")\n",
    "        print(build_analyst_note_v2())\n",
    "        print(nightly_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "73f7f209-b57d-4990-a2cb-d167b909f20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _define_once(\"_MAIN_DEMO_NEXT_DEFINED\"):\n",
    "    def main_demo_next_steps(webhook_url: str | None = None, use_rag: bool = False):\n",
    "        update_events_from_sources()\n",
    "        enrich_events_and_aggregate()\n",
    "        write_run_meta()\n",
    "        if webhook_url:\n",
    "            try:\n",
    "                trigger_alerts_if_needed(webhook_url)\n",
    "            except Exception as e:\n",
    "                print(f\"[alert] skipped: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "5a46edfb-a4be-4c89-868d-2e2c0506ec07",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _define_once(\"_SEVERITYMODEL_FIT_PATCHED\"):\n",
    "    def _sev_fit_patch(self, texts, labels):\n",
    "        if not texts:\n",
    "            warnings.warn(\"[warn] fit called with empty texts; skipping.\"); return self\n",
    "        if len(texts) != len(labels):\n",
    "            raise ValueError(\"texts and labels must have the same length\")\n",
    "        y = np.asarray(labels, dtype=int)\n",
    "        if getattr(self, \"emb\", None) is None:\n",
    "            p = float(np.mean(y)) if y.size else 0.0\n",
    "            self.model = {\"prior\": max(0.0, min(1.0, p))}\n",
    "            warnings.warn(\"[note] sentence-transformers missing; using prior-only severity.\"); return self\n",
    "        uniq = np.unique(y)\n",
    "        if not getattr(self, \"_sk_ok\", False) or uniq.size < 2:\n",
    "            p = float(np.mean(y)) if y.size else 0.0\n",
    "            self.model = {\"prior\": max(0.0, min(1.0, p))}\n",
    "            if not getattr(self, \"_sk_ok\", False):\n",
    "                warnings.warn(\"[note] sklearn missing; using prior-only severity.\")\n",
    "            elif uniq.size < 2:\n",
    "                warnings.warn(\"[note] one-class labels; using prior-only severity.\")\n",
    "            return self\n",
    "        X = self._embed(texts)\n",
    "        try:\n",
    "            clf = self._LR(max_iter=200); clf.fit(X, y); self.model = clf\n",
    "        except Exception as e:\n",
    "            p = float(np.mean(y))\n",
    "            self.model = {\"prior\": max(0.0, min(1.0, p))}\n",
    "            warnings.warn(f\"[note] LR fit failed: {e}. Using prior-only severity.\")\n",
    "        return self\n",
    "    SeverityModel.fit = _sev_fit_patch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "f350e11f-2960-48ab-98a1-77ddb57dab3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    _define_once\n",
    "except NameError:\n",
    "    def _define_once(flag_name: str) -> bool:\n",
    "        if globals().get(flag_name, False):\n",
    "            print(f\"[guard] {flag_name} already set — skipping.\")\n",
    "            return False\n",
    "        globals()[flag_name] = True\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "b98c939e-5dd3-42fb-ac47-90a1f0fc8ac2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[io] LIVE_CSV = C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\live_dataset.csv\n"
     ]
    }
   ],
   "source": [
    "if _define_once(\"_STORAGE_HELPERS_DEFINED\"):\n",
    "    import os\n",
    "    import pandas as pd\n",
    "    import numpy as np\n",
    "    from pathlib import Path\n",
    "    from datetime import datetime, timezone\n",
    "    OUT = Path(globals().get(\"OUT\", Path.cwd() / \"outputs\"))\n",
    "    OUT.mkdir(parents=True, exist_ok=True)\n",
    "    LIVE_CSV = globals().get(\"LIVE_CSV\", OUT / \"live_dataset.csv\")\n",
    "    CANON_COLS = globals().get(\"CANON_COLS\", [\n",
    "        \"ts\",\"pool\",\"dex_spot\",\"dex_twap\",\"oracle_ratio\",\"dev\",\"dev_roll_std\",\n",
    "        \"tvl_outflow_rate\",\"virtual_price\",\"spot_twap_gap_bps\",\"r0\",\"r1\",\"r0_delta\",\"r1_delta\",\n",
    "        \"event_severity_max_24h\",\"event_count_24h\",\"feeds_fresh\",\"run_quality_pass\",\"model_version\",\"block\",\n",
    "        \"z_if\",\"z_lof\",\"z_ocsvm\",\"z_cusum\",\"z_ae\",\"anom_fused\",\n",
    "        \"y_10m\",\n",
    "        \"neighbor_max_dev\",\"neighbor_avg_anom\",\"lead_lag_best\",\"corr_best\",\n",
    "    ])\n",
    "    def _now_iso() -> str:\n",
    "        return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "    def _iso_str(s) -> str:\n",
    "        ts = pd.to_datetime(s, errors=\"coerce\", utc=True)\n",
    "        if isinstance(ts, pd.Series):\n",
    "            return ts.dt.strftime(\"%Y-%m-%d %H:%M:%S%z\").fillna(\"\")\n",
    "        return ts.strftime(\"%Y-%m-%d %H:%M:%S%z\") if not pd.isna(ts) else \"\"\n",
    "    def ensure_live_schema(df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df = df.copy()\n",
    "        for col in CANON_COLS:\n",
    "            if col not in df.columns:\n",
    "                df[col] = np.nan\n",
    "        return df[CANON_COLS]\n",
    "    def write_live(df: pd.DataFrame) -> Path:\n",
    "        df2 = ensure_live_schema(df)\n",
    "        if \"ts\" in df2.columns:\n",
    "            df2[\"ts\"] = _iso_str(df2[\"ts\"])\n",
    "        LIVE_CSV.parent.mkdir(parents=True, exist_ok=True)\n",
    "        df2.to_csv(LIVE_CSV, index=False)\n",
    "        return LIVE_CSV\n",
    "    def load_live(n: int | None = None) -> pd.DataFrame:\n",
    "        if not LIVE_CSV.exists():\n",
    "            return pd.DataFrame(columns=CANON_COLS)\n",
    "        df = pd.read_csv(LIVE_CSV, dtype=str)\n",
    "        num_cols = [c for c in CANON_COLS if c not in (\"ts\",\"pool\",\"model_version\",\"block\")]\n",
    "        for c in num_cols:\n",
    "            if c in df.columns:\n",
    "                df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "        if \"ts\" in df.columns:\n",
    "            df[\"ts\"] = pd.to_datetime(df[\"ts\"], utc=True, errors=\"coerce\")\n",
    "        df = ensure_live_schema(df)\n",
    "        if n:\n",
    "            df = df.tail(n)\n",
    "        return df\n",
    "    def append_live(rows: pd.DataFrame) -> Path:\n",
    "        base = load_live()\n",
    "        merged = pd.concat([base, rows], ignore_index=True)\n",
    "        write_live(merged)\n",
    "        print(f\"[append] {len(rows)} rows → {LIVE_CSV}\")\n",
    "        return LIVE_CSV\n",
    "    print(f\"[io] LIVE_CSV = {LIVE_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2038cd31-2ff9-44cf-9f8c-ca350e16b96e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import deque\n",
    "if _define_once(\"_NETWORK_FEATURES_BLOCK_DEFINED\"):\n",
    "    _NET_WIN = globals().get(\"_NET_WIN\", 60)\n",
    "    _hist_dev  = globals().get(\"_hist_dev\", {})\n",
    "    _hist_anom = globals().get(\"_hist_anom\", {})\n",
    "    def _push_hist(d: dict, key: str, val: float, win: int = _NET_WIN):\n",
    "        q = d.get(key)\n",
    "        if q is None:\n",
    "            q = deque(maxlen=win)\n",
    "            d[key] = q\n",
    "        q.append(float(val) if pd.notna(val) else 0.0)\n",
    "    def _rolling_corr(x: np.ndarray, y: np.ndarray) -> float:\n",
    "        if len(x) < 3 or len(y) < 3 or len(x) != len(y):\n",
    "            return np.nan\n",
    "        x = np.asarray(x); y = np.asarray(y)\n",
    "        if x.std() == 0 or y.std() == 0:\n",
    "            return 0.0\n",
    "        return float(np.corrcoef(x, y)[0,1])\n",
    "    def _cross_corr_lag(x: np.ndarray, y: np.ndarray, max_lag: int = 10) -> tuple[float,int]:\n",
    "        if len(x) < 5 or len(y) < 5:\n",
    "            return np.nan, 0\n",
    "        x = np.asarray(x); y = np.asarray(y)\n",
    "        best, best_lag = -2.0, 0\n",
    "        for lag in range(-max_lag, max_lag+1):\n",
    "            if lag < 0:\n",
    "                xs, ys = x[:lag], y[-lag:]\n",
    "            elif lag > 0:\n",
    "                xs, ys = x[lag:], y[:-lag]\n",
    "            else:\n",
    "                xs, ys = x, y\n",
    "            if len(xs) < 5:\n",
    "                continue\n",
    "            c = _rolling_corr(xs, ys)\n",
    "            if np.isnan(c): \n",
    "                continue\n",
    "            if c > best:\n",
    "                best, best_lag = c, lag\n",
    "        return float(best), int(best_lag)\n",
    "    def compute_network_features() -> pd.DataFrame:\n",
    "        \"\"\"Compute neighbor_max_dev / neighbor_avg_anom and best lead/lag correlation per pool.\n",
    "           Updates the last rows in live CSV and returns a small summary frame.\n",
    "        \"\"\"\n",
    "        if \"load_live\" not in globals() or \"write_live\" not in globals():\n",
    "            raise RuntimeError(\"load_live/write_live not found — run the storage helpers patch first.\")\n",
    "        df = load_live().copy()\n",
    "        if df.empty:\n",
    "            print(\"[network] live is empty; skipping\")\n",
    "            return pd.DataFrame(columns=[\"ts\",\"pool\",\"neighbor_max_dev\",\"neighbor_avg_anom\",\"lead_lag_best\",\"corr_best\"])\n",
    "        for col in (\"pool\",\"dev\",\"anom_fused\",\"ts\"):\n",
    "            if col not in df.columns:\n",
    "                df[col] = np.nan\n",
    "        df = df.sort_values([\"ts\"])\n",
    "        for _, r in df.iterrows():\n",
    "            _push_hist(_hist_dev,  str(r[\"pool\"]), float(r.get(\"dev\", 0.0)))\n",
    "            _push_hist(_hist_anom, str(r[\"pool\"]), float(r.get(\"anom_fused\", 0.0)))\n",
    "        pools = list(df[\"pool\"].dropna().unique())\n",
    "        rows = []\n",
    "        for p in pools:\n",
    "            dev_p = list(_hist_dev.get(p, []))\n",
    "            neigh = [q for q in pools if q != p]\n",
    "            neigh_max_dev  = 0.0\n",
    "            neigh_avg_anom = 0.0\n",
    "            lead_lag_best  = 0\n",
    "            corr_best      = 0.0\n",
    "            for q in neigh:\n",
    "                dev_q = list(_hist_dev.get(q, []))\n",
    "                an_q  = list(_hist_anom.get(q, []))\n",
    "                if dev_q:\n",
    "                    neigh_max_dev = max(neigh_max_dev, float(np.nanmax(np.abs(dev_q))))\n",
    "                if an_q:\n",
    "                    neigh_avg_anom += float(np.nanmean(an_q))\n",
    "                if dev_p and dev_q:\n",
    "                    c, lag = _cross_corr_lag(np.array(dev_p), np.array(dev_q), max_lag=10)\n",
    "                    if np.isfinite(c) and abs(c) > abs(corr_best):\n",
    "                        corr_best, lead_lag_best = c, lag\n",
    "            if len(neigh) > 0:\n",
    "                neigh_avg_anom /= len(neigh)\n",
    "            rows.append({\n",
    "                \"pool\": p,\n",
    "                \"neighbor_max_dev\": float(neigh_max_dev),\n",
    "                \"neighbor_avg_anom\": float(neigh_avg_anom),\n",
    "                \"lead_lag_best\": int(lead_lag_best),\n",
    "                \"corr_best\": float(corr_best),\n",
    "            })\n",
    "        net = pd.DataFrame(rows)\n",
    "        latest = df.groupby(\"pool\", as_index=False).tail(1).copy()\n",
    "        for col, default in [\n",
    "            (\"neighbor_max_dev\", np.nan),\n",
    "            (\"neighbor_avg_anom\", np.nan),\n",
    "            (\"lead_lag_best\", 0),\n",
    "            (\"corr_best\", np.nan),\n",
    "        ]:\n",
    "            if col not in latest.columns:\n",
    "                latest[col] = default\n",
    "        if not net.empty:\n",
    "            latest = latest.drop(columns=[\"neighbor_max_dev\",\"neighbor_avg_anom\",\"lead_lag_best\",\"corr_best\"], errors=\"ignore\")\n",
    "            latest = latest.merge(net, on=\"pool\", how=\"left\")\n",
    "            latest[\"neighbor_max_dev\"]  = latest[\"neighbor_max_dev\"].astype(float)\n",
    "            latest[\"neighbor_avg_anom\"] = latest[\"neighbor_avg_anom\"].astype(float)\n",
    "            latest[\"lead_lag_best\"]     = latest[\"lead_lag_best\"].fillna(0).astype(int)\n",
    "            latest[\"corr_best\"]         = latest[\"corr_best\"].astype(float)\n",
    "        live = load_live()\n",
    "        live[\"ts_str\"]   = _iso_str(live[\"ts\"])\n",
    "        latest[\"ts_str\"] = _iso_str(latest[\"ts\"])\n",
    "        live_idx = live.set_index([\"pool\",\"ts_str\"])\n",
    "        upd_idx  = latest.set_index([\"pool\",\"ts_str\"])[[\"neighbor_max_dev\",\"neighbor_avg_anom\",\"lead_lag_best\",\"corr_best\"]]\n",
    "        live_idx.update(upd_idx)\n",
    "        live_updated = live_idx.reset_index().drop(columns=[\"ts_str\"])\n",
    "        write_live(live_updated)\n",
    "        print(\"[network] updated neighbor features & corr/lag on last rows\")\n",
    "        return latest[[\"ts\",\"pool\",\"neighbor_max_dev\",\"neighbor_avg_anom\",\"lead_lag_best\",\"corr_best\"]].sort_values(\"pool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "384a7ee6-31b6-489f-9350-12540af6743c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "afda5871-2ae8-4b5c-aed6-a0cf712f4787",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _define_once(\"_EXPLAIN_FORECAST_PATCHED\"):\n",
    "    def explain_forecast_10m(feature_cols: list | None = None, n_repeats: int = 8) -> dict:\n",
    "        \"\"\"Permutation-importance on the 10m forecaster; robust to NaNs & one-class edge cases.\"\"\"\n",
    "        if not globals().get(\"_SK_OK\", False):\n",
    "            payload = {\"ts\": _now_iso(), \"top_contributors\": [], \"all_features\": []}\n",
    "            EXPLAIN_JSON.write_text(json.dumps(payload, indent=2))\n",
    "            print(f\"[explain] wrote {EXPLAIN_JSON}\")\n",
    "            return payload\n",
    "        df = load_live()\n",
    "        if df.empty:\n",
    "            raise ValueError(\"live_dataset is empty.\")\n",
    "        if \"y_10m\" not in df.columns or df[\"y_10m\"].nunique(dropna=True) < 2:\n",
    "            df[\"y_10m\"] = _ensure_labels(df)\n",
    "            write_live(df)\n",
    "        if feature_cols is None:\n",
    "            feature_cols = [\n",
    "                \"dev\",\"dev_roll_std\",\"tvl_outflow_rate\",\"spot_twap_gap_bps\",\n",
    "                \"oracle_ratio\",\"anom_fused\",\"r0_delta\",\"r1_delta\",\n",
    "                \"event_severity_max_24h\",\"event_count_24h\"\n",
    "            ]\n",
    "        use_cols = [c for c in feature_cols if c in df.columns]\n",
    "        X = df[use_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0).to_numpy()\n",
    "        y = pd.to_numeric(df[\"y_10m\"], errors=\"coerce\").fillna(0).astype(int).to_numpy()\n",
    "        tr_idx, te_idx = _time_split_idx(df[\"ts\"], 0.70)\n",
    "        if te_idx.size == 0:\n",
    "            payload = {\"ts\": _now_iso(), \"top_contributors\": [], \"all_features\": []}\n",
    "            EXPLAIN_JSON.write_text(json.dumps(payload, indent=2))\n",
    "            print(\"[explain] not enough test data; wrote empty payload.\")\n",
    "            return payload\n",
    "        Xte, yte = X[te_idx], y[te_idx]\n",
    "        clf, calib = _load_forecaster()\n",
    "        if clf is None and calib is None:\n",
    "            _ = train_forecaster_10m(feature_cols=use_cols)\n",
    "            clf, calib = _load_forecaster()\n",
    "        model = calib if calib is not None else clf\n",
    "        if len(np.unique(yte)) < 2:\n",
    "            payload = {\"ts\": _now_iso(), \"top_contributors\": [], \"all_features\": []}\n",
    "            EXPLAIN_JSON.write_text(json.dumps(payload, indent=2))\n",
    "            print(\"[explain] one-class test labels; wrote empty payload.\")\n",
    "            return payload\n",
    "        from sklearn.inspection import permutation_importance\n",
    "        r = permutation_importance(model, Xte, yte, n_repeats=n_repeats, scoring=\"average_precision\", random_state=42)\n",
    "        imp = sorted(zip(use_cols, r.importances_mean, r.importances_std), key=lambda z: z[1], reverse=True)\n",
    "        top3 = [f\"{name} (+{mean:.4f}±{std:.4f} AP)\" for name, mean, std in imp[:3]]\n",
    "        payload = {\"ts\": _now_iso(), \"top_contributors\": top3, \"all_features\": [{\"feature\": n, \"mean\": float(m), \"std\": float(s)} for n,m,s in imp]}\n",
    "        EXPLAIN_JSON.write_text(json.dumps(payload, indent=2))\n",
    "        print(f\"[explain] wrote {EXPLAIN_JSON}\")\n",
    "        return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "273db9d9-6e9b-4cfb-9e73-0ea4730d97e8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[run] sample → zoo → train/score → explain → reports\n",
      "[rpc] Already initialized; not switching modes.\n",
      "[env] Output base: C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\n",
      "[chain] mock_mode=True rpc=https://cloudflare-eth.com\n",
      "[append] 3 rows → C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\live_dataset.csv\n",
      "[zoo] anomaly scores updated\n",
      "[note] forecaster step skipped: Invalid classes inferred from unique values of `y`.  Expected: [0], got [1]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\1076424455.py:109: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\1076424455.py:109: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\1076424455.py:109: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\1076424455.py:109: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[network] updated neighbor features & corr/lag on last rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\2931418067.py:45: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\2931418067.py:45: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[enrich] wrote event aggregates to live tail rows\n",
      "[note] decide_latest skipped: name 'decide_latest' is not defined\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\1076424455.py:109: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\1076424455.py:109: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\1076424455.py:109: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\1076424455.py:109: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[network] updated neighbor features & corr/lag on last rows\n",
      "[ok] Analyst Note exported → C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\analyst_note.pdf\n",
      "{'ok': True, 'note': 'Fused anomaly now=1.00; 10-min risk=0.62. Confidence High. Top drivers: dev_roll_std (+0.0654±0.0167 AP); oracle_ratio (+0.0201±0.0102 AP); r1_delta (+0.0012±0.0012 AP) Propagation: DAI/USDC_univ3 leads peers (corr=0.96). Action: monitor in 5m; if risk rises above 0.60 or 3 consecutive reds, engage mitigation (widen slippage / reroute).', 'pdf': 'C:\\\\Users\\\\aniru\\\\OneDrive\\\\Desktop\\\\ML tutorial\\\\DEFI Depeg sentinel\\\\analyst_note.pdf'}\n",
      "[report] decide_latest failed: name 'decide_latest' is not defined\n",
      "[ok] Markdown PDF exported → C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\report.pdf\n",
      "{'ok': True, 'markdown': '# Nightly Model Report\\nGenerated: 2025-08-28T20:49:17+00:00\\n\\n## Winner Detector Today\\n- **Winner:** `z_if`  (by AP proxy on |dev|≥0.3%)\\n\\nAP scores:\\n- z_if: 0.778\\n- z_ae: 0.710\\n- z_ocsvm: 0.513\\n- z_lof: 0.301\\n- anom_fused: 0.166\\n- z_cusum: 0.166\\n\\n## Forecast Calibration (10m)\\n- bin≈0.56: observed 1.00 (n=16)\\n- bin≈0.70: observed 1.00 (n=20)\\n- bin≈0.87: observed 1.00 (n=24)\\n\\n## Incidents (last window)\\n- None...', 'pdf': 'C:\\\\Users\\\\aniru\\\\OneDrive\\\\Desktop\\\\ML tutorial\\\\DEFI Depeg sentinel\\\\report.pdf'}\n",
      "[offchain] fetch error https://status.curve.fi/api/v2/status.json: HTTPSConnectionPool(host='status.curve.finance', port=443): Max retries exceeded with url: /api/v2/status.json (Caused by NameResolutionError(\"<urllib3.connection.HTTPSConnection object at 0x000001CDE3E27F50>: Failed to resolve 'status.curve.finance' ([Errno 11001] getaddrinfo failed)\"))\n",
      "[offchain] events.json updated: 89 events\n",
      "[enrich] wrote event aggregates to live tail rows\n",
      "[ok] RUN_META.json → C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\RUN_META.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\2931418067.py:45: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\2931418067.py:45: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n"
     ]
    }
   ],
   "source": [
    "print(\"[run] sample → zoo → train/score → explain → reports\")\n",
    "onchain = init_onchain_locked()  \n",
    "main_demo()\n",
    "main_demo_next_steps(webhook_url=None, use_rag=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "81c0683f-1286-4ab6-9ea6-fa9f03368d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "FORECAST_30M_PATH = OUT_MODEL / \"forecast_30m_xgb.joblib\"\n",
    "CALIB_30M_PATH    = OUT_MODEL / \"forecast_30m_calib.joblib\"\n",
    "FORECAST_30M_PARQUET = OUT / \"forecast_30m.parquet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "98da4cbf-2e8a-4512-bdfb-14e9007f5739",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"y_30m\" not in CANON_COLS:\n",
    "    CANON_COLS.insert(CANON_COLS.index(\"y_10m\")+1, \"y_30m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "3ab33cda-8e1a-4daa-b8de-e17d245ef40b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_demo():\n",
    "    print(f\"[env] Output base: {OUT}\")\n",
    "    print(f\"[chain] mock_mode={CFG.mock_mode} rpc={CFG.eth_rpc}\")\n",
    "    rollover_if_needed()\n",
    "    sample_once()\n",
    "    run_anomaly_zoo_update_live()\n",
    "    df = load_live()\n",
    "    df = ensure_targets_all(df) \n",
    "    if _XGB_OK and _SK_OK:\n",
    "        try:\n",
    "            train_forecaster_10m()\n",
    "            score_latest_10m()\n",
    "            explain_forecast_10m()\n",
    "            train_forecaster_30m()\n",
    "            score_latest_30m()\n",
    "        except Exception as e:\n",
    "            print(f\"[note] forecaster step skipped: {e}\")\n",
    "    else:\n",
    "        print(\"[note] skipping forecaster (xgboost or sklearn missing)\")\n",
    "    compute_network_features()\n",
    "    enrich_events_and_aggregate()\n",
    "    try:\n",
    "        dec = decide_latest(12)\n",
    "        print(dec.tail(3))\n",
    "    except Exception as e:\n",
    "        print(f\"[note] decide_latest skipped: {e}\")\n",
    "    print(build_analyst_note_v2())\n",
    "    print(nightly_report())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "93478cf6-e010-417b-bfe2-e68a1a728d84",
   "metadata": {},
   "outputs": [],
   "source": [
    "def rotate_live_to_archive(today_utc: Optional[pd.Timestamp] = None) -> dict:\n",
    "    \"\"\"\n",
    "    Move rows with ts date < today_utc to archive/live_YYYYMMDD.csv (append).\n",
    "    Keep only today's rows in LIVE_CSV.\n",
    "    \"\"\"\n",
    "    if not LIVE_CSV.exists():\n",
    "        return {\"moved\": 0, \"kept\": 0, \"files\": []}\n",
    "    try:\n",
    "        df = load_live()  \n",
    "    except Exception as e:\n",
    "        print(f\"[archive] load failed: {e}\")\n",
    "        return {\"moved\": 0, \"kept\": 0, \"files\": []}\n",
    "    if df.empty or \"ts\" not in df.columns:\n",
    "        return {\"moved\": 0, \"kept\": 0, \"files\": []}\n",
    "    today = (today_utc or pd.Timestamp.utcnow()).normalize()\n",
    "    dts = df[\"ts\"].dt.normalize()\n",
    "    old_mask = dts < today\n",
    "    if not old_mask.any():\n",
    "        return {\"moved\": 0, \"kept\": int((~old_mask).sum()), \"files\": []}\n",
    "    moved = int(old_mask.sum())\n",
    "    kept = int((~old_mask).sum())\n",
    "    files = []\n",
    "    df_old = df.loc[old_mask].copy()\n",
    "    df_old[\"date\"] = df_old[\"ts\"].dt.strftime(\"%Y%m%d\")\n",
    "    for d, chunk in df_old.groupby(\"date\"):\n",
    "        path = OUT_ARCHIVE / f\"live_{d}.csv\"\n",
    "        header = not path.exists()\n",
    "        ensure_live_schema(chunk.drop(columns=[\"date\"], errors=\"ignore\")).to_csv(path, index=False, header=header, mode=\"a\", lineterminator=\"\\n\")\n",
    "        files.append(str(path))\n",
    "    df_new = df.loc[~old_mask].copy()\n",
    "    write_live(df_new)\n",
    "    print(f\"[archive] moved={moved} kept={kept} files={len(files)}\")\n",
    "    return {\"moved\": moved, \"kept\": kept, \"files\": files}\n",
    "def rollover_if_needed():\n",
    "    try:\n",
    "        return rotate_live_to_archive()\n",
    "    except Exception as e:\n",
    "        print(f\"[archive] rollover skipped: {e}\")\n",
    "        return {\"moved\": 0, \"kept\": 0, \"files\": []}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "cc5676ad-d577-4c8a-b9d4-23c8d807d926",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_targets_all(df: pd.DataFrame | None = None, base_thr: float = 0.005) -> pd.DataFrame:\n",
    "    df = load_live() if df is None else df.copy()\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = df.sort_values([\"pool\",\"ts\"]).reset_index(drop=True)\n",
    "    if \"y_10m\" not in df.columns or df[\"y_10m\"].isna().any() or df[\"y_10m\"].nunique(dropna=True) < 2:\n",
    "        df[\"y_10m\"] = _ensure_labels(df, base_thr=base_thr)\n",
    "    if \"y_30m\" not in df.columns or df[\"y_30m\"].isna().any():\n",
    "        df[\"y_30m\"] = label_targets(\n",
    "            df, horizon=30, dev_thr=base_thr, fused_col=\"anom_fused\" if \"anom_fused\" in df.columns else None\n",
    "        ).astype(int)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "e93523ea-213e-4a99-b089-d1b7048531cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\578213378.py:14: RuntimeWarning: All-NaN slice encountered\n",
      "  cond_dev = np.nanmax(g_dev[j1:j2])\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\578213378.py:14: RuntimeWarning: All-NaN slice encountered\n",
      "  cond_dev = np.nanmax(g_dev[j1:j2])\n"
     ]
    }
   ],
   "source": [
    "df = load_live()\n",
    "df = ensure_targets_all(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "39dc47e6-87f9-4bec-9524-361227e3147c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_forecaster_30m():\n",
    "    clf = load(FORECAST_30M_PATH) if FORECAST_30M_PATH.exists() else None\n",
    "    calib = load(CALIB_30M_PATH) if CALIB_30M_PATH.exists() else None\n",
    "    return clf, calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "8c32f7b1-c749-4d96-9e5a-9d8c87263617",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_forecaster_30m(feature_cols: List[str] | None = None, label_col: str = \"y_30m\"):\n",
    "    if not _XGB_OK or not _SK_OK: raise RuntimeError(\"xgboost + scikit-learn required to train 30m forecaster.\")\n",
    "    df = load_live().copy()\n",
    "    if df.empty: raise ValueError(\"live_dataset is empty. sample more rows first.\")\n",
    "    df = ensure_targets_all(df)\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [\"dev\",\"dev_roll_std\",\"tvl_outflow_rate\",\"spot_twap_gap_bps\",\"oracle_ratio\",\n",
    "                        \"anom_fused\",\"r0_delta\",\"r1_delta\",\"event_severity_max_24h\",\"event_count_24h\"]\n",
    "    use_cols = [c for c in feature_cols if c in df.columns]\n",
    "    X = df[use_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0).to_numpy()\n",
    "    y = pd.to_numeric(df[label_col], errors=\"coerce\").fillna(0).astype(int).to_numpy()\n",
    "    ts = df[\"ts\"]; tr_idx, te_idx = _time_split_idx(ts, 0.70)\n",
    "    Xtr, Xte = X[tr_idx], X[te_idx]; ytr, yte = y[tr_idx], y[te_idx]\n",
    "    clf = xgb.XGBClassifier(n_estimators=300, max_depth=4, learning_rate=0.06,\n",
    "                            subsample=0.9, colsample_bytree=0.8,\n",
    "                            objective=\"binary:logistic\", eval_metric=\"logloss\",\n",
    "                            random_state=SEED, n_jobs=0)\n",
    "    clf.fit(Xtr, ytr)\n",
    "    calib = None\n",
    "    unique, counts = np.unique(ytr, return_counts=True)\n",
    "    min_cls = min(counts) if len(counts)==2 else 0\n",
    "    if min_cls >= 3:\n",
    "        try: calib = CalibratedClassifierCV(clf, method=\"isotonic\", cv=3).fit(Xtr, ytr)\n",
    "        except Exception: calib = None\n",
    "    if calib is None and min_cls >= 2:\n",
    "        try: calib = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=2).fit(Xtr, ytr)\n",
    "        except Exception: calib = None\n",
    "    def _proba(model, X_): return (model or clf).predict_proba(X_)[:,1]\n",
    "    ap, bs = float(\"nan\"), float(\"nan\")\n",
    "    if len(np.unique(yte))>1:\n",
    "        try: ap = average_precision_score(yte, _proba(calib, Xte))\n",
    "        except Exception: pass\n",
    "        try: bs = float(brier_score_loss(yte, _proba(calib, Xte)))\n",
    "        except Exception: bs = float(\"nan\")\n",
    "    print(f\"[forecast30m] AP={ap if pd.notna(ap) else float('nan'):.3f}  Brier={bs if pd.notna(bs) else float('nan'):.3f}  n_te={len(yte)}  calib={'iso3' if (calib and getattr(calib,'method','')=='isotonic') else ('sig2' if calib else 'none')}\")\n",
    "    dump(clf, FORECAST_30M_PATH)\n",
    "    if calib: dump(calib, CALIB_30M_PATH)\n",
    "    elif CALIB_30M_PATH.exists(): CALIB_30M_PATH.unlink()\n",
    "    tail = df.tail(6).copy()\n",
    "    X_tail = tail[use_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0).to_numpy()\n",
    "    tail[\"risk_forecast_30m\"] = _proba(calib, X_tail)\n",
    "    return tail[[\"ts\",\"pool\",\"anom_fused\",\"risk_forecast_30m\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "4670d240-520e-4fbe-ad45-420f73f7fa8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_latest_30m(n_tail: int = 60, feature_cols: Sequence[str] | None = None, write_parquet: bool = True) -> pd.DataFrame:\n",
    "    df = load_live()\n",
    "    if df.empty: raise ValueError(\"live_dataset is empty. Run sample_once() first.\")\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [\"dev\",\"dev_roll_std\",\"tvl_outflow_rate\",\"spot_twap_gap_bps\",\"oracle_ratio\",\n",
    "                        \"anom_fused\",\"r0_delta\",\"r1_delta\",\"event_severity_max_24h\",\"event_count_24h\"]\n",
    "    use_cols = [c for c in feature_cols if c in df.columns]\n",
    "    X_tail = df.tail(n_tail)[use_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0).to_numpy()\n",
    "    clf, calib = _load_forecaster_30m()\n",
    "    if clf is None and calib is None:\n",
    "        _ = train_forecaster_30m(feature_cols=list(use_cols))\n",
    "        clf, calib = _load_forecaster_30m()\n",
    "    model = calib if calib is not None else clf\n",
    "    p = model.predict_proba(X_tail)[:,1]\n",
    "    out = df.tail(n_tail).copy(); out[\"risk_forecast_30m\"] = p\n",
    "    out = out[[\"ts\",\"pool\",\"anom_fused\",\"risk_forecast_30m\"]]\n",
    "    if write_parquet:\n",
    "        try:\n",
    "            out.to_parquet(FORECAST_30M_PARQUET, index=False); print(f\"[forecast30m] wrote {FORECAST_30M_PARQUET}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] parquet write failed: {e}\")\n",
    "    return out.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "046651c1-a5d7-4133-95a8-c11d90002981",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[note] forecaster step skipped: Invalid classes inferred from unique values of `y`.  Expected: [0], got [1]\n"
     ]
    }
   ],
   "source": [
    "if _XGB_OK and _SK_OK:\n",
    "    try:\n",
    "        train_forecaster_10m(); score_latest_10m(); explain_forecast_10m()\n",
    "        train_forecaster_30m(); score_latest_30m()\n",
    "    except Exception as e:\n",
    "        print(f\"[note] forecaster step skipped: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "07f051b1-771a-40eb-84e6-9c6a5cd4da6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    scored30 = score_latest_30m(n_tail=len(tail), write_parquet=False)\n",
    "    if not scored30.empty:\n",
    "        rmax30 = scored30.sort_values(\"risk_forecast_30m\", ascending=False).iloc[0]\n",
    "        p30 = float(rmax30.get(\"risk_forecast_30m\", 0.0))\n",
    "    else:\n",
    "        p30 = 0.0\n",
    "except Exception:\n",
    "    p30 = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "7df1f727-f375-40b5-aabb-ee2d21baacbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_analyst_note_v2() -> dict:\n",
    "    \"\"\"Build a short analyst note + export PDF. No NameErrors, even if 30m model is missing.\"\"\"\n",
    "    import json\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    tail = load_live().sort_values(\"ts\").groupby(\"pool\").tail(1)\n",
    "    if tail.empty:\n",
    "        return {\"ok\": False, \"reason\": \"no data\"}\n",
    "    top3 = []\n",
    "    try:\n",
    "        if EXPLAIN_JSON.exists():\n",
    "            explain = json.loads(EXPLAIN_JSON.read_text())\n",
    "        else:\n",
    "            explain = explain_forecast_10m()\n",
    "        top3 = (explain or {}).get(\"top_contributors\", [])[:3]\n",
    "    except Exception:\n",
    "        pass\n",
    "    cue = None\n",
    "    try:\n",
    "        net = compute_network_features()\n",
    "        if isinstance(net, pd.DataFrame) and not net.empty:\n",
    "            nrow = net.sort_values(\"neighbor_avg_anom\", ascending=False).iloc[0]\n",
    "            lead_str = \"leads\" if nrow[\"lead_lag_best\"] > 0 else (\"lags\" if nrow[\"lead_lag_best\"] < 0 else \"co-moves\")\n",
    "            cue = f\"{nrow['pool']} {lead_str} peers (corr={float(nrow['corr_best']):.2f}).\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    risk_now = 0.0  \n",
    "    p10 = 0.0      \n",
    "    p30 = 0.0       \n",
    "    try:\n",
    "        n_tail = max(1, len(tail))\n",
    "        scored10 = score_latest_10m(n_tail=n_tail, write_parquet=False)\n",
    "        if isinstance(scored10, pd.DataFrame) and not scored10.empty:\n",
    "            rmax10 = scored10.sort_values(\"risk_forecast_10m\", ascending=False).iloc[0]\n",
    "            p10 = float(rmax10.get(\"risk_forecast_10m\", 0.0))\n",
    "            risk_now = float(rmax10.get(\"anom_fused\", 0.0))\n",
    "    except Exception:\n",
    "        pass\n",
    "    if not np.isfinite(risk_now) or risk_now == 0.0:\n",
    "        try:\n",
    "            risk_now = float(tail.get(\"anom_fused\", pd.Series([0.0])).iloc[-1])\n",
    "        except Exception:\n",
    "            risk_now = 0.0\n",
    "    try:\n",
    "        if \"score_latest_30m\" in globals():\n",
    "            n_tail = max(1, len(tail))\n",
    "            scored30 = score_latest_30m(n_tail=n_tail, write_parquet=False)\n",
    "            if isinstance(scored30, pd.DataFrame) and not scored30.empty:\n",
    "                rmax30 = scored30.sort_values(\"risk_forecast_30m\", ascending=False).iloc[0]\n",
    "                p30 = float(rmax30.get(\"risk_forecast_30m\", 0.0))\n",
    "    except Exception:\n",
    "        p30 = 0.0\n",
    "    band = (\n",
    "        \"High\" if (p10 >= 0.60 or p30 >= 0.60 or risk_now >= 0.90)\n",
    "        else (\"Medium\" if (p10 >= 0.35 or p30 >= 0.50 or risk_now >= 0.70) else \"Low\")\n",
    "    )\n",
    "    try:\n",
    "        freshness = \"Fresh\" if bool(tail[\"feeds_fresh\"].iloc[-1]) else \"Stale\"\n",
    "    except Exception:\n",
    "        freshness = \"Unknown\"\n",
    "    note_lines = [\n",
    "        f\"Fused anomaly now={risk_now:.2f}; 10-min risk={p10:.2f}; 30-min risk={p30:.2f}. Confidence {band}.\"\n",
    "    ]\n",
    "    if top3:\n",
    "        note_lines.append(\"Top drivers: \" + \"; \".join(top3))\n",
    "    if cue:\n",
    "        note_lines.append(\"Propagation: \" + cue)\n",
    "    note_lines.append(\n",
    "        \"Action: monitor in 5m; if risk > 0.60 or 3 consecutive reds, engage mitigation (widen slippage / reroute).\"\n",
    "    )\n",
    "    note = \" \".join(note_lines)[:900]\n",
    "    try:\n",
    "        pdf_path = export_analyst_note_pdf(\n",
    "            note_text=note,\n",
    "            risk_now=risk_now,\n",
    "            risk_10m=p10,\n",
    "            risk_30m=p30,\n",
    "            contributors=top3,\n",
    "            freshness=freshness,\n",
    "            confidence=band,\n",
    "            out_path=OUT / \"analyst_note.pdf\",\n",
    "            title=\"Analyst Note v2\"\n",
    "        )\n",
    "    except Exception:\n",
    "        pdf_path = OUT / \"analyst_note.txt\"\n",
    "        try:\n",
    "            pdf_path.write_text(note)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return {\"ok\": True, \"note\": note, \"pdf\": str(pdf_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "041e822d-1050-4476-9dc0-d2525139bfa9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\1076424455.py:109: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\1076424455.py:109: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\1076424455.py:109: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\1076424455.py:109: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[network] updated neighbor features & corr/lag on last rows\n",
      "[ok] Analyst Note exported → C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\analyst_note.pdf\n",
      "{'ok': True, 'note': 'Fused anomaly now=1.00; 10-min risk=0.62; 30-min risk=0.86. Confidence High. Top drivers: dev_roll_std (+0.0654±0.0167 AP); oracle_ratio (+0.0201±0.0102 AP); r1_delta (+0.0012±0.0012 AP) Propagation: DAI/USDC_univ3 leads peers (corr=0.96). Action: monitor in 5m; if risk > 0.60 or 3 consecutive reds, engage mitigation (widen slippage / reroute).', 'pdf': 'C:\\\\Users\\\\aniru\\\\OneDrive\\\\Desktop\\\\ML tutorial\\\\DEFI Depeg sentinel\\\\analyst_note.pdf'}\n"
     ]
    }
   ],
   "source": [
    "res = build_analyst_note_v2()\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "a3013bf4-781f-428b-9b4e-2c4faaa86b0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, numpy as np, pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "c48419d5-6893-4285-9299-cde963e30b63",
   "metadata": {},
   "outputs": [],
   "source": [
    "tail = load_live().sort_values(\"ts\").groupby(\"pool\").tail(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "7696c6fb-bafe-4937-8f31-8722bcc37c6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top3 = []\n",
    "try:\n",
    "    if EXPLAIN_JSON.exists():\n",
    "        explain = json.loads(EXPLAIN_JSON.read_text())\n",
    "    else:\n",
    "        explain = explain_forecast_10m()\n",
    "    top3 = (explain or {}).get(\"top_contributors\", [])[:3]\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "ecc68281-b8ef-4f3d-90ff-209f09879ff0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[network] updated neighbor features & corr/lag on last rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\1076424455.py:109: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\1076424455.py:109: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\1076424455.py:109: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\1076424455.py:109: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n"
     ]
    }
   ],
   "source": [
    "cue = None\n",
    "try:\n",
    "    net = compute_network_features()\n",
    "    if isinstance(net, pd.DataFrame) and not net.empty:\n",
    "        nrow = net.sort_values(\"neighbor_avg_anom\", ascending=False).iloc[0]\n",
    "        lead_str = \"leads\" if nrow[\"lead_lag_best\"] > 0 else (\"lags\" if nrow[\"lead_lag_best\"] < 0 else \"co-moves\")\n",
    "        cue = f\"{nrow['pool']} {lead_str} peers (corr={float(nrow['corr_best']):.2f}).\"\n",
    "except Exception:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "42ed2136-ef66-4f83-836e-cbb4e8ae92e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "risk_now = 0.0\n",
    "p10 = 0.0\n",
    "p30 = 0.0\n",
    "try:\n",
    "    n_tail = max(1, len(tail))\n",
    "    scored10 = score_latest_10m(n_tail=n_tail, write_parquet=False)\n",
    "    if isinstance(scored10, pd.DataFrame) and not scored10.empty:\n",
    "        rmax10 = scored10.sort_values(\"risk_forecast_10m\", ascending=False).iloc[0]\n",
    "        p10 = float(rmax10.get(\"risk_forecast_10m\", 0.0))\n",
    "        risk_now = float(rmax10.get(\"anom_fused\", 0.0))\n",
    "except Exception:\n",
    "    pass\n",
    "if not np.isfinite(risk_now) or risk_now == 0.0:\n",
    "    try:\n",
    "        risk_now = float(tail.get(\"anom_fused\", pd.Series([0.0])).iloc[-1])\n",
    "    except Exception:\n",
    "        risk_now = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "2894e8cf-10bc-486f-9c2b-07122a9503a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    if \"score_latest_30m\" in globals():\n",
    "        scored30 = score_latest_30m(n_tail=max(1, len(tail)), write_parquet=False)\n",
    "        if isinstance(scored30, pd.DataFrame) and not scored30.empty:\n",
    "            rmax30 = scored30.sort_values(\"risk_forecast_30m\", ascending=False).iloc[0]\n",
    "            p30 = float(rmax30.get(\"risk_forecast_30m\", 0.0))\n",
    "except Exception:\n",
    "    p30 = 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "id": "8e5ec38b-1255-4053-a0c5-9123a33bea3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "band = (\n",
    "    \"High\" if (p10 >= 0.60 or p30 >= 0.60 or risk_now >= 0.90)\n",
    "    else (\"Medium\" if (p10 >= 0.35 or p30 >= 0.50 or risk_now >= 0.70) else \"Low\")\n",
    ")\n",
    "try:\n",
    "    freshness = \"Fresh\" if bool(tail[\"feeds_fresh\"].iloc[-1]) else \"Stale\"\n",
    "except Exception:\n",
    "    freshness = \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "06641212-ce98-4936-a44c-660475258a1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Analyst Note exported → C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\analyst_note.pdf\n",
      "{'ok': True, 'note': 'Fused anomaly now=1.00; 10-min risk=0.62; 30-min risk=0.86. Confidence High. Top drivers: dev_roll_std (+0.0654±0.0167 AP); oracle_ratio (+0.0201±0.0102 AP); r1_delta (+0.0012±0.0012 AP) Propagation: DAI/USDC_univ3 leads peers (corr=0.96). Action: monitor in 5m; if risk > 0.60 or 3 consecutive reds, engage mitigation (widen slippage / reroute).', 'pdf': 'C:\\\\Users\\\\aniru\\\\OneDrive\\\\Desktop\\\\ML tutorial\\\\DEFI Depeg sentinel\\\\analyst_note.pdf'}\n"
     ]
    }
   ],
   "source": [
    "note_lines = [f\"Fused anomaly now={risk_now:.2f}; 10-min risk={p10:.2f}; 30-min risk={p30:.2f}. Confidence {band}.\"]\n",
    "if top3:\n",
    "    note_lines.append(\"Top drivers: \" + \"; \".join(top3))\n",
    "if cue:\n",
    "    note_lines.append(\"Propagation: \" + cue)\n",
    "note_lines.append(\"Action: monitor in 5m; if risk > 0.60 or 3 consecutive reds, engage mitigation (widen slippage / reroute).\")\n",
    "note = \" \".join(note_lines)[:900]\n",
    "try:\n",
    "    pdf_path = export_analyst_note_pdf(\n",
    "        note_text=note,\n",
    "        risk_now=risk_now,\n",
    "        risk_10m=p10,\n",
    "        risk_30m=p30,\n",
    "        contributors=top3,\n",
    "        freshness=freshness,\n",
    "        confidence=band,\n",
    "        out_path=OUT / \"analyst_note.pdf\",\n",
    "        title=\"Analyst Note v2\"\n",
    "    )\n",
    "except Exception:\n",
    "    pdf_path = OUT / \"analyst_note.txt\"\n",
    "    try:\n",
    "        pdf_path.write_text(note)\n",
    "    except Exception:\n",
    "        pass\n",
    "print({\"ok\": True, \"note\": note, \"pdf\": str(pdf_path)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "3959c5d6-2de5-44e2-ae71-02dd9488a115",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _seq_windows_by_pool(df: pd.DataFrame, cols: list[str], win: int = 30):\n",
    "    df = df.sort_values([\"pool\", \"ts\"]).copy()\n",
    "    idx_map, windows = [], []\n",
    "    for _, g in df.groupby(\"pool\", sort=False):\n",
    "        A = g[cols].astype(float).replace([np.inf, -np.inf], np.nan).fillna(0.0).to_numpy()\n",
    "        gi = g.index.to_list()\n",
    "        for i in range(len(A)):\n",
    "            idx_map.append(gi[i])\n",
    "            windows.append(A[i+1-win:i+1] if i+1 >= win else None)\n",
    "    return idx_map, windows, df.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "363a5fe2-39f0-4a67-8ab8-fc11e792a832",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _lstm_autoencoder_scores(df: pd.DataFrame, cols: list[str] = [\"dev\",\"dev_roll_std\",\"tvl_outflow_rate\"], win: int = 30) -> np.ndarray:\n",
    "    if not _TORCH_OK:\n",
    "        return np.zeros(len(df), dtype=float)\n",
    "    counts = df.groupby(\"pool\").size()\n",
    "    min_hist = int(counts.min()) if not counts.empty else 0\n",
    "    win_eff = max(8, min(win, min_hist))\n",
    "    if win_eff < 8:\n",
    "        return np.zeros(len(df), dtype=float)\n",
    "    idx_map, windows, full_index = _seq_windows_by_pool(df, cols, win=win_eff)\n",
    "    valid = [i for i,w in enumerate(windows) if w is not None]\n",
    "    if not valid:\n",
    "        return np.zeros(len(df), dtype=float)\n",
    "    X = np.stack([windows[i] for i in valid]).astype(np.float32)  \n",
    "    T, F = X.shape[1], X.shape[2]\n",
    "    import torch, torch.nn as nn\n",
    "    class LSTMAE(nn.Module):\n",
    "        def __init__(self, fdim, h=16):\n",
    "            super().__init__()\n",
    "            self.enc = nn.LSTM(input_size=fdim, hidden_size=max(8, h), batch_first=True)\n",
    "            self.dec = nn.LSTM(input_size=max(8, h), hidden_size=fdim, batch_first=True)\n",
    "        def forward(self, x):\n",
    "            z,_ = self.enc(x)\n",
    "            y,_ = self.dec(z[:, -1:, :].repeat(1, x.size(1), 1))\n",
    "            return y\n",
    "    device = \"cpu\"\n",
    "    model = LSTMAE(F, h=max(8, F*2)).to(device)\n",
    "    opt = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "    crit = nn.MSELoss()\n",
    "    xb = torch.from_numpy(X).to(device)\n",
    "    model.train()\n",
    "    for _ in range(6):\n",
    "        opt.zero_grad(); recon = model(xb); loss = crit(recon, xb); loss.backward(); opt.step()\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        recon = model(xb).cpu().numpy()\n",
    "    err = ((recon - X)**2).mean(axis=(1,2))\n",
    "    err = _scale_01(err)\n",
    "    pos_map = {ix: j for j, ix in enumerate(full_index)}\n",
    "    scores = np.zeros(len(full_index), dtype=float)\n",
    "    for i_win, e in zip(valid, err):\n",
    "        scores[pos_map[idx_map[i_win]]] = float(e)\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "66275082-9c1d-417c-8de6-b0826de62c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_network_features_v2(win: int = 60) -> pd.DataFrame:\n",
    "    df = load_live().copy()\n",
    "    if df.empty:\n",
    "        print(\"[network] live is empty; skipping\")\n",
    "        return pd.DataFrame(columns=[\"ts\",\"pool\",\"neighbor_max_dev\",\"neighbor_avg_anom\",\"lead_lag_best\",\"corr_best\"])\n",
    "    df = df.sort_values(\"ts\")\n",
    "    for _, r in df.iterrows():\n",
    "        _push_hist(_hist_dev,  r[\"pool\"], r.get(\"dev\", 0.0), win)\n",
    "        _push_hist(_hist_anom, r[\"pool\"], r.get(\"anom_fused\", 0.0), win)\n",
    "    pools = list(df[\"pool\"].dropna().unique())\n",
    "    rows = []\n",
    "    for p in pools:\n",
    "        dev_p = list(_hist_dev.get(p, []))\n",
    "        neigh = [q for q in pools if q != p]\n",
    "        neigh_max_dev  = 0.0\n",
    "        neigh_avg_anom = 0.0\n",
    "        lead_lag_best  = 0\n",
    "        corr_best      = 0.0\n",
    "        used = 0\n",
    "        for q in neigh:\n",
    "            dev_q = list(_hist_dev.get(q, []))\n",
    "            an_q  = list(_hist_anom.get(q, []))\n",
    "            if dev_q:\n",
    "                neigh_max_dev = max(neigh_max_dev, float(np.nanmax(np.abs(dev_q))))\n",
    "            if an_q:\n",
    "                neigh_avg_anom += float(np.nanmean(an_q)); used += 1\n",
    "            if dev_p and dev_q:\n",
    "                c, lag = _cross_corr_lag(np.array(dev_p), np.array(dev_q), max_lag=10)\n",
    "                if np.isfinite(c) and abs(c) > abs(corr_best):\n",
    "                    corr_best, lead_lag_best = float(c), int(lag)\n",
    "        if used > 0:\n",
    "            neigh_avg_anom /= used\n",
    "        rows.append({\n",
    "            \"pool\": p,\n",
    "            \"neighbor_max_dev\": float(neigh_max_dev),\n",
    "            \"neighbor_avg_anom\": float(neigh_avg_anom),\n",
    "            \"lead_lag_best\": int(lead_lag_best),\n",
    "            \"corr_best\": float(corr_best),\n",
    "        })\n",
    "    latest = df.groupby(\"pool\", as_index=False).tail(1).copy()\n",
    "    net = pd.DataFrame(rows)\n",
    "    latest = latest.drop(columns=[\"neighbor_max_dev\",\"neighbor_avg_anom\",\"lead_lag_best\",\"corr_best\"], errors=\"ignore\")\n",
    "    latest = latest.merge(net, on=\"pool\", how=\"left\")\n",
    "    latest[\"neighbor_max_dev\"]  = latest[\"neighbor_max_dev\"].fillna(0.0)\n",
    "    latest[\"neighbor_avg_anom\"] = latest[\"neighbor_avg_anom\"].fillna(0.0)\n",
    "    latest[\"lead_lag_best\"]     = latest[\"lead_lag_best\"].fillna(0).astype(int)\n",
    "    latest[\"corr_best\"]         = latest[\"corr_best\"].fillna(0.0)\n",
    "    live = load_live()\n",
    "    live[\"ts_str\"] = _iso_str(live[\"ts\"]); latest[\"ts_str\"] = _iso_str(latest[\"ts\"])\n",
    "    live_idx = live.set_index([\"pool\",\"ts_str\"])\n",
    "    upd_idx  = latest.set_index([\"pool\",\"ts_str\"])[[\"neighbor_max_dev\",\"neighbor_avg_anom\",\"lead_lag_best\",\"corr_best\"]]\n",
    "    live_idx.update(upd_idx)\n",
    "    live_updated = live_idx.reset_index().drop(columns=[\"ts_str\"])\n",
    "    write_live(live_updated)\n",
    "    print(\"[network] v2 updated neighbor features (no NaNs)\")\n",
    "    return latest[[\"ts\",\"pool\",\"neighbor_max_dev\",\"neighbor_avg_anom\",\"lead_lag_best\",\"corr_best\"]].sort_values(\"pool\")\n",
    "compute_network_features = compute_network_features_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "e0fae2be-7cef-4f13-8781-492403c7d80f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fixup_coldstart_nans() -> pd.DataFrame:\n",
    "    df = load_live()\n",
    "    if df.empty:\n",
    "        print(\"[fixup] no data\")\n",
    "        return df\n",
    "    df = df.sort_values([\"pool\",\"ts\"]).reset_index(drop=True)\n",
    "    if \"dev\" in df.columns:\n",
    "        roll = (df.groupby(\"pool\")[\"dev\"]\n",
    "                  .rolling(20, min_periods=3).std()\n",
    "                  .reset_index(level=0, drop=True))\n",
    "        df[\"dev_roll_std\"] = roll.fillna(0.0)\n",
    "    for c, default in [\n",
    "        (\"neighbor_max_dev\", 0.0),\n",
    "        (\"neighbor_avg_anom\", 0.0),\n",
    "        (\"lead_lag_best\", 0),\n",
    "        (\"corr_best\", 0.0),\n",
    "        (\"z_ae_seq\", 0.0),\n",
    "    ]:\n",
    "        if c in df.columns:\n",
    "            df[c] = df[c].fillna(default)\n",
    "    df = ensure_targets_all(df)\n",
    "    write_live(df)\n",
    "    print(\"[fixup] filled NaNs and ensured labels\")\n",
    "    return df.tail(12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "53e7f446-93ec-4a96-8512-330c11a6bdea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_anomaly_zoo_update_live_v2(features_cols: List[str] | None = None) -> pd.DataFrame:\n",
    "    df = load_live()\n",
    "    if df.empty or df[\"dev\"].isna().all():\n",
    "        print(\"[zoo] live dataset empty; sample first.\")\n",
    "        return df\n",
    "    if not features_cols:\n",
    "        features_cols = [\"dev\",\"dev_roll_std\",\"tvl_outflow_rate\",\"spot_twap_gap_bps\",\"oracle_ratio\"]\n",
    "    use_cols = [c for c in features_cols if c in df.columns]\n",
    "    if not use_cols:\n",
    "        raise ValueError(\"No usable feature columns found in live dataset.\")\n",
    "    df_proc = df.copy()\n",
    "    X = df_proc[use_cols].astype(float).replace([np.inf, -np.inf], np.nan).fillna(0.0).to_numpy()\n",
    "    scaler = StandardScaler() if _SK_OK else None\n",
    "    Xs = scaler.fit_transform(X) if scaler is not None else X\n",
    "    z_if = np.zeros(len(df_proc))\n",
    "    if _SK_OK:\n",
    "        if_clf = IsolationForest(n_estimators=200, contamination=\"auto\", random_state=SEED)\n",
    "        z_if = _scale_01(-if_clf.fit(Xs).score_samples(Xs))\n",
    "    z_lof = np.zeros(len(df_proc))\n",
    "    if _SK_OK:\n",
    "        try:\n",
    "            lof2 = LocalOutlierFactor(n_neighbors=20, novelty=True).fit(Xs)\n",
    "            z_lof = _scale_01(-lof2.score_samples(Xs))\n",
    "        except Exception:\n",
    "            lof = LocalOutlierFactor(n_neighbors=20, novelty=False)\n",
    "            z_lof = _scale_01((-lof.fit_predict(Xs)).astype(float))\n",
    "    z_ocsvm = np.zeros(len(df_proc))\n",
    "    if _SK_OK:\n",
    "        try:\n",
    "            oc = OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.05)\n",
    "            z_ocsvm = _scale_01(-oc.fit(Xs).decision_function(Xs))\n",
    "        except Exception:\n",
    "            z_ocsvm = np.zeros(len(df_proc))\n",
    "    z_cusum = []\n",
    "    for _, g in df_proc.sort_values([\"pool\",\"ts\"]).groupby(\"pool\", sort=False):\n",
    "        z_cusum.append(_cusum_score(g[\"dev\"]))\n",
    "    z_cusum = pd.concat(z_cusum).reindex(df_proc.index).fillna(0.0).to_numpy()\n",
    "    z_cusum = _scale_01(z_cusum)\n",
    "    try:\n",
    "        z_ae = _autoencoder_scores(df_proc, Xs)\n",
    "    except Exception:\n",
    "        z_ae = np.zeros(len(df_proc))\n",
    "    try:\n",
    "        z_ae_seq = _lstm_autoencoder_scores(df_proc, cols=[\"dev\",\"dev_roll_std\",\"tvl_outflow_rate\"], win=30)\n",
    "    except Exception:\n",
    "        z_ae_seq = np.zeros(len(df_proc))\n",
    "    fused = np.nanmax(np.vstack([z_if, z_lof, z_ocsvm, z_cusum, z_ae, z_ae_seq]), axis=0)\n",
    "    df_proc[\"z_if\"] = z_if\n",
    "    df_proc[\"z_lof\"] = z_lof\n",
    "    df_proc[\"z_ocsvm\"] = z_ocsvm\n",
    "    df_proc[\"z_cusum\"] = z_cusum\n",
    "    df_proc[\"z_ae\"] = z_ae\n",
    "    df_proc[\"z_ae_seq\"] = z_ae_seq\n",
    "    df_proc[\"anom_fused\"] = fused\n",
    "    write_live(df_proc)\n",
    "    print(\"[zoo] anomaly scores (incl. LSTM AE) updated\")\n",
    "    return df_proc.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "6c0bc729-14a5-4851-b993-f735342584b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_anomaly_zoo_update_live = run_anomaly_zoo_update_live_v2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "4e8b70f8-a91e-495e-ba00-a4e438d7037b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[append] 3 rows → C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\live_dataset.csv\n",
      "[zoo] anomaly scores (incl. LSTM AE) updated\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>pool</th>\n",
       "      <th>dex_spot</th>\n",
       "      <th>dex_twap</th>\n",
       "      <th>oracle_ratio</th>\n",
       "      <th>dev</th>\n",
       "      <th>dev_roll_std</th>\n",
       "      <th>tvl_outflow_rate</th>\n",
       "      <th>virtual_price</th>\n",
       "      <th>spot_twap_gap_bps</th>\n",
       "      <th>...</th>\n",
       "      <th>z_cusum</th>\n",
       "      <th>z_ae</th>\n",
       "      <th>anom_fused</th>\n",
       "      <th>y_10m</th>\n",
       "      <th>y_30m</th>\n",
       "      <th>neighbor_max_dev</th>\n",
       "      <th>neighbor_avg_anom</th>\n",
       "      <th>lead_lag_best</th>\n",
       "      <th>corr_best</th>\n",
       "      <th>z_ae_seq</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2227</th>\n",
       "      <td>2025-08-28 03:59:22+00:00</td>\n",
       "      <td>USDC/USDT_univ3</td>\n",
       "      <td>1.000115</td>\n",
       "      <td>0.999982</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.001727</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.330640e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002764</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.747763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2228</th>\n",
       "      <td>2025-08-28 04:00:22+00:00</td>\n",
       "      <td>USDC/USDT_univ3</td>\n",
       "      <td>1.000268</td>\n",
       "      <td>1.000039</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.002689</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.290690e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002508</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004702</td>\n",
       "      <td>0.71868</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.749184</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2229</th>\n",
       "      <td>2025-08-28 04:02:08+00:00</td>\n",
       "      <td>USDC/USDT_univ3</td>\n",
       "      <td>1.000136</td>\n",
       "      <td>1.000059</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-0.004277</td>\n",
       "      <td>NaN</td>\n",
       "      <td>7.732510e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003892</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.744028</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2230</th>\n",
       "      <td>2025-08-28 04:03:30+00:00</td>\n",
       "      <td>USDC/USDT_univ3</td>\n",
       "      <td>0.999768</td>\n",
       "      <td>1.000091</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002406</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.224675e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002928</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.745633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2231</th>\n",
       "      <td>2025-08-28 20:49:13+00:00</td>\n",
       "      <td>USDC/USDT_univ3</td>\n",
       "      <td>1.000149</td>\n",
       "      <td>1.000149</td>\n",
       "      <td>1.000163</td>\n",
       "      <td>0.000163</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.001625</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001520</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004702</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.467008</td>\n",
       "      <td>0.747316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2232</th>\n",
       "      <td>2025-08-28 20:49:13+00:00</td>\n",
       "      <td>DAI/USDC_univ3</td>\n",
       "      <td>0.999641</td>\n",
       "      <td>0.999641</td>\n",
       "      <td>0.999427</td>\n",
       "      <td>-0.000573</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.007126</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-1.110622e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002418</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004702</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.964808</td>\n",
       "      <td>0.745918</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2233</th>\n",
       "      <td>2025-08-28 20:49:13+00:00</td>\n",
       "      <td>3pool_curve</td>\n",
       "      <td>1.000056</td>\n",
       "      <td>1.000056</td>\n",
       "      <td>0.999955</td>\n",
       "      <td>-0.000045</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.003111</td>\n",
       "      <td>1.000056</td>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002075</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.004702</td>\n",
       "      <td>1.00000</td>\n",
       "      <td>-8.0</td>\n",
       "      <td>0.467008</td>\n",
       "      <td>0.748683</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2234</th>\n",
       "      <td>2025-08-28 20:50:37+00:00</td>\n",
       "      <td>USDC/USDT_univ3</td>\n",
       "      <td>0.999826</td>\n",
       "      <td>1.000084</td>\n",
       "      <td>0.999878</td>\n",
       "      <td>-0.000122</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000096</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.586003e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002052</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.746995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2235</th>\n",
       "      <td>2025-08-28 20:50:37+00:00</td>\n",
       "      <td>DAI/USDC_univ3</td>\n",
       "      <td>0.999216</td>\n",
       "      <td>0.999556</td>\n",
       "      <td>0.999121</td>\n",
       "      <td>-0.000879</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.002207</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-3.396891e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003243</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.748325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2236</th>\n",
       "      <td>2025-08-28 20:50:37+00:00</td>\n",
       "      <td>3pool_curve</td>\n",
       "      <td>0.999797</td>\n",
       "      <td>1.000004</td>\n",
       "      <td>0.999766</td>\n",
       "      <td>-0.000234</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.002809</td>\n",
       "      <td>0.999797</td>\n",
       "      <td>-2.066987e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.001651</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.748106</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>10 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ts             pool  dex_spot  dex_twap  \\\n",
       "2227 2025-08-28 03:59:22+00:00  USDC/USDT_univ3  1.000115  0.999982   \n",
       "2228 2025-08-28 04:00:22+00:00  USDC/USDT_univ3  1.000268  1.000039   \n",
       "2229 2025-08-28 04:02:08+00:00  USDC/USDT_univ3  1.000136  1.000059   \n",
       "2230 2025-08-28 04:03:30+00:00  USDC/USDT_univ3  0.999768  1.000091   \n",
       "2231 2025-08-28 20:49:13+00:00  USDC/USDT_univ3  1.000149  1.000149   \n",
       "2232 2025-08-28 20:49:13+00:00   DAI/USDC_univ3  0.999641  0.999641   \n",
       "2233 2025-08-28 20:49:13+00:00      3pool_curve  1.000056  1.000056   \n",
       "2234 2025-08-28 20:50:37+00:00  USDC/USDT_univ3  0.999826  1.000084   \n",
       "2235 2025-08-28 20:50:37+00:00   DAI/USDC_univ3  0.999216  0.999556   \n",
       "2236 2025-08-28 20:50:37+00:00      3pool_curve  0.999797  1.000004   \n",
       "\n",
       "      oracle_ratio       dev  dev_roll_std  tvl_outflow_rate  virtual_price  \\\n",
       "2227           NaN       NaN           0.0         -0.001727            NaN   \n",
       "2228           NaN       NaN           0.0          0.002689            NaN   \n",
       "2229           NaN       NaN           0.0         -0.004277            NaN   \n",
       "2230           NaN       NaN           NaN          0.002406            NaN   \n",
       "2231      1.000163  0.000163           NaN         -0.001625            NaN   \n",
       "2232      0.999427 -0.000573           NaN          0.007126            NaN   \n",
       "2233      0.999955 -0.000045           NaN         -0.003111       1.000056   \n",
       "2234      0.999878 -0.000122           NaN          0.000096            NaN   \n",
       "2235      0.999121 -0.000879           NaN         -0.002207            NaN   \n",
       "2236      0.999766 -0.000234           NaN          0.002809       0.999797   \n",
       "\n",
       "      spot_twap_gap_bps  ...  z_cusum      z_ae  anom_fused  y_10m  y_30m  \\\n",
       "2227       1.330640e+00  ...      1.0  0.002764         1.0    1.0    NaN   \n",
       "2228       2.290690e+00  ...      1.0  0.002508         1.0    1.0    NaN   \n",
       "2229       7.732510e-01  ...      1.0  0.003892         1.0    0.0    NaN   \n",
       "2230      -3.224675e+00  ...      1.0  0.002928         1.0    0.0    NaN   \n",
       "2231       0.000000e+00  ...      1.0  0.001520         1.0    NaN    NaN   \n",
       "2232      -1.110622e-12  ...      1.0  0.002418         1.0    NaN    NaN   \n",
       "2233       0.000000e+00  ...      1.0  0.002075         1.0    NaN    NaN   \n",
       "2234      -2.586003e+00  ...      1.0  0.002052         1.0    NaN    NaN   \n",
       "2235      -3.396891e+00  ...      1.0  0.003243         1.0    NaN    NaN   \n",
       "2236      -2.066987e+00  ...      1.0  0.001651         1.0    NaN    NaN   \n",
       "\n",
       "      neighbor_max_dev  neighbor_avg_anom  lead_lag_best corr_best  z_ae_seq  \n",
       "2227               NaN                NaN            NaN       NaN  0.747763  \n",
       "2228          0.004702            0.71868            0.0  0.000000  0.749184  \n",
       "2229               NaN                NaN            NaN       NaN  0.744028  \n",
       "2230               NaN                NaN            NaN       NaN  0.745633  \n",
       "2231          0.004702            1.00000            8.0  0.467008  0.747316  \n",
       "2232          0.004702            1.00000            2.0  0.964808  0.745918  \n",
       "2233          0.004702            1.00000           -8.0  0.467008  0.748683  \n",
       "2234               NaN                NaN            NaN       NaN  0.746995  \n",
       "2235               NaN                NaN            NaN       NaN  0.748325  \n",
       "2236               NaN                NaN            NaN       NaN  0.748106  \n",
       "\n",
       "[10 rows x 33 columns]"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_once()\n",
    "run_anomaly_zoo_update_live() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "ab1a85d6-3d75-4306-987f-ff18ebe78c3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\anaconda3\\lib\\site-packages (4.55.4)\n",
      "Requirement already satisfied: accelerate in c:\\anaconda3\\lib\\site-packages (1.10.1)\n",
      "Requirement already satisfied: sentence-transformers in c:\\anaconda3\\lib\\site-packages (5.1.0)\n",
      "Requirement already satisfied: filelock in c:\\anaconda3\\lib\\site-packages (from transformers) (3.19.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.34.0 in c:\\anaconda3\\lib\\site-packages (from transformers) (0.34.4)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\anaconda3\\lib\\site-packages (from transformers) (2.1.3)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\anaconda3\\lib\\site-packages (from transformers) (25.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\anaconda3\\lib\\site-packages (from transformers) (6.0.2)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\anaconda3\\lib\\site-packages (from transformers) (2025.7.34)\n",
      "Requirement already satisfied: requests in c:\\anaconda3\\lib\\site-packages (from transformers) (2.32.4)\n",
      "Requirement already satisfied: tokenizers<0.22,>=0.21 in c:\\anaconda3\\lib\\site-packages (from transformers) (0.21.4)\n",
      "Requirement already satisfied: safetensors>=0.4.3 in c:\\anaconda3\\lib\\site-packages (from transformers) (0.6.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\anaconda3\\lib\\site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (2025.7.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\anaconda3\\lib\\site-packages (from huggingface-hub<1.0,>=0.34.0->transformers) (4.14.1)\n",
      "Requirement already satisfied: psutil in c:\\users\\aniru\\appdata\\roaming\\python\\python312\\site-packages (from accelerate) (7.0.0)\n",
      "Requirement already satisfied: torch>=2.0.0 in c:\\anaconda3\\lib\\site-packages (from accelerate) (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in c:\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.7.1)\n",
      "Requirement already satisfied: scipy in c:\\anaconda3\\lib\\site-packages (from sentence-transformers) (1.16.1)\n",
      "Requirement already satisfied: Pillow in c:\\anaconda3\\lib\\site-packages (from sentence-transformers) (11.3.0)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (78.1.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda3\\lib\\site-packages (from torch>=2.0.0->accelerate) (3.1.6)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch>=2.0.0->accelerate) (1.3.0)\n",
      "Requirement already satisfied: colorama in c:\\anaconda3\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\anaconda3\\lib\\site-packages (from jinja2->torch>=2.0.0->accelerate) (3.0.2)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (1.5.1)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\anaconda3\\lib\\site-packages (from scikit-learn->sentence-transformers) (3.6.0)\n",
      "Requirement already satisfied: bitsandbytes in c:\\anaconda3\\lib\\site-packages (0.47.0)\n",
      "Requirement already satisfied: torch<3,>=2.2 in c:\\anaconda3\\lib\\site-packages (from bitsandbytes) (2.5.1)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\anaconda3\\lib\\site-packages (from bitsandbytes) (2.1.3)\n",
      "Requirement already satisfied: filelock in c:\\anaconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.19.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\anaconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (4.14.1)\n",
      "Requirement already satisfied: setuptools in c:\\anaconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (78.1.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in c:\\anaconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (1.13.1)\n",
      "Requirement already satisfied: networkx in c:\\anaconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in c:\\anaconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (3.1.6)\n",
      "Requirement already satisfied: fsspec in c:\\anaconda3\\lib\\site-packages (from torch<3,>=2.2->bitsandbytes) (2025.7.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\anaconda3\\lib\\site-packages (from sympy==1.13.1->torch<3,>=2.2->bitsandbytes) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\anaconda3\\lib\\site-packages (from jinja2->torch<3,>=2.2->bitsandbytes) (3.0.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers accelerate sentence-transformers \n",
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "989abdf6-0bc5-439f-bdd5-e4f7b3fa86d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: faiss-cpu in c:\\anaconda3\\lib\\site-packages (1.12.0)\n",
      "Requirement already satisfied: numpy<3.0,>=1.25.0 in c:\\anaconda3\\lib\\site-packages (from faiss-cpu) (2.1.3)\n",
      "Requirement already satisfied: packaging in c:\\anaconda3\\lib\\site-packages (from faiss-cpu) (25.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "14bb6601-3d70-499e-881f-1e779c43f534",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, re\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import datetime, timezone\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "2939163d-20b7-44f2-9011-c0809a9c664b",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEVICE = \"cuda\" if torch.cuda.is_available() else (\n",
    "    \"mps\" if getattr(torch.backends, \"mps\", None) and torch.backends.mps.is_available() else \"cpu\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "01669ff8-dc1d-497d-962b-f82380ea33ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[llm] device=cuda dtype=torch.float16\n"
     ]
    }
   ],
   "source": [
    "def _best_dtype(device: str):\n",
    "    if device == \"cuda\":\n",
    "        try:\n",
    "            major, _ = torch.cuda.get_device_capability(0)\n",
    "            return torch.bfloat16 if major >= 8 else torch.float16\n",
    "        except Exception:\n",
    "            return torch.float16\n",
    "    return torch.float32\n",
    "DTYPE = _best_dtype(DEVICE)\n",
    "print(f\"[llm] device={DEVICE} dtype={DTYPE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "8fe6b21f-bd70-4dc0-bf9d-884ef83edcfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class HFSeq2Seq:\n",
    "    \"\"\"\n",
    "    * Enforces encoder truncation (<= max_input_tokens).\n",
    "    * Retries on CUDA OOM (halve lengths) then CPU fallback for the call.\n",
    "    * Tries 4-bit (bnb) -> 8-bit -> fp16/bf16 depending on availability.\n",
    "    * If 4-bit device_map='auto' would offload to CPU/disk (disallowed), retries without auto-map.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        model_name: str = \"google/flan-t5-small\",\n",
    "        device: str = DEVICE,\n",
    "        dtype = DTYPE,\n",
    "        max_new_tokens: int = 128,\n",
    "        max_input_tokens: int = 480,\n",
    "        load_quant: str | None = \"4bit\"  \n",
    "    ):\n",
    "        self.model_name = model_name\n",
    "        self.device = device\n",
    "        self.dtype = dtype\n",
    "        self.max_new_tokens = int(max_new_tokens)\n",
    "        self.max_input_tokens = int(max_input_tokens)\n",
    "        hf_token = os.getenv(\"HUGGINGFACE_HUB_TOKEN\", None)\n",
    "        tok_kwargs = {}\n",
    "        if hf_token:\n",
    "            tok_kwargs[\"use_auth_token\"] = hf_token\n",
    "            tok_kwargs[\"token\"] = hf_token\n",
    "        self.tok = AutoTokenizer.from_pretrained(model_name, **tok_kwargs)\n",
    "        try:\n",
    "            self.tok.model_max_length = min(self.tok.model_max_length or 512, self.max_input_tokens)\n",
    "        except Exception:\n",
    "            pass\n",
    "        self.model = self._load_with_fallbacks(model_name, device, dtype, load_quant)\n",
    "    def _load_with_fallbacks(self, model_name, device, dtype, load_quant):\n",
    "        if device == \"cuda\" and load_quant == \"4bit\":\n",
    "            try:\n",
    "                from transformers import BitsAndBytesConfig\n",
    "                print(f\"[llm] trying {model_name} in 4bit (auto-map)\")\n",
    "                q = BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_compute_dtype=dtype,\n",
    "                    bnb_4bit_use_double_quant=True,\n",
    "                    bnb_4bit_quant_type=\"nf4\",\n",
    "                )\n",
    "                return AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    torch_dtype=dtype,\n",
    "                    quantization_config=q,\n",
    "                    device_map=\"auto\",\n",
    "                )\n",
    "            except ValueError as e:\n",
    "                if \"dispatched on the CPU or the disk\" in str(e):\n",
    "                    try:\n",
    "                        from transformers import BitsAndBytesConfig\n",
    "                        print(f\"[llm] retry {model_name} 4bit (no auto-map)\")\n",
    "                        q = BitsAndBytesConfig(\n",
    "                            load_in_4bit=True,\n",
    "                            bnb_4bit_compute_dtype=dtype,\n",
    "                            bnb_4bit_use_double_quant=True,\n",
    "                            bnb_4bit_quant_type=\"nf4\",\n",
    "                        )\n",
    "                        m = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                            model_name,\n",
    "                            low_cpu_mem_usage=True,\n",
    "                            torch_dtype=dtype,\n",
    "                            quantization_config=q,\n",
    "                        )\n",
    "                        return m.to(\"cuda\")\n",
    "                    except torch.cuda.OutOfMemoryError:\n",
    "                        print(\"[llm] OOM loading 4bit fully on GPU; falling back to 8bit…\")\n",
    "                    except Exception as e2:\n",
    "                        print(f\"[llm] 4bit (no auto-map) failed: {e2}\")\n",
    "                else:\n",
    "                    print(f\"[llm] 4bit (auto) failed: {e}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[llm] 4bit load skipped: {e}\")\n",
    "        if device == \"cuda\" and load_quant in (\"4bit\",\"8bit\"):\n",
    "            try:\n",
    "                from transformers import BitsAndBytesConfig\n",
    "                print(f\"[llm] trying {model_name} in 8bit (auto-map + CPU offload)\")\n",
    "                q = BitsAndBytesConfig(load_in_8bit=True)\n",
    "                return AutoModelForSeq2SeqLM.from_pretrained(\n",
    "                    model_name,\n",
    "                    low_cpu_mem_usage=True,\n",
    "                    torch_dtype=dtype,\n",
    "                    quantization_config=q,\n",
    "                    device_map=\"auto\",\n",
    "                    llm_int8_enable_fp32_cpu_offload=True,\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"[llm] 8bit load skipped: {e}\")\n",
    "        print(f\"[llm] loading {model_name} in {('fp16/bf16' if device in ('cuda','mps') else 'fp32')} (no quant)\")\n",
    "        m = AutoModelForSeq2SeqLM.from_pretrained(\n",
    "            model_name,\n",
    "            low_cpu_mem_usage=True,\n",
    "            torch_dtype=(dtype if device in (\"cuda\",\"mps\") else torch.float32),\n",
    "        )\n",
    "        if device == \"cuda\":\n",
    "            try:\n",
    "                m = m.to(\"cuda\")\n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                print(\"[llm] OOM moving model to GPU; keeping on CPU.\")\n",
    "        elif device == \"mps\":\n",
    "            try:\n",
    "                m = m.to(\"mps\")\n",
    "            except Exception:\n",
    "                pass\n",
    "        return m\n",
    "    @torch.inference_mode()\n",
    "    def generate(self, prompt: str, max_new_tokens: int | None = None, max_input_tokens: int | None = None) -> str:\n",
    "        mt  = int(max_new_tokens or self.max_new_tokens)\n",
    "        mit = int(max_input_tokens or self.max_input_tokens)\n",
    "        def _encode(_mit):\n",
    "            return self.tok(prompt, return_tensors=\"pt\", truncation=True, max_length=_mit, padding=\"longest\")\n",
    "        def _gen(enc, _mt):\n",
    "            enc = {k: v.to(self.model.device) for k, v in enc.items()}\n",
    "            return self.model.generate(**enc, max_new_tokens=_mt, do_sample=False, use_cache=True, num_beams=1)\n",
    "        try:\n",
    "            enc = _encode(mit)\n",
    "            out = _gen(enc, mt)\n",
    "        except torch.cuda.OutOfMemoryError:\n",
    "            try:\n",
    "                torch.cuda.empty_cache()\n",
    "            except Exception:\n",
    "                pass\n",
    "            mt2, mit2 = max(64, mt//2), max(256, mit//2)\n",
    "            try:\n",
    "                enc = _encode(mit2)\n",
    "                out = _gen(enc, mt2)\n",
    "            except torch.cuda.OutOfMemoryError:\n",
    "                print(\"[llm] OOM on GPU during gen — CPU fallback for this call.\")\n",
    "                self.model = self.model.to(\"cpu\")\n",
    "                enc = {k: v.to(\"cpu\") for k, v in enc.items()}\n",
    "                out = self.model.generate(**enc, max_new_tokens=mt2, do_sample=False, use_cache=True, num_beams=1)\n",
    "        except RuntimeError as e:\n",
    "            if \"sequence length\" in str(e).lower():\n",
    "                enc = _encode(480)\n",
    "                out = _gen(enc, mt)\n",
    "            else:\n",
    "                return f\"[LLM-ERROR] {e}\"\n",
    "        return self.tok.decode(out[0], skip_special_tokens=True).strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "ff3e0c63-6e0f-4983-b628-c3bd6e905210",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[llm] trying google/flan-t5-small in 4bit (auto-map)\n",
      "[llm] trying google/flan-t5-small in 4bit (auto-map)\n",
      "[llm] IE on cuda:0 | NOTE on cuda:0\n"
     ]
    }
   ],
   "source": [
    "T5_IE   = HFSeq2Seq(model_name=\"google/flan-t5-small\", device=DEVICE, dtype=DTYPE, max_new_tokens=120, max_input_tokens=480, load_quant=\"4bit\")\n",
    "T5_NOTE = HFSeq2Seq(model_name=\"google/flan-t5-small\", device=DEVICE, dtype=DTYPE, max_new_tokens=128, max_input_tokens=480, load_quant=\"4bit\")\n",
    "print(\"[llm] IE on\", T5_IE.model.device, \"| NOTE on\", T5_NOTE.model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "fe3dba3a-0c4c-441e-8d1a-ff7b81f8f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _truncate(s: str, limit: int = 3000) -> str:\n",
    "    try:\n",
    "        return s if len(s) <= limit else s[:limit] + \"\\n... [truncated]\"\n",
    "    except Exception:\n",
    "        return str(s)[:limit]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "f6ab7e23-4474-4cf2-bd93-9d1ddabcbeb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _json_between(s: str):\n",
    "    if not isinstance(s, str):\n",
    "        return []\n",
    "    m = list(re.finditer(r'\\[.*\\]|\\{.*\\}', s, flags=re.S))\n",
    "    if not m:\n",
    "        return []\n",
    "    block = m[-1].group(0)\n",
    "    try:\n",
    "        return json.loads(block)\n",
    "    except Exception:\n",
    "        block2 = re.sub(r\"(\\w+)\\s*:\", r'\"\\1\":', block).replace(\"'\", '\"')\n",
    "        try:\n",
    "            return json.loads(block2)\n",
    "        except Exception:\n",
    "            return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "6dbc018a-afad-4157-8ec4-bdd8fd5c0e40",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _now_iso() -> str:\n",
    "    return datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "def _rag_hits(rag_obj, query: str, k: int = 5):\n",
    "    try:\n",
    "        return rag_obj.search(query, k=k)\n",
    "    except Exception:\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "6b08f449-01b4-48b6-9b41-fee1b34520a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ie_extract_events_t5(docs: list[dict], use_rag: bool = False, k_ctx: int = 5) -> list[dict]:\n",
    "    if \"T5_IE\" not in globals():\n",
    "        raise RuntimeError(\"T5_IE not initialized.\")\n",
    "    docs_min = []\n",
    "    for d in (docs or [])[:12]:\n",
    "        docs_min.append({\n",
    "            \"title\": d.get(\"title\") or d.get(\"status\") or \"doc\",\n",
    "            \"text\": (d.get(\"body\") or d.get(\"msg\") or d.get(\"summary\") or \"\")[:400],\n",
    "            \"ts\": d.get(\"ts\") or _now_iso(),\n",
    "            \"source\": d.get(\"source\") or \"\",\n",
    "        })\n",
    "    ctx = []\n",
    "    if use_rag and \"RAG_B\" in globals():\n",
    "        hits = _rag_hits(RAG_B, query=\"stablecoin depeg OR curve status OR usdc usdt dai incident\", k=k_ctx)\n",
    "        ctx = [{\"title\": h.get(\"title\",\"\"), \"text\": str(h.get(\"text\",\"\"))[:300], \"ts\": h.get(\"ts\",\"\"), \"source\": h.get(\"source\",\"\")} for h in hits]\n",
    "    instr = (\n",
    "        \"Extract risk events from DOCS (and CONTEXT if present). \"\n",
    "        \"Return a JSON array. Each item has: type, severity, ts, summary, source. \"\n",
    "        \"severity is 1..5 (5=highest). summary <= 140 chars. Only include depeg-relevant items.\"\n",
    "    )\n",
    "    payload = {\"context\": ctx, \"docs\": docs_min}\n",
    "    prompt = f\"{instr}\\nDATA:\\n{_truncate(json.dumps(payload, ensure_ascii=False), 3000)}\\nJSON:\"\n",
    "    raw = T5_IE.generate(prompt, max_new_tokens=110, max_input_tokens=480)\n",
    "    ev = _json_between(raw)\n",
    "    out = []\n",
    "    if isinstance(ev, list) and ev:\n",
    "        for e in ev:\n",
    "            sv = pd.to_numeric(e.get(\"severity\", 1), errors=\"coerce\")\n",
    "            sv = int(1 if pd.isna(sv) else max(1, min(5, int(sv))))\n",
    "            out.append({\n",
    "                \"type\": str(e.get(\"type\",\"information\")),\n",
    "                \"severity\": sv,\n",
    "                \"ts\": str(e.get(\"ts\") or _now_iso()),\n",
    "                \"summary\": str(e.get(\"summary\",\"\"))[:140],\n",
    "                \"source\": str(e.get(\"source\",\"\")),\n",
    "            })\n",
    "    else:\n",
    "        s0 = docs_min[0][\"source\"] if docs_min else \"\"\n",
    "        t0 = docs_min[0][\"ts\"] if docs_min else _now_iso()\n",
    "        out = [{\"type\":\"information\",\"severity\":1,\"ts\":t0,\"summary\":\"(fallback) no actionable events parsed\",\"source\":s0}]\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "06e85466-b070-442a-9060-08f7ea3dfa97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reason_and_write_t5(feats_df: pd.DataFrame, events: list[dict], use_rag: bool = True, k_ctx: int = 3) -> dict:\n",
    "    if \"T5_NOTE\" not in globals():\n",
    "        raise RuntimeError(\"T5_NOTE not initialized.\")\n",
    "    cols = [\"pool\",\"dev\",\"anom_fused\",\"r0_delta\",\"r1_delta\",\"dev_roll_std\",\n",
    "            \"tvl_outflow_rate\",\"spot_twap_gap_bps\",\"oracle_ratio\",\"risk_forecast_10m\"]\n",
    "    cols = [c for c in cols if isinstance(feats_df, pd.DataFrame) and c in feats_df.columns]\n",
    "    onchain = (feats_df[cols].tail(12).replace([np.inf,-np.inf], np.nan).fillna(0.0).round(6)\n",
    "               .to_dict(orient=\"records\")) if cols else []\n",
    "    ctxA, ctxB = [], []\n",
    "    if use_rag and \"RAG_A\" in globals():\n",
    "        a = _rag_hits(RAG_A, query=\"dex-oracle spread twap liquidity imbalance stablecoin\", k=k_ctx)\n",
    "        ctxA = [{\"title\": h.get(\"title\",\"\"), \"text\": str(h.get(\"text\",\"\"))[:280]} for h in a]\n",
    "    if use_rag and \"RAG_B\" in globals():\n",
    "        b = _rag_hits(RAG_B, query=\"curve status incident outage depeg\", k=k_ctx)\n",
    "        ctxB = [{\"title\": h.get(\"title\",\"\"), \"text\": str(h.get(\"text\",\"\"))[:280], \"source\": h.get(\"source\",\"\")} for h in b]\n",
    "    citations = []\n",
    "    for e in (events or [])[:5]:\n",
    "        src = e.get(\"source\"); \n",
    "        if src: citations.append(str(src))\n",
    "    for h in ctxB:\n",
    "        src = h.get(\"source\")\n",
    "        if src and src not in citations:\n",
    "            citations.append(src)\n",
    "    instr = (\n",
    "        \"You are a DeFi risk analyst. Using ONCHAIN, EVENTS, and CONTEXT, output JSON with:\\n\"\n",
    "        \"- risk_score (0..100 integer)\\n\"\n",
    "        \"- analyst_note (<=200 words)\\n\"\n",
    "        \"- actions (<=3 items, each {title, rationale})\\n\"\n",
    "        \"Emphasize DEX-oracle dev, anom_fused, TVL outflow, spot–TWAP gap, and event severity. \"\n",
    "        \"If risk_forecast_10m exists, factor it in. No markdown in JSON.\"\n",
    "    )\n",
    "    data = {\n",
    "        \"onchain\": onchain,\n",
    "        \"events\": [{\"summary\": e.get(\"summary\",\"\")[:160], \"severity\": int(e.get(\"severity\",1)), \"source\": e.get(\"source\",\"\")} for e in (events or [])[:8]],\n",
    "        \"context\": {\"kb\": ctxA, \"news\": ctxB}\n",
    "    }\n",
    "    prompt = f\"{instr}\\nDATA:\\n{_truncate(json.dumps(data, ensure_ascii=False), 3000)}\\nJSON:\"\n",
    "    raw = T5_NOTE.generate(prompt, max_new_tokens=120, max_input_tokens=480)\n",
    "    obj = _json_between(raw)\n",
    "    def _fallback():\n",
    "        risk_now = float(np.nanmax([r.get(\"anom_fused\", 0.0) for r in onchain]) if onchain else 0.0)\n",
    "        p10 = float(np.nanmax([r.get(\"risk_forecast_10m\", 0.0) for r in onchain]) if onchain else 0.0)\n",
    "        rs = int(max(0, min(100, round(100 * max(risk_now, p10)))))\n",
    "        note = \"Monitor spreads and TVL outflows; no strong signals parsed. Re-run in 5 minutes.\"\n",
    "        acts = [\n",
    "            {\"title\":\"Monitor in 5m\",\"rationale\":\"Confirm spreads, TWAP gap, and event freshness\"},\n",
    "            {\"title\":\"Reroute if |dev|>0.6%\",\"rationale\":\"Avoid slippage during transient depegs\"},\n",
    "            {\"title\":\"Escalate on 3 reds\",\"rationale\":\"Trigger mitigation webhook with human ack\"},\n",
    "        ]\n",
    "        return {\"risk_score\": rs, \"analyst_note\": note, \"actions\": acts}\n",
    "    if not isinstance(obj, dict) or \"risk_score\" not in obj:\n",
    "        obj = _fallback()\n",
    "    try:\n",
    "        rs = int(max(0, min(100, int(pd.to_numeric(obj.get(\"risk_score\", 0), errors=\"coerce\") or 0))))\n",
    "    except Exception:\n",
    "        rs = _fallback()[\"risk_score\"]\n",
    "    note = str(obj.get(\"analyst_note\", \"\"))[:1200]\n",
    "    acts = obj.get(\"actions\", [])\n",
    "    if not isinstance(acts, list):\n",
    "        acts = []\n",
    "    acts2 = [{\"title\": str(a.get(\"title\",\"Action\"))[:80], \"rationale\": str(a.get(\"rationale\",\"\"))[:200]} for a in acts[:3]]\n",
    "    return {\"risk_score\": rs, \"analyst_note\": note, \"actions\": acts2, \"citations\": citations[:6]}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "d128bfcb-693a-4294-9309-4cdadc9c8541",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi import FastAPI, Query\n",
    "from typing import List, Optional\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "607eb2e6-637b-493f-a5ab-6b53c78cbf68",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = FastAPI(title=\"Depeg Sentinel MCP\", version=\"2.0.0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "8511c52c-088d-4e1a-95ce-7e806a6c9890",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/ml/score_zoo\")\n",
    "def score_zoo(pools: Optional[List[str]] = Query(default=None)):\n",
    "    df = run_anomaly_zoo_update_live()\n",
    "    if pools:\n",
    "        df = df[df[\"pool\"].isin(pools)]\n",
    "    return df[[\"ts\",\"pool\",\"z_if\",\"z_lof\",\"z_ocsvm\",\"z_cusum\",\"z_ae\",\"z_ae_seq\",\"anom_fused\"]].tail(200).to_dict(\"records\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "2a91a44b-db1b-4cfe-b8c8-495caf0264a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/ml/forecast\")\n",
    "def forecast(pools: Optional[List[str]] = Query(default=None), horizon: List[int] = Query(default=[10])):\n",
    "    out = {}\n",
    "    if 10 in horizon:\n",
    "        out[\"h10\"] = score_latest_10m(n_tail=200, write_parquet=False).to_dict(\"records\")\n",
    "    if pools:\n",
    "        for k in out:\n",
    "            out[k] = [r for r in out[k] if r[\"pool\"] in pools]\n",
    "    return out\n",
    "@app.get(\"/ml/explain\")\n",
    "def explain(pools: Optional[List[str]] = Query(default=None)):\n",
    "    exp = explain_forecast_10m()\n",
    "    return exp\n",
    "@app.get(\"/intel/top_events\")\n",
    "def top_events(since: Optional[str] = None):\n",
    "    ev = _load_events()\n",
    "    if since:\n",
    "        ev = [e for e in ev if str(e.get(\"ts\",\"\")) >= since]\n",
    "    ev.sort(key=lambda e: int(e.get(\"severity\", 0)), reverse=True)\n",
    "    return ev[:50]\n",
    "@app.get(\"/signals/network\")\n",
    "def signals_network(pools: Optional[List[str]] = Query(default=None)):\n",
    "    df = compute_network_features()\n",
    "    if pools:\n",
    "        df = df[df[\"pool\"].isin(pools)]\n",
    "    return df.to_dict(\"records\")\n",
    "@app.post(\"/policy/decide\")\n",
    "def policy_decide_api(state: dict | None = None):\n",
    "    df = decide_latest(n_tail=20)\n",
    "    return df.tail(1).to_dict(\"records\")[0]\n",
    "@app.get(\"/policy/retrain_check\")\n",
    "def retrain_check():\n",
    "    return {\"should_retrain\": True, \"reason\": \"scheduled nightly or drift threshold (PR-AUC drop) hit\"}\n",
    "@app.get(\"/policy/snapshot\")\n",
    "def snapshot():\n",
    "    note = build_analyst_note_v2()\n",
    "    rep = nightly_report()\n",
    "    return {\"note\": note, \"report\": rep}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "bfe32d9a-07d0-4815-8ba6-ddc07ccfa9f8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "PROFILE = \"balanced_fast\"  \n",
    "PROFILES = {\n",
    "    \"blitz\":          {\"LOOP_SECS\": 10, \"TRAIN_EVERY\": 180},   \n",
    "    \"balanced_fast\":  {\"LOOP_SECS\": 30, \"TRAIN_EVERY\": 120},   \n",
    "    \"cheap\":          {\"LOOP_SECS\": 60, \"TRAIN_EVERY\": 120},   \n",
    "}\n",
    "FAST_CFG = {\n",
    "    \"TAIL_WIN\":     500,  \n",
    "    \"EVENTS_EVERY\": 6,    \n",
    "    \"NETWORK_EVERY\":2,    \n",
    "    \"META_EVERY\":   2,   \n",
    "}\n",
    "LOOP_SECS = PROFILES[PROFILE][\"LOOP_SECS\"]\n",
    "TRAIN_EVERY = PROFILES[PROFILE][\"TRAIN_EVERY\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "f33d8ed5-99e1-4b2f-b895-f4184e2d2426",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, math\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "8a3a8852-5942-4165-ba12-28ab46c2a860",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _unique_oracle_addrs() -> list[str]:\n",
    "    return list({a for a in CFG.chainlink_feeds.values() if a})\n",
    "def _fetch_oracles_parallel() -> dict[str, OnchainResult]:\n",
    "    addrs = _unique_oracle_addrs()\n",
    "    out: dict[str, OnchainResult] = {}\n",
    "    if not addrs: \n",
    "        return out\n",
    "    with ThreadPoolExecutor(max_workers=min(8, len(addrs))) as ex:\n",
    "        futs = {ex.submit(onchain.get_oracle_price, a): a for a in addrs}\n",
    "        for fut in as_completed(futs):\n",
    "            a = futs[fut]\n",
    "            try:\n",
    "                out[a] = fut.result()\n",
    "            except Exception as e:\n",
    "                out[a] = OnchainResult(False, _now_iso(), None, {}, str(e))\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "0480b8eb-3a41-4cc0-a66b-be0f869bc324",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_once_parallel(ts_override: str | None = None) -> pd.DataFrame:\n",
    "    \"\"\"Faster sample: parallel oracles + pools. Optional timestamp override for bootstrapping.\"\"\"\n",
    "    ts_now = ts_override or _now_iso()\n",
    "    oracle_res = _fetch_oracles_parallel()\n",
    "    def _oracle_px_for(sym: str) -> float:\n",
    "        feed = CFG.chainlink_feeds.get(sym, \"\")\n",
    "        r = oracle_res.get(feed)\n",
    "        return float(r.data.get(\"price\")) if (r and r.ok and r.data.get(\"price\") is not None) else float(\"nan\")\n",
    "    def _fetch_pool(pool_name: str, meta: dict) -> dict:\n",
    "        typ  = meta.get(\"type\"); addr = meta.get(\"address\"); sym = meta.get(\"symbol\", \"USDC/USD\")\n",
    "        dex_spot = np.nan; virt_price = np.nan; r0 = r1 = np.nan\n",
    "        if typ == \"uniswap_v3\":\n",
    "            uni = onchain.get_univ3_price(addr)\n",
    "            res = onchain.get_reserves_min(addr)\n",
    "            if uni and uni.ok: dex_spot = float(uni.data.get(\"price\", np.nan))\n",
    "            if res and res.ok: r0 = float(res.data.get(\"r0\", np.nan)); r1 = float(res.data.get(\"r1\", np.nan))\n",
    "        elif typ == \"curve\":\n",
    "            cur = onchain.get_curve_virtual_price(addr)\n",
    "            res = onchain.get_reserves_min(addr)\n",
    "            if cur and cur.ok: virt_price = float(cur.data.get(\"virtual_price\", np.nan))\n",
    "            if res and res.ok: r0 = float(res.data.get(\"r0\", np.nan)); r1 = float(res.data.get(\"r1\", np.nan))\n",
    "        return {\n",
    "            \"ts\": ts_now, \"pool\": pool_name,\n",
    "            \"dex_spot\": dex_spot, \"virtual_price\": virt_price,\n",
    "            \"oracle_px\": _oracle_px_for(sym),\n",
    "            \"r0\": r0, \"r1\": r1, \"block\": None,\n",
    "        }\n",
    "    rows: list[dict] = []\n",
    "    with ThreadPoolExecutor(max_workers=min(8, len(CFG.pools))) as ex:\n",
    "        futs = {ex.submit(_fetch_pool, name, meta): name for name, meta in CFG.pools.items()}\n",
    "        for fut in as_completed(futs):\n",
    "            try: rows.append(fut.result())\n",
    "            except Exception: pass\n",
    "    feat = engineer_features(rows)\n",
    "    append_live(feat)\n",
    "    return feat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "afc3d9ad-ea10-4bf3-928d-df4bea915af2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_anomaly_zoo_update_live_fast(tail_win: int = 500, features_cols: list[str] | None = None) -> pd.DataFrame:\n",
    "    df = load_live()\n",
    "    if df.empty or df[\"dev\"].isna().all(): return df\n",
    "    if not features_cols:\n",
    "        features_cols = [\"dev\",\"dev_roll_std\",\"tvl_outflow_rate\",\"spot_twap_gap_bps\",\"oracle_ratio\"]\n",
    "    use_cols = [c for c in features_cols if c in df.columns]\n",
    "    if not use_cols: return df.tail(1)\n",
    "    tail = df.tail(tail_win).copy()\n",
    "    X = tail[use_cols].astype(float).replace([np.inf,-np.inf], np.nan).fillna(0.0).to_numpy()\n",
    "    if _SK_OK:\n",
    "        from sklearn.preprocessing import StandardScaler\n",
    "        Xs = StandardScaler().fit_transform(X)\n",
    "    else:\n",
    "        Xs = X\n",
    "    z_if = np.zeros(len(tail)); z_lof = np.zeros(len(tail)); z_ocsvm = np.zeros(len(tail))\n",
    "    if _SK_OK:\n",
    "        from sklearn.ensemble import IsolationForest\n",
    "        from sklearn.neighbors import LocalOutlierFactor\n",
    "        from sklearn.svm import OneClassSVM\n",
    "        try:\n",
    "            z_if = _scale_01(-IsolationForest(n_estimators=150, contamination=\"auto\", random_state=SEED, n_jobs=-1).fit(Xs).score_samples(Xs))\n",
    "        except Exception: pass\n",
    "        try:\n",
    "            lof2 = LocalOutlierFactor(n_neighbors=20, novelty=True).fit(Xs)\n",
    "            z_lof = _scale_01(-lof2.score_samples(Xs))\n",
    "        except Exception:\n",
    "            try:\n",
    "                z_lof = _scale_01((-LocalOutlierFactor(n_neighbors=20).fit_predict(Xs)).astype(float))\n",
    "            except Exception: pass\n",
    "        try:\n",
    "            z_ocsvm = _scale_01(-OneClassSVM(kernel=\"rbf\", gamma=\"scale\", nu=0.05).fit(Xs).decision_function(Xs))\n",
    "        except Exception: pass\n",
    "    z_cus_all = []\n",
    "    for _, g in df.sort_values([\"pool\",\"ts\"]).groupby(\"pool\", sort=False):\n",
    "        z_cus_all.append(_cusum_score(g[\"dev\"]))\n",
    "    z_cus = pd.concat(z_cus_all).reindex(tail.index).fillna(0.0).to_numpy()\n",
    "    z_cus = _scale_01(z_cus)\n",
    "    try:\n",
    "        z_ae = _autoencoder_scores(tail, Xs)\n",
    "    except Exception:\n",
    "        z_ae = np.zeros(len(tail))\n",
    "    fused = np.nanmax(np.vstack([z_if, z_lof, z_ocsvm, z_cus, z_ae]), axis=0)\n",
    "    for k, arr in [(\"z_if\",z_if),(\"z_lof\",z_lof),(\"z_ocsvm\",z_ocsvm),(\"z_cusum\",z_cus),(\"z_ae\",z_ae),(\"anom_fused\",fused)]:\n",
    "        tail[k] = arr\n",
    "    live = load_live()\n",
    "    cols = [\"z_if\",\"z_lof\",\"z_ocsvm\",\"z_cusum\",\"z_ae\",\"anom_fused\"]\n",
    "    live.loc[tail.index, cols] = tail[cols]\n",
    "    write_live(live)\n",
    "    return tail.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "56c9a9ae-dd8f-41aa-ba29-5ea97e357f4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _gen_series_step(prev, vol=1e-4, mean=0.0, jump_prob=0.02, jump_bps=30.0, floor=None, ceil=None):\n",
    "    if prev is None or not np.isfinite(prev): prev = 1.0\n",
    "    step = np.random.normal(mean, vol)\n",
    "    shock = np.random.normal(0.0, jump_bps)/1e4 if (np.random.rand() < float(jump_prob)) else 0.0\n",
    "    nxt = prev * (1.0 + step + shock)\n",
    "    if floor is not None: nxt = max(floor, nxt)\n",
    "    if ceil  is not None: nxt = min(ceil,  nxt)\n",
    "    return float(nxt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "c04e5de9-7193-4a9a-a711-1c46b701cf31",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hyperbootstrap(target_rows: int = 800, step_seconds: int = 15, batch_size: int = 500):\n",
    "    \"\"\"\n",
    "    Fill live_dataset.csv to 'target_rows' quickly with realistic synthetic ticks.\n",
    "    Uses current state for seed; writes in big batches for speed; runs zoo+train once at end.\n",
    "    \"\"\"\n",
    "    try: latest_real = sample_once_parallel()\n",
    "    except Exception: latest_real = sample_once()\n",
    "    live0 = load_live(); have = int(len(live0))\n",
    "    if have >= target_rows:\n",
    "        print(f\"[hyperbootstrap] already have {have} rows; nothing to do.\")\n",
    "        return\n",
    "    latest = load_live().sort_values(\"ts\").groupby(\"pool\").tail(1)\n",
    "    if latest.empty: latest = latest_real\n",
    "    pools = list(latest[\"pool\"].unique()); P = max(1, len(pools))\n",
    "    steps_needed = int(math.ceil((target_rows - have)/P))\n",
    "    print(f\"[hyperbootstrap] need ~{steps_needed} synthetic steps ({P} pools) → target_rows={target_rows}\")\n",
    "    state = {}\n",
    "    for _, r in latest.iterrows():\n",
    "        p = r[\"pool\"]\n",
    "        state[p] = {\n",
    "            \"dex_spot\":      float(r.get(\"dex_spot\", 1.0)),\n",
    "            \"oracle_px\":     float(r.get(\"oracle_px\", 1.0)),\n",
    "            \"virtual_price\": float(r.get(\"virtual_price\", 1.0)),\n",
    "            \"r0\":            float(r.get(\"r0\", 1e9)),\n",
    "            \"r1\":            float(r.get(\"r1\", 1e9)),\n",
    "        }\n",
    "    now = datetime.now(timezone.utc)\n",
    "    start = now - timedelta(seconds=steps_needed * step_seconds)\n",
    "    buf = []\n",
    "    for i in range(steps_needed):\n",
    "        ts_i = (start + timedelta(seconds=i * step_seconds)).isoformat(timespec=\"seconds\")\n",
    "        for p in pools:\n",
    "            st = state[p]\n",
    "            new_oracle = _gen_series_step(st[\"oracle_px\"],     vol=2e-5,  jump_prob=0.01, jump_bps=5.0,  floor=0.95, ceil=1.05)\n",
    "            new_spot   = _gen_series_step(st[\"dex_spot\"],      vol=6e-5,  jump_prob=0.03, jump_bps=40., floor=0.90, ceil=1.10)\n",
    "            new_vp     = _gen_series_step(st[\"virtual_price\"], vol=1.5e-5,jump_prob=0.005,jump_bps=3.0, floor=0.98, ceil=1.02)\n",
    "            r0 = float(st[\"r0\"] * (1.0 + np.random.normal(0, 2e-3)))\n",
    "            r1 = float(st[\"r1\"] * (1.0 + np.random.normal(0, 2e-3)))\n",
    "            buf.append({\"ts\": ts_i, \"pool\": p, \"dex_spot\": new_spot, \"oracle_px\": new_oracle,\n",
    "                        \"virtual_price\": new_vp, \"r0\": r0, \"r1\": r1, \"block\": None})\n",
    "            st[\"dex_spot\"], st[\"oracle_px\"], st[\"virtual_price\"], st[\"r0\"], st[\"r1\"] = new_spot, new_oracle, new_vp, r0, r1\n",
    "        if len(buf) >= batch_size or (i+1) == steps_needed:\n",
    "            feat = engineer_features(buf); append_live(feat); buf = []\n",
    "            if (i+1) % (batch_size*1) == 0: print(f\"[hyperbootstrap] step {i+1}/{steps_needed}\")\n",
    "    run_anomaly_zoo_update_live_fast(tail_win=FAST_CFG[\"TAIL_WIN\"])\n",
    "    df_all = load_live()\n",
    "    if \"y_10m\" not in df_all.columns or df_all[\"y_10m\"].nunique() < 2:\n",
    "        df_all[\"y_10m\"] = _ensure_labels(df_all); write_live(df_all)\n",
    "    try: train_forecaster_10m()\n",
    "    except Exception as e: print(f\"[hyperbootstrap] forecaster train skipped: {e}\")\n",
    "    score_latest_10m(write_parquet=False)\n",
    "    explain_forecast_10m()\n",
    "    write_run_meta({\"bootstrap\": True})\n",
    "    print(\"[hyperbootstrap] complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "fe3971fc-d7d3-41f8-9fb3-7836f5b17d8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_loop_i = 0\n",
    "def job_sample_and_score_fast():\n",
    "    \"\"\"Hot path every loop; stagger heavy stuff.\"\"\"\n",
    "    global _loop_i\n",
    "    _loop_i += 1\n",
    "    t0 = time.time()\n",
    "    sample_once_parallel()\n",
    "    run_anomaly_zoo_update_live_fast(FAST_CFG[\"TAIL_WIN\"])\n",
    "    score_latest_10m(write_parquet=False)\n",
    "    if _loop_i % FAST_CFG[\"NETWORK_EVERY\"] == 0:\n",
    "        compute_network_features()\n",
    "    if _loop_i % FAST_CFG[\"EVENTS_EVERY\"] == 0:\n",
    "        update_events_from_sources()\n",
    "        enrich_events_and_aggregate()\n",
    "    if _loop_i % TRAIN_EVERY == 0:\n",
    "        try: train_forecaster_10m()\n",
    "        except Exception: pass\n",
    "    if _loop_i % FAST_CFG[\"META_EVERY\"] == 0:\n",
    "        write_run_meta()\n",
    "    dt = time.time() - t0\n",
    "    sleep_s = max(5, LOOP_SECS - dt)\n",
    "    print(f\"[loop {_loop_i}] {dt:.2f}s → sleep {sleep_s:.2f}s\")\n",
    "    time.sleep(sleep_s) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "77335d2e-a8a4-4a1a-bb3e-283d1f41025a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[append] 3 rows → C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\live_dataset.csv\n",
      "[hyperbootstrap] already have 2240 rows; nothing to do.\n",
      "[append] 3 rows → C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\live_dataset.csv\n",
      "[loop 1] 1.45s → sleep 28.55s\n",
      "[append] 3 rows → C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\live_dataset.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\2667498210.py:52: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\2667498210.py:52: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\2667498210.py:52: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\2667498210.py:52: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[network] v2 updated neighbor features (no NaNs)\n",
      "[ok] RUN_META.json → C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\RUN_META.json\n",
      "[loop 2] 2.27s → sleep 27.73s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[133], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m hyperbootstrap(target_rows\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m800\u001b[39m, step_seconds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m500\u001b[39m)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m----> 4\u001b[0m     job_sample_and_score_fast()\n",
      "Cell \u001b[1;32mIn[132], line 23\u001b[0m, in \u001b[0;36mjob_sample_and_score_fast\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m sleep_s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m5\u001b[39m, LOOP_SECS \u001b[38;5;241m-\u001b[39m dt)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[loop \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_loop_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms → sleep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msleep_s\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(sleep_s)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    hyperbootstrap(target_rows=800, step_seconds=15, batch_size=500)\n",
    "    while True:\n",
    "        job_sample_and_score_fast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "486b6f85-d8e6-4560-9f69-28fc2477ebb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[append] 3 rows → C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\live_dataset.csv\n",
      "[loop 3] 1.62s → sleep 28.38s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[134], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m job_sample_and_score_fast()\n",
      "Cell \u001b[1;32mIn[132], line 23\u001b[0m, in \u001b[0;36mjob_sample_and_score_fast\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m sleep_s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m5\u001b[39m, LOOP_SECS \u001b[38;5;241m-\u001b[39m dt)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[loop \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_loop_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms → sleep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msleep_s\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(sleep_s)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "job_sample_and_score_fast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "f0c47f7f-6f4d-43cc-8660-1a0a37dd3cd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "e420155b-1557-409b-bf9e-75fd81956d3d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[append] 3 rows → C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\live_dataset.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\2667498210.py:52: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\2667498210.py:52: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\2667498210.py:52: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\2667498210.py:52: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[network] v2 updated neighbor features (no NaNs)\n",
      "[ok] RUN_META.json → C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\RUN_META.json\n",
      "[loop 4] 2.15s → sleep 57.85s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[136], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m FAST_CFG[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMETA_EVERY\u001b[39m\u001b[38;5;124m\"\u001b[39m]    \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m4\u001b[39m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):     \n\u001b[1;32m----> 7\u001b[0m     job_sample_and_score_fast()\n",
      "Cell \u001b[1;32mIn[132], line 23\u001b[0m, in \u001b[0;36mjob_sample_and_score_fast\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m sleep_s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m5\u001b[39m, LOOP_SECS \u001b[38;5;241m-\u001b[39m dt)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[loop \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_loop_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms → sleep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msleep_s\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(sleep_s)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "LOOP_SECS = 60          \n",
    "TRAIN_EVERY = 240       \n",
    "FAST_CFG[\"EVENTS_EVERY\"]  = 12\n",
    "FAST_CFG[\"NETWORK_EVERY\"] = 4\n",
    "FAST_CFG[\"META_EVERY\"]    = 4\n",
    "for _ in range(30):     \n",
    "    job_sample_and_score_fast()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "5e8c5327-07ca-42ad-995f-0bde0335e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "if _loop_i % TRAIN_EVERY == 0:\n",
    "    try:\n",
    "        train_if_needed(force=False, min_hours=4.0, min_new_rows=400, label_drift_thr=0.15)\n",
    "    except Exception as e:\n",
    "        print(\"[train_if_needed] error:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "d00adbac-e58a-4277-b38f-9c54a2592853",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[rotate] skip: name 'rotate_live_csv' is not defined\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    rotate_live_csv(max_mb=100)\n",
    "except Exception as e:\n",
    "    print(\"[rotate] skip:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "34591f00-eadd-4010-a1fc-d69ca4eec75a",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEBHOOK = os.environ.get(\"ALERT_WEBHOOK\", \"\").strip()\n",
    "def alerts_enabled() -> bool:\n",
    "    return bool(WEBHOOK and WEBHOOK.startswith(\"http\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "cfea38ea-28cb-4d18-a649-84f24a028052",
   "metadata": {},
   "outputs": [],
   "source": [
    "WEBHOOK = \"https://hooks.slack.com/services/XXX/YYY/ZZZ\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "49cde998-d930-493f-9978-0df89a4bf4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "if alerts_enabled() and (_loop_i % 3 == 0):\n",
    "    try:\n",
    "        trigger_alerts_if_needed(WEBHOOK, n_tail=80, min_rows_for_incidents=60)\n",
    "    except Exception as e:\n",
    "        print(\"[alert] skip:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "973d87f0-fb7b-477c-9183-ede2201436c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    WEBHOOK\n",
    "except NameError:\n",
    "    WEBHOOK = \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "id": "64f3f5c9-22ad-4a88-8473-23bce285836b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_alerts_if_needed(webhook_url: str, n_tail: int = 80, min_rows_for_incidents: int = 60) -> bool:\n",
    "    \"\"\"Post 'red' incidents to a webhook; dedup + cooldown built in.\"\"\"\n",
    "    if not webhook_url:\n",
    "        print(\"[alert] no webhook configured\")\n",
    "        return False\n",
    "    try:\n",
    "        import requests\n",
    "    except Exception as e:\n",
    "        print(\"[alert] requests not available:\", e)\n",
    "        return False\n",
    "    try:\n",
    "        dec = decide_latest(n_tail=n_tail, min_rows_for_incidents=min_rows_for_incidents)\n",
    "    except Exception as e:\n",
    "        print(\"[alert] decide_latest failed:\", e)\n",
    "        return False\n",
    "    reds = dec[dec[\"level\"] == \"red\"] if isinstance(dec, pd.DataFrame) else pd.DataFrame()\n",
    "    if reds.empty:\n",
    "        print(\"[alert] no red alerts\")\n",
    "        return True\n",
    "    payload = reds.to_dict(orient=\"records\")\n",
    "    envelope = {\"alerts\": payload, \"ts\": _now_iso()}\n",
    "    blob = json.dumps(envelope, sort_keys=True)\n",
    "    h = hashlib.sha1(blob.encode()).hexdigest()\n",
    "    now = time.time()\n",
    "    if _last_alert[\"hash\"] == h and (now - _last_alert[\"ts\"] < ALERT_COOLDOWN):\n",
    "        print(\"[alert] suppressed (duplicate/cooldown)\")\n",
    "        return True\n",
    "    try:\n",
    "        r = requests.post(webhook_url, json=envelope, timeout=8)\n",
    "        print(f\"[alert] posted {len(payload)} red alerts → {r.status_code}\")\n",
    "        if 200 <= r.status_code < 300:\n",
    "            _last_alert.update({\"ts\": now, \"hash\": h})\n",
    "            return True\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(\"[alert] post failed:\", e)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "0c3bfab6-7502-44f7-aded-d76fd5766e2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "if WEBHOOK and (_loop_i % 3 == 0):\n",
    "    trigger_alerts_if_needed(WEBHOOK, n_tail=80, min_rows_for_incidents=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "a9142d8c-8228-4894-bab2-0c8b97dfa30f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[explain] wrote C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\explain.json\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\2667498210.py:52: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\2667498210.py:52: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\2667498210.py:52: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\2667498210.py:52: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[network] v2 updated neighbor features (no NaNs)\n",
      "[ok] Analyst Note exported → C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\analyst_note.pdf\n",
      "{'ok': True, 'note': 'Fused anomaly now=0.91; 10-min risk=1.00; 30-min risk=0.86. Confidence High. Top drivers: dev_roll_std (+0.0812±0.0074 AP); oracle_ratio (+0.0109±0.0020 AP); spot_twap_gap_bps (+0.0015±0.0032 AP) Propagation: DAI/USDC_univ3 lags peers (corr=0.80). Action: monitor in 5m; if risk > 0.60 or 3 consecutive reds, engage mitigation (widen slippage / reroute).', 'pdf': 'C:\\\\Users\\\\aniru\\\\OneDrive\\\\Desktop\\\\ML tutorial\\\\DEFI Depeg sentinel\\\\analyst_note.pdf'}\n",
      "[report] decide_latest failed: name 'decide_latest' is not defined\n",
      "[ok] Markdown PDF exported → C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\report.pdf\n",
      "{'ok': True, 'markdown': '# Nightly Model Report\\nGenerated: 2025-08-28T21:03:42+00:00\\n\\n## Winner Detector Today\\n- **Winner:** `z_ae`  (by AP proxy on |dev|≥0.3%)\\n\\nAP scores:\\n- z_ae: 0.787\\n- z_if: 0.739\\n- z_ocsvm: 0.524\\n- z_lof: 0.330\\n- anom_fused: 0.167\\n- z_cusum: 0.164\\n\\n## Forecast Calibration (10m)\\n- bin≈0.56: observed 1.00 (n=16)\\n- bin≈0.70: observed 1.00 (n=20)\\n- bin≈0.87: observed 1.00 (n=24)\\n\\n## Incidents (last window)\\n- None...', 'pdf': 'C:\\\\Users\\\\aniru\\\\OneDrive\\\\Desktop\\\\ML tutorial\\\\DEFI Depeg sentinel\\\\report.pdf'}\n",
      "[ok] RUN_META.json → C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\RUN_META.json\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/Users/aniru/OneDrive/Desktop/ML tutorial/DEFI Depeg sentinel/RUN_META.json')"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "try:\n",
    "    explain_forecast_10m(n_repeats=6)\n",
    "    print(build_analyst_note_v2())\n",
    "    print(nightly_report())\n",
    "except Exception as e:\n",
    "    print(\"[nightly] skip:\", e)\n",
    "write_run_meta({\"stopped_at_loop\": int(_loop_i), \"cadence_s\": int(LOOP_SECS)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "ef89c026-aca5-4897-9e6b-4679039f35b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "if \"risk_forecast_10m\" not in CANON_COLS:\n",
    "    CANON_COLS.append(\"risk_forecast_10m\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "83ce084d-2591-484c-b4f9-4419751ebd36",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_forecast_to_live(n_tail: int = 200):\n",
    "    f = score_latest_10m(n_tail=n_tail, write_parquet=False)[[\"ts\",\"pool\",\"risk_forecast_10m\"]]\n",
    "    live = load_live()\n",
    "    if live.empty or f.empty:\n",
    "        print(\"[attach] nothing to merge\"); return live\n",
    "    live[\"ts_str\"] = _iso_str(live[\"ts\"])\n",
    "    f[\"ts_str\"]    = _iso_str(f[\"ts\"])\n",
    "    live_idx = live.set_index([\"pool\",\"ts_str\"])\n",
    "    upd_idx  = f.set_index([\"pool\",\"ts_str\"])[[\"risk_forecast_10m\"]]\n",
    "    live_idx.update(upd_idx)\n",
    "    live_updated = live_idx.reset_index().drop(columns=[\"ts_str\"])\n",
    "    write_live(live_updated)\n",
    "    return live_updated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "992797c5-a914-43d3-ad13-49cf79c6b2b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                            ts                       pool       dev  \\\n",
      "2250 2025-08-28 21:02:23+00:00             DAI/USDC_univ3       NaN   \n",
      "2249 2025-08-28 21:02:23+00:00            USDC/USDT_univ3       NaN   \n",
      "2251 2025-08-28 21:02:23+00:00                3pool_curve -0.000107   \n",
      "170                        NaT  2025-08-23 22:38:02+00:00 -0.000161   \n",
      "171                        NaT  2025-08-23 22:38:02+00:00 -0.000122   \n",
      "172                        NaT  2025-08-23 22:38:02+00:00 -0.000617   \n",
      "\n",
      "      anom_fused  risk_forecast_10m  feeds_fresh  \n",
      "2250    0.018041           0.747306          1.0  \n",
      "2249    1.000000           0.830525          1.0  \n",
      "2251    0.910041           1.000000          1.0  \n",
      "170     1.000000                NaN          NaN  \n",
      "171     1.000000                NaN          NaN  \n",
      "172     1.000000                NaN          NaN  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\2113058907.py:10: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n"
     ]
    }
   ],
   "source": [
    "live2 = attach_forecast_to_live(n_tail=200)\n",
    "print(live2.sort_values(\"ts\").tail(6)[[\"ts\",\"pool\",\"dev\",\"anom_fused\",\"risk_forecast_10m\",\"feeds_fresh\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "d397438b-badd-47d0-a60c-0630054b0278",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[resume] TWAP seeds: 4 pools\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\2667498210.py:52: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\2667498210.py:52: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\2667498210.py:52: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\2667498210.py:52: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  live_idx.update(upd_idx)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[network] v2 updated neighbor features (no NaNs)\n",
      "[explain] wrote C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\explain.json\n",
      "[resume] state seeded from live; continue cruising.\n"
     ]
    }
   ],
   "source": [
    "def seed_state_from_live():\n",
    "    df = load_live()\n",
    "    if df.empty:\n",
    "        print(\"[resume] no live data yet; nothing to seed\")\n",
    "        return False\n",
    "    last = df.sort_values(\"ts\").groupby(\"pool\", as_index=False).tail(1)\n",
    "    seeded = 0\n",
    "    for _, r in last.iterrows():\n",
    "        p = r[\"pool\"]\n",
    "        tw = r.get(\"dex_twap\")\n",
    "        sp = r.get(\"dex_spot\")\n",
    "        if pd.notna(tw):\n",
    "            _TWAP.state[p] = float(tw); seeded += 1\n",
    "        elif pd.notna(sp):\n",
    "            _TWAP.state[p] = float(sp); seeded += 1\n",
    "    print(f\"[resume] TWAP seeds: {seeded} pools\")\n",
    "    try:\n",
    "        compute_network_features()\n",
    "    except Exception as e:\n",
    "        print(\"[resume] network seed skipped:\", e)\n",
    "    try:\n",
    "        explain_forecast_10m(n_repeats=4)\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(\"[resume] state seeded from live; continue cruising.\")\n",
    "    return True\n",
    "_ = seed_state_from_live()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "5aa47500-ce41-40b9-aba9-c86df8318c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOOP_SECS   = 60      \n",
    "TRAIN_EVERY = 240      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "e6a1d7a4-0fa0-4a9a-8265-de2831b9243c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "WEBHOOK = os.environ.get(\"ALERT_WEBHOOK\", \"\").strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "658b6a4c-904a-4632-b371-f87d6cbeb192",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[append] 3 rows → C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\live_dataset.csv\n",
      "[loop 5] 0.97s → sleep 59.03s\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[152], line 4\u001b[0m\n\u001b[0;32m      2\u001b[0m     _loop_i \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m30\u001b[39m):\n\u001b[1;32m----> 4\u001b[0m     job_sample_and_score_fast()  \n\u001b[0;32m      5\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _loop_i \u001b[38;5;241m%\u001b[39m TRAIN_EVERY \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m      6\u001b[0m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "Cell \u001b[1;32mIn[132], line 23\u001b[0m, in \u001b[0;36mjob_sample_and_score_fast\u001b[1;34m()\u001b[0m\n\u001b[0;32m     21\u001b[0m sleep_s \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(\u001b[38;5;241m5\u001b[39m, LOOP_SECS \u001b[38;5;241m-\u001b[39m dt)\n\u001b[0;32m     22\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[loop \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_loop_i\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mdt\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms → sleep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msleep_s\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124ms\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m time\u001b[38;5;241m.\u001b[39msleep(sleep_s)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if '_loop_i' not in globals():\n",
    "    _loop_i = 0\n",
    "for _ in range(30):\n",
    "    job_sample_and_score_fast()  \n",
    "    if _loop_i % TRAIN_EVERY == 0:\n",
    "        try:\n",
    "            train_if_needed(force=False, min_hours=4.0, min_new_rows=400, label_drift_thr=0.15)\n",
    "        except Exception as e:\n",
    "            print(\"[train_if_needed] error:\", e)\n",
    "    if WEBHOOK and (_loop_i % 3 == 0):\n",
    "        try:\n",
    "            trigger_alerts_if_needed(WEBHOOK, n_tail=80, min_rows_for_incidents=60)\n",
    "        except Exception as e:\n",
    "            print(\"[alert] skip:\", e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "3091efda-6911-490d-840f-d0e4dd2bee42",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime, timezone\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "20d232f1-b523-4b26-8fd8-c4f35b048b09",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _utc_now_ts() -> int:\n",
    "    return int(datetime.now(timezone.utc).timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "3e2e9733-2c34-46ba-9bd4-d622847bc97b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feeds_fresh_gate(heartbeat_s: int = 120, max_block_lag_s: int = 120) -> dict:\n",
    "    \"\"\"\n",
    "    Checks:\n",
    "      - Each Chainlink feed's `updatedAt` against now.\n",
    "      - Latest on-chain block timestamp (when not mock).\n",
    "    Returns dict with details + a boolean 'feeds_fresh'.\n",
    "    \"\"\"\n",
    "    details = {\"now\": _utc_now_ts(), \"feeds\": [], \"block\": {}, \"feeds_fresh\": True}\n",
    "    for sym, addr in getattr(CFG, \"chainlink_feeds\", {}).items():\n",
    "        r = onchain.get_oracle_price(addr)\n",
    "        upd = int(r.data.get(\"updatedAt\", 0)) if (r and r.ok and \"updatedAt\" in r.data) else 0\n",
    "        lag = details[\"now\"] - upd if upd > 0 else math.inf\n",
    "        ok  = (lag <= heartbeat_s)\n",
    "        details[\"feeds\"].append({\"symbol\": sym, \"updatedAt\": upd, \"lag_s\": lag, \"ok\": ok})\n",
    "        if not ok:\n",
    "            details[\"feeds_fresh\"] = False\n",
    "    try:\n",
    "        if (getattr(onchain, \"mock\", True) is False) and getattr(onchain, \"w3\", None) is not None:\n",
    "            latest = onchain.w3.eth.get_block(\"latest\")\n",
    "            blk_ts = int(latest.timestamp)\n",
    "            blk_lag = details[\"now\"] - blk_ts\n",
    "            details[\"block\"] = {\"timestamp\": blk_ts, \"lag_s\": blk_lag, \"ok\": blk_lag <= max_block_lag_s}\n",
    "            if blk_lag > max_block_lag_s:\n",
    "                details[\"feeds_fresh\"] = False\n",
    "        else:\n",
    "            details[\"block\"] = {\"timestamp\": None, \"lag_s\": None, \"ok\": True}\n",
    "    except Exception as e:\n",
    "        details[\"block\"] = {\"timestamp\": None, \"lag_s\": None, \"ok\": False, \"err\": str(e)}\n",
    "        details[\"feeds_fresh\"] = False\n",
    "    return details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "8241ff18-50c0-4824-9151-8c2d71dee43e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stamp_feeds_fresh_on_live(heartbeat_s: int = 120, max_block_lag_s: int = 120) -> dict:\n",
    "    \"\"\"Writes `feeds_fresh` onto the **latest row per pool** in live CSV (True/False).\"\"\"\n",
    "    gate = compute_feeds_fresh_gate(heartbeat_s=heartbeat_s, max_block_lag_s=max_block_lag_s)\n",
    "    df = load_live()\n",
    "    if df.empty:\n",
    "        return gate\n",
    "    latest = df.sort_values(\"ts\").groupby(\"pool\", as_index=False).tail(1).copy()\n",
    "    latest[\"feeds_fresh\"] = bool(gate[\"feeds_fresh\"])\n",
    "    live = load_live()\n",
    "    live[\"ts_str\"] = live[\"ts\"].dt.strftime(\"%Y-%m-%d %H:%M:%S%z\")\n",
    "    latest[\"ts_str\"] = latest[\"ts\"].dt.strftime(\"%Y-%m-%d %H:%M:%S%z\")\n",
    "    idx_live = live.set_index([\"pool\",\"ts_str\"])\n",
    "    idx_upd  = latest.set_index([\"pool\",\"ts_str\"])[[\"feeds_fresh\"]]\n",
    "    idx_live.update(idx_upd)\n",
    "    out = idx_live.reset_index().drop(columns=[\"ts_str\"])\n",
    "    write_live(out)\n",
    "    return gate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "dd757d7e-6376-4606-aa86-806a6286e379",
   "metadata": {},
   "outputs": [],
   "source": [
    "_UNIV3_OBS_ABI = [\n",
    "    {\"name\":\"observe\",\"inputs\":[{\"type\":\"uint32[]\",\"name\":\"secondsAgos\"}],\n",
    "     \"outputs\":[{\"type\":\"int56[]\",\"name\":\"tickCumulatives\"},\n",
    "                {\"type\":\"uint160[]\",\"name\":\"secondsPerLiquidityCumulativeX128\"}],\n",
    "     \"stateMutability\":\"view\",\"type\":\"function\"},\n",
    "    {\"name\":\"token0\",\"inputs\":[],\"outputs\":[{\"type\":\"address\"}], \"stateMutability\":\"view\",\"type\":\"function\"},\n",
    "    {\"name\":\"token1\",\"inputs\":[],\"outputs\":[{\"type\":\"address\"}], \"stateMutability\":\"view\",\"type\":\"function\"},\n",
    "]\n",
    "_ERC20_DEC_ABI = [\n",
    "    {\"name\":\"decimals\",\"inputs\":[],\"outputs\":[{\"type\":\"uint8\"}], \"stateMutability\":\"view\",\"type\":\"function\"},\n",
    "    {\"name\":\"symbol\",\"inputs\":[],\"outputs\":[{\"type\":\"string\"}],   \"stateMutability\":\"view\",\"type\":\"function\"},\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "id": "9b98cc7e-a7b4-4eb0-a695-25547c488dac",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tick_to_price_01(tick: float) -> float:\n",
    "    return float(1.0001 ** tick)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "id": "b62b4705-53e8-477f-b4d5-9ccb02d287ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_univ3_twap_price(pool_addr: str, window_s: int = 300) -> float:\n",
    "    \"\"\"\n",
    "    Real TWAP over `window_s` using Uniswap v3 observe().\n",
    "    Returns price accounting for token decimals (token0/token1).\n",
    "    \"\"\"\n",
    "    if getattr(onchain, \"mock\", True) or not getattr(onchain, \"w3\", None):\n",
    "        uni = onchain.get_univ3_price(pool_addr)\n",
    "        return float(uni.data.get(\"price\", np.nan)) if (uni and uni.ok) else np.nan\n",
    "    from web3 import Web3\n",
    "    pool = Web3.to_checksum_address(pool_addr)\n",
    "    c = onchain.w3.eth.contract(address=pool, abi=_UNIV3_OBS_ABI)\n",
    "    secs = [window_s, 0]\n",
    "    tc, _spl = c.functions.observe(secs).call()\n",
    "    tick_avg = (tc[-1] - tc[0]) / float(window_s)\n",
    "    t0 = c.functions.token0().call()\n",
    "    t1 = c.functions.token1().call()\n",
    "    t0c = onchain.w3.eth.contract(address=t0, abi=_ERC20_DEC_ABI)\n",
    "    t1c = onchain.w3.eth.contract(address=t1, abi=_ERC20_DEC_ABI)\n",
    "    d0 = int(t0c.functions.decimals().call())\n",
    "    d1 = int(t1c.functions.decimals().call())\n",
    "    p_01 = _tick_to_price_01(tick_avg)            \n",
    "    price = p_01 * (10 ** (d0 - d1))              \n",
    "    return float(price)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "id": "3271f07d-76c3-4a3b-b359-87b5ae38a24d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def refresh_true_twap_and_gap(window_s: int = 300) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    For Uniswap v3 pools in CFG.pools:\n",
    "      - compute real TWAP via observe()\n",
    "      - update latest row's `dex_twap` and `spot_twap_gap_bps = (dex_spot - dex_twap)/dex_twap * 1e4`\n",
    "    \"\"\"\n",
    "    df = load_live()\n",
    "    if df.empty:\n",
    "        return df\n",
    "    df = df.sort_values(\"ts\")\n",
    "    latest = df.groupby(\"pool\", as_index=False).tail(1).copy()\n",
    "    updates = []\n",
    "    for name, meta in getattr(CFG, \"pools\", {}).items():\n",
    "        if meta.get(\"type\") != \"uniswap_v3\":\n",
    "            continue\n",
    "        addr = meta[\"address\"]\n",
    "        try:\n",
    "            twap = get_univ3_twap_price(addr, window_s=window_s)\n",
    "        except Exception:\n",
    "            twap = np.nan\n",
    "        sp = latest.loc[latest[\"pool\"] == name, \"dex_spot\"].astype(float).fillna(np.nan)\n",
    "        sp = float(sp.iloc[0]) if len(sp) else np.nan\n",
    "        gap_bps = ((sp - twap) / twap * 1e4) if (np.isfinite(sp) and np.isfinite(twap) and twap != 0) else np.nan\n",
    "        updates.append({\"pool\": name, \"dex_twap\": twap, \"spot_twap_gap_bps\": gap_bps})\n",
    "    if not updates:\n",
    "        return df.tail(6)\n",
    "    upd = pd.DataFrame(updates)\n",
    "    latest = latest.drop(columns=[\"dex_twap\",\"spot_twap_gap_bps\"], errors=\"ignore\").merge(upd, on=\"pool\", how=\"left\")\n",
    "    live = load_live()\n",
    "    live[\"ts_str\"]   = live[\"ts\"].dt.strftime(\"%Y-%m-%d %H:%M:%S%z\")\n",
    "    latest[\"ts_str\"] = latest[\"ts\"].dt.strftime(\"%Y-%m-%d %H:%M:%S%z\")\n",
    "    idx_live = live.set_index([\"pool\",\"ts_str\"])\n",
    "    idx_upd  = latest.set_index([\"pool\",\"ts_str\"])[[\"dex_twap\",\"spot_twap_gap_bps\"]]\n",
    "    idx_live.update(idx_upd)\n",
    "    out = idx_live.reset_index().drop(columns=[\"ts_str\"])\n",
    "    write_live(out)\n",
    "    return out.sort_values(\"ts\").tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "c19b2d96-1036-4fdb-a3a7-d8ee76044e77",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\1776474418.py:1: DeprecationWarning: Importing FieldValidationInfo from `pydantic` is deprecated. This feature is either no longer supported, or is not public.\n",
      "  from pydantic import BaseModel, Field, field_validator, ConfigDict, FieldValidationInfo\n"
     ]
    }
   ],
   "source": [
    "from pydantic import BaseModel, Field, field_validator, ConfigDict, FieldValidationInfo\n",
    "from typing import Optional, Any\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "9af534bd-c9d6-4d76-86e2-17d0ab03aa22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_na(v: Any) -> bool:\n",
    "    try:\n",
    "        if v is None: return True\n",
    "        if isinstance(v, float) and (np.isnan(v) or np.isinf(v)): return True\n",
    "        if isinstance(v, str) and v.strip() == \"\": return True\n",
    "        return bool(pd.isna(v))\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "id": "9f4dcb73-4dc7-4f5f-8efb-88c712eb67d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LiveRowModel(BaseModel):\n",
    "    model_config = ConfigDict(extra='allow')  \n",
    "    ts: str\n",
    "    pool: str\n",
    "    dex_spot: Optional[float] = None\n",
    "    oracle_px: Optional[float] = None\n",
    "    dev: Optional[float] = None\n",
    "    dex_twap: Optional[float] = None\n",
    "    oracle_ratio: Optional[float] = None\n",
    "    dev_roll_std: Optional[float] = Field(default=None, ge=0)\n",
    "    tvl_outflow_rate: Optional[float] = None\n",
    "    virtual_price: Optional[float] = Field(default=None, ge=0)\n",
    "    spot_twap_gap_bps: Optional[float] = None\n",
    "    r0: Optional[float] = Field(default=None, ge=0)\n",
    "    r1: Optional[float] = Field(default=None, ge=0)\n",
    "    feeds_fresh: bool = False\n",
    "    run_quality_pass: bool = True\n",
    "    y_10m: Optional[int] = Field(default=None, ge=0, le=1)\n",
    "    y_30m: Optional[int] = Field(default=None, ge=0, le=1)\n",
    "    block: Optional[int] = Field(default=None, ge=0)\n",
    "    @field_validator('ts', mode='before')\n",
    "    @classmethod\n",
    "    def _ts_to_str(cls, v: Any) -> str:\n",
    "        if isinstance(v, pd.Timestamp):\n",
    "            v = v.tz_localize('UTC') if v.tzinfo is None else v.tz_convert('UTC')\n",
    "            return v.strftime(\"%Y-%m-%d %H:%M:%S%z\")\n",
    "        return str(v)\n",
    "    @field_validator('dex_spot','oracle_px','dev','dex_twap','oracle_ratio',\n",
    "                     'dev_roll_std','tvl_outflow_rate','virtual_price',\n",
    "                     'spot_twap_gap_bps','r0','r1', mode='before')\n",
    "    @classmethod\n",
    "    def _float_nan_to_none(cls, v: Any):\n",
    "        if _is_na(v): return None\n",
    "        try: return float(v)\n",
    "        except Exception: return None\n",
    "    @field_validator('y_10m','y_30m','block', mode='before')\n",
    "    @classmethod\n",
    "    def _int_nan_to_none(cls, v: Any, info: FieldValidationInfo):\n",
    "        if _is_na(v): return None\n",
    "        try:\n",
    "            iv = int(float(v))\n",
    "        except Exception:\n",
    "            return None\n",
    "        if info.field_name in ('y_10m','y_30m'):\n",
    "            return 1 if iv >= 1 else 0\n",
    "        return iv\n",
    "    @field_validator('feeds_fresh','run_quality_pass', mode='before')\n",
    "    @classmethod\n",
    "    def _boolify(cls, v: Any) -> bool:\n",
    "        if isinstance(v, (bool, np.bool_)): return bool(v)\n",
    "        if _is_na(v): return False\n",
    "        s = str(v).strip().lower()\n",
    "        if s in ('1','true','yes','y','on'): return True\n",
    "        if s in ('0','false','no','n','off','nan','none',''): return False\n",
    "        try: return bool(int(float(v)))\n",
    "        except Exception: return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "id": "3fd2571c-5044-4f78-accd-bab5d3e12c50",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _coerce_bool_series(s: pd.Series) -> pd.Series:\n",
    "    return s.apply(lambda v: True if str(v).strip().lower() in ('1','true','yes','y','on')\n",
    "                   else False if str(v).strip().lower() in ('0','false','no','n','off','nan','none','')\n",
    "                   else (bool(v) if isinstance(v, (bool, np.bool_)) else False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "617e9136-1cc0-4dcd-a705-38a7f4763ded",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _coerce_df_min(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for c in (\"ts\",\"pool\",\"dex_spot\",\"oracle_px\",\"dev\",\"feeds_fresh\",\"run_quality_pass\"):\n",
    "        if c not in df.columns:\n",
    "            df[c] = np.nan if c not in (\"feeds_fresh\",\"run_quality_pass\") else False\n",
    "    if not pd.api.types.is_string_dtype(df[\"ts\"]):\n",
    "        df[\"ts\"] = pd.to_datetime(df[\"ts\"], utc=True, errors=\"coerce\")\n",
    "    df[\"ts\"] = df[\"ts\"].apply(lambda x: x.strftime(\"%Y-%m-%d %H:%M:%S%z\") if isinstance(x, pd.Timestamp) else str(x))\n",
    "    for b in (\"feeds_fresh\",\"run_quality_pass\"):\n",
    "        if b in df.columns:\n",
    "            df[b] = _coerce_bool_series(df[b])\n",
    "        else:\n",
    "            df[b] = False\n",
    "    for c in (\"y_10m\",\"y_30m\",\"block\"):\n",
    "        if c in df.columns:\n",
    "            def _coerce_int(v):\n",
    "                if _is_na(v): return None\n",
    "                try:\n",
    "                    iv = int(float(v))\n",
    "                except Exception:\n",
    "                    return None\n",
    "                if c in (\"y_10m\",\"y_30m\"):\n",
    "                    return 1 if iv >= 1 else 0\n",
    "                return iv\n",
    "            df[c] = df[c].apply(_coerce_int)\n",
    "    return df\n",
    "def _validate_rows_only(rows: pd.DataFrame) -> pd.DataFrame:\n",
    "    rows = _coerce_df_min(rows)\n",
    "    out = []\n",
    "    for i, r in rows.iterrows():\n",
    "        try:\n",
    "            m = LiveRowModel(**r.to_dict())  \n",
    "            out.append(m.model_dump())\n",
    "        except Exception as e:\n",
    "            raise ValueError(f\"[append validation] bad row at index {i}: {e}\")\n",
    "    return pd.DataFrame(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "id": "2f37feee-aeb3-4048-aea5-b8f4124e2922",
   "metadata": {},
   "outputs": [],
   "source": [
    "def append_live_validated_rows_only(rows: pd.DataFrame) -> None:\n",
    "    cleaned = _validate_rows_only(rows)\n",
    "    base = load_live()\n",
    "    merged = pd.concat([base, cleaned], ignore_index=True)\n",
    "    write_live(merged)\n",
    "    print(f\"[append+validate] appended {len(cleaned)} rows\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "0692fc09-553b-4f0b-91bb-f73bb9c86209",
   "metadata": {},
   "outputs": [],
   "source": [
    "def migrate_existing_live_to_valid() -> None:\n",
    "    base = load_live()\n",
    "    if base.empty:\n",
    "        print(\"[migrate] no existing rows; nothing to do.\")\n",
    "        return\n",
    "    base = _coerce_df_min(base)\n",
    "    for c, default in [\n",
    "        (\"dev_roll_std\", 0.0),\n",
    "        (\"virtual_price\", None),\n",
    "        (\"dex_twap\", None),\n",
    "        (\"oracle_ratio\", None),\n",
    "        (\"spot_twap_gap_bps\", None),\n",
    "        (\"r0\", None), (\"r1\", None),\n",
    "        (\"y_10m\", None), (\"y_30m\", None),\n",
    "        (\"block\", None),\n",
    "    ]:\n",
    "        if c in base.columns:\n",
    "            if default is None:\n",
    "                base[c] = base[c].apply(lambda v: None if _is_na(v) else v)\n",
    "            else:\n",
    "                base[c] = base[c].apply(lambda v: default if _is_na(v) else v)\n",
    "    cleaned = _validate_rows_only(base)\n",
    "    write_live(cleaned)\n",
    "    print(f\"[migrate] repaired & wrote {len(cleaned)} rows → {LIVE_CSV}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "f61543e7-d29e-4814-bdd9-e620a8d234c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[migrate] repaired & wrote 2255 rows → C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\live_dataset.csv\n",
      "[patch] append_live → append_live_validated_rows_only\n"
     ]
    }
   ],
   "source": [
    "migrate_existing_live_to_valid()\n",
    "try:\n",
    "    _append_live_orig\n",
    "except NameError:\n",
    "    _append_live_orig = append_live\n",
    "def append_live(df):\n",
    "    return append_live_validated_rows_only(df)\n",
    "print(\"[patch] append_live → append_live_validated_rows_only\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "9da1881e-9fec-40f4-bcb8-619673933af3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "97215089-103b-45cf-83d7-f0ad6f6e2d7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def enforce_live_dtypes(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "    for b in (\"feeds_fresh\",\"run_quality_pass\"):\n",
    "        if b in df.columns:\n",
    "            df[b] = pd.Series(df[b]).astype(\"boolean\").fillna(False)\n",
    "    for c in (\"y_10m\",\"y_30m\",\"block\"):\n",
    "        if c in df.columns:\n",
    "            s = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "            if c in (\"y_10m\",\"y_30m\"):\n",
    "                s = s.clip(0,1)  \n",
    "                s = s.fillna(0).astype(\"Int64\")\n",
    "            else:\n",
    "                s = s.round().astype(\"Int64\")\n",
    "            df[c] = s\n",
    "    float_cols = [\n",
    "        \"dex_spot\",\"oracle_px\",\"dev\",\"dex_twap\",\"oracle_ratio\",\"dev_roll_std\",\"tvl_outflow_rate\",\n",
    "        \"virtual_price\",\"spot_twap_gap_bps\",\"r0\",\"r1\",\"r0_delta\",\"r1_delta\",\n",
    "        \"neighbor_max_dev\",\"neighbor_avg_anom\",\"corr_best\",\n",
    "        \"z_if\",\"z_lof\",\"z_ocsvm\",\"z_cusum\",\"z_ae\",\"z_ae_seq\",\n",
    "        \"anom_fused\",\"risk_forecast_10m\",\"risk_forecast_30m\"\n",
    "    ]\n",
    "    for c in float_cols:\n",
    "        if c in df.columns:\n",
    "            df[c] = pd.to_numeric(df[c], errors=\"coerce\")\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "5f737a64-01d8-45c7-93ac-bcbcff65fe80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_labels_fixed_on_live(\n",
    "    dev_thr: float = 0.005,\n",
    "    fused_thr: float = 0.90,\n",
    "    h10: int = 10,\n",
    "    h30: int = 30,\n",
    "    persist_version: bool = True,\n",
    "    fill_only: bool = False,  \n",
    "):\n",
    "    df = load_live()\n",
    "    if df.empty:\n",
    "        print(\"[labels] live empty; nothing to do.\")\n",
    "        return df\n",
    "    df = df.sort_values([\"pool\",\"ts\"]).reset_index(drop=True)\n",
    "    has_fused = \"anom_fused\" in df.columns\n",
    "    def _label_group(g: pd.DataFrame, horizon: int) -> pd.Series:\n",
    "        dev_cond   = g[\"dev\"].abs().fillna(0).ge(dev_thr)\n",
    "        fused_cond = g[\"anom_fused\"].fillna(0).ge(fused_thr) if has_fused else pd.Series(False, index=g.index)\n",
    "        cond = (dev_cond | fused_cond).astype(bool)\n",
    "        any_future = pd.Series(False, index=g.index)\n",
    "        for j in range(1, horizon + 1):\n",
    "            any_future = any_future | cond.shift(-j, fill_value=False)\n",
    "        return any_future.astype(\"Int64\")\n",
    "    y10 = df.groupby(\"pool\", sort=False, group_keys=False).apply(lambda g: _label_group(g, h10))\n",
    "    y30 = df.groupby(\"pool\", sort=False, group_keys=False).apply(lambda g: _label_group(g, h30))\n",
    "    if fill_only:\n",
    "        if \"y_10m\" not in df.columns:\n",
    "            df[\"y_10m\"] = pd.Series([pd.NA]*len(df), dtype=\"Int64\")\n",
    "        if \"y_30m\" not in df.columns:\n",
    "            df[\"y_30m\"] = pd.Series([pd.NA]*len(df), dtype=\"Int64\")\n",
    "        m10 = df[\"y_10m\"].isna()\n",
    "        m30 = df[\"y_30m\"].isna()\n",
    "        df.loc[m10, \"y_10m\"] = y10.loc[m10]\n",
    "        df.loc[m30, \"y_30m\"] = y30.loc[m30]\n",
    "    else:\n",
    "        df[\"y_10m\"] = y10\n",
    "        df[\"y_30m\"] = y30\n",
    "    df = enforce_live_dtypes(df)\n",
    "    write_live(df)\n",
    "    print(f\"[labels] wrote y_10m/y_30m with dev_thr={dev_thr:.4f}, fused_thr={fused_thr:.2f}, h10={h10}, h30={h30}\")\n",
    "    if persist_version:\n",
    "        try:\n",
    "            ver = {\n",
    "                \"ts\": _now_iso(),\n",
    "                \"labeling\": {\n",
    "                    \"y_10m\": {\"dev_thr\": float(dev_thr), \"fused_thr\": float(fused_thr), \"horizon\": int(h10)},\n",
    "                    \"y_30m\": {\"dev_thr\": float(dev_thr), \"fused_thr\": float(fused_thr), \"horizon\": int(h30)},\n",
    "                }\n",
    "            }\n",
    "            (OUT_MODEL / \"version.json\").write_text(json.dumps(ver, indent=2))\n",
    "            print(f\"[labels] persisted thresholds → {OUT_MODEL / 'version.json'}\")\n",
    "        except Exception as e:\n",
    "            print(\"[labels] could not persist version.json:\", e)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "3e7cab16-560b-419c-a54e-fea4d3d5df67",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\644838851.py:4: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  merged = pd.concat([base, cleaned], ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[append+validate] appended 3 rows\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\22724122.py:34: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  idx_live.update(idx_upd)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\22724122.py:34: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  idx_live.update(idx_upd)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\3459606029.py:14: PerformanceWarning: indexing past lexsort depth may impact performance.\n",
      "  idx_live.update(idx_upd)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\3459606029.py:14: FutureWarning: Setting an item of incompatible dtype is deprecated and will raise in a future error of pandas. Value '[nan nan nan ... False False False]' has dtype incompatible with float64, please explicitly cast to a compatible dtype first.\n",
      "  idx_live.update(idx_upd)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\3021236328.py:23: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y10 = df.groupby(\"pool\", sort=False, group_keys=False).apply(lambda g: _label_group(g, h10))\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\3021236328.py:24: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  y30 = df.groupby(\"pool\", sort=False, group_keys=False).apply(lambda g: _label_group(g, h30))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[labels] wrote y_10m/y_30m with dev_thr=0.0050, fused_thr=0.90, h10=10, h30=30\n",
      "[labels] persisted thresholds → C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\model\\version.json\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ts</th>\n",
       "      <th>pool</th>\n",
       "      <th>dex_spot</th>\n",
       "      <th>dex_twap</th>\n",
       "      <th>oracle_ratio</th>\n",
       "      <th>dev</th>\n",
       "      <th>dev_roll_std</th>\n",
       "      <th>tvl_outflow_rate</th>\n",
       "      <th>virtual_price</th>\n",
       "      <th>spot_twap_gap_bps</th>\n",
       "      <th>...</th>\n",
       "      <th>z_cusum</th>\n",
       "      <th>z_ae</th>\n",
       "      <th>anom_fused</th>\n",
       "      <th>y_10m</th>\n",
       "      <th>y_30m</th>\n",
       "      <th>neighbor_max_dev</th>\n",
       "      <th>neighbor_avg_anom</th>\n",
       "      <th>lead_lag_best</th>\n",
       "      <th>corr_best</th>\n",
       "      <th>risk_forecast_10m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2025-08-26 14:57:08+00:00</td>\n",
       "      <td>2025-08-23 22:38:02+00:00</td>\n",
       "      <td>0.999490</td>\n",
       "      <td>0.999490</td>\n",
       "      <td>0.999477</td>\n",
       "      <td>-0.000523</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999486</td>\n",
       "      <td>-1.110789e-12</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001043</td>\n",
       "      <td>0.100257</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2025-08-26 14:57:23+00:00</td>\n",
       "      <td>2025-08-23 22:38:02+00:00</td>\n",
       "      <td>0.999401</td>\n",
       "      <td>0.999472</td>\n",
       "      <td>0.999370</td>\n",
       "      <td>-0.000630</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999483</td>\n",
       "      <td>-7.115763e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001192</td>\n",
       "      <td>0.111186</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2025-08-26 14:57:38+00:00</td>\n",
       "      <td>2025-08-23 22:38:02+00:00</td>\n",
       "      <td>0.999519</td>\n",
       "      <td>0.999482</td>\n",
       "      <td>0.999466</td>\n",
       "      <td>-0.000534</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999454</td>\n",
       "      <td>3.738850e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001013</td>\n",
       "      <td>0.105200</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2025-08-26 14:57:53+00:00</td>\n",
       "      <td>2025-08-23 22:38:02+00:00</td>\n",
       "      <td>0.999417</td>\n",
       "      <td>0.999469</td>\n",
       "      <td>0.999356</td>\n",
       "      <td>-0.000644</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999456</td>\n",
       "      <td>-5.219837e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.001149</td>\n",
       "      <td>0.111402</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2025-08-26 14:58:08+00:00</td>\n",
       "      <td>2025-08-23 22:38:02+00:00</td>\n",
       "      <td>0.999303</td>\n",
       "      <td>0.999436</td>\n",
       "      <td>0.999234</td>\n",
       "      <td>-0.000766</td>\n",
       "      <td>0.000098</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.999464</td>\n",
       "      <td>-1.323117e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000992</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2253</th>\n",
       "      <td>2025-08-28 21:00:53+00:00</td>\n",
       "      <td>USDC/USDT_univ3</td>\n",
       "      <td>1.000051</td>\n",
       "      <td>1.000022</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>-0.001644</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.977360e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.003168</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004702</td>\n",
       "      <td>0.980871</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.432038</td>\n",
       "      <td>0.542200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2254</th>\n",
       "      <td>2025-08-28 21:01:39+00:00</td>\n",
       "      <td>USDC/USDT_univ3</td>\n",
       "      <td>0.999471</td>\n",
       "      <td>0.999912</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001967</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-4.404776e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.002050</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.819711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2255</th>\n",
       "      <td>2025-08-28 21:02:23+00:00</td>\n",
       "      <td>USDC/USDT_univ3</td>\n",
       "      <td>1.000072</td>\n",
       "      <td>0.999944</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.001551</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.286553e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000079</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0.004702</td>\n",
       "      <td>0.968146</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0.432541</td>\n",
       "      <td>0.830525</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2256</th>\n",
       "      <td>2025-08-28 21:03:53+00:00</td>\n",
       "      <td>USDC/USDT_univ3</td>\n",
       "      <td>0.999976</td>\n",
       "      <td>0.999950</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000926</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2.558185e-01</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.000057</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2257</th>\n",
       "      <td>2025-08-28 21:07:21+00:00</td>\n",
       "      <td>USDC/USDT_univ3</td>\n",
       "      <td>0.999692</td>\n",
       "      <td>0.999936</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-0.002890</td>\n",
       "      <td>NaN</td>\n",
       "      <td>-2.438944e+00</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2258 rows × 33 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                            ts                       pool  dex_spot  dex_twap  \\\n",
       "0    2025-08-26 14:57:08+00:00  2025-08-23 22:38:02+00:00  0.999490  0.999490   \n",
       "1    2025-08-26 14:57:23+00:00  2025-08-23 22:38:02+00:00  0.999401  0.999472   \n",
       "2    2025-08-26 14:57:38+00:00  2025-08-23 22:38:02+00:00  0.999519  0.999482   \n",
       "3    2025-08-26 14:57:53+00:00  2025-08-23 22:38:02+00:00  0.999417  0.999469   \n",
       "4    2025-08-26 14:58:08+00:00  2025-08-23 22:38:02+00:00  0.999303  0.999436   \n",
       "...                        ...                        ...       ...       ...   \n",
       "2253 2025-08-28 21:00:53+00:00            USDC/USDT_univ3  1.000051  1.000022   \n",
       "2254 2025-08-28 21:01:39+00:00            USDC/USDT_univ3  0.999471  0.999912   \n",
       "2255 2025-08-28 21:02:23+00:00            USDC/USDT_univ3  1.000072  0.999944   \n",
       "2256 2025-08-28 21:03:53+00:00            USDC/USDT_univ3  0.999976  0.999950   \n",
       "2257 2025-08-28 21:07:21+00:00            USDC/USDT_univ3  0.999692  0.999936   \n",
       "\n",
       "      oracle_ratio       dev  dev_roll_std  tvl_outflow_rate  virtual_price  \\\n",
       "0         0.999477 -0.000523      0.000000               NaN       0.999486   \n",
       "1         0.999370 -0.000630      0.000000               NaN       0.999483   \n",
       "2         0.999466 -0.000534      0.000000               NaN       0.999454   \n",
       "3         0.999356 -0.000644      0.000000               NaN       0.999456   \n",
       "4         0.999234 -0.000766      0.000098               NaN       0.999464   \n",
       "...            ...       ...           ...               ...            ...   \n",
       "2253           NaN       NaN      0.000000         -0.001644            NaN   \n",
       "2254           NaN       NaN      0.000000          0.001967            NaN   \n",
       "2255           NaN       NaN      0.000000          0.001551            NaN   \n",
       "2256           NaN       NaN      0.000000          0.000926            NaN   \n",
       "2257           NaN       NaN           NaN         -0.002890            NaN   \n",
       "\n",
       "      spot_twap_gap_bps  ...  z_cusum      z_ae  anom_fused  y_10m  y_30m  \\\n",
       "0         -1.110789e-12  ...      0.0  0.001043    0.100257      1      1   \n",
       "1         -7.115763e-01  ...      0.0  0.001192    0.111186      1      1   \n",
       "2          3.738850e-01  ...      0.0  0.001013    0.105200      1      1   \n",
       "3         -5.219837e-01  ...      0.0  0.001149    0.111402      1      1   \n",
       "4         -1.323117e+00  ...      1.0  0.000992    1.000000      1      1   \n",
       "...                 ...  ...      ...       ...         ...    ...    ...   \n",
       "2253       2.977360e-01  ...      1.0  0.003168    1.000000      1      1   \n",
       "2254      -4.404776e+00  ...      1.0  0.002050    1.000000      1      1   \n",
       "2255       1.286553e+00  ...      1.0  0.000079    1.000000      1      1   \n",
       "2256       2.558185e-01  ...      1.0  0.000057    1.000000      0      0   \n",
       "2257      -2.438944e+00  ...      NaN       NaN         NaN      0      0   \n",
       "\n",
       "      neighbor_max_dev  neighbor_avg_anom  lead_lag_best corr_best  \\\n",
       "0                  NaN                NaN            NaN       NaN   \n",
       "1                  NaN                NaN            NaN       NaN   \n",
       "2                  NaN                NaN            NaN       NaN   \n",
       "3                  NaN                NaN            NaN       NaN   \n",
       "4                  NaN                NaN            NaN       NaN   \n",
       "...                ...                ...            ...       ...   \n",
       "2253          0.004702           0.980871            8.0  0.432038   \n",
       "2254               NaN                NaN            NaN       NaN   \n",
       "2255          0.004702           0.968146            8.0  0.432541   \n",
       "2256               NaN                NaN            NaN       NaN   \n",
       "2257               NaN                NaN            NaN       NaN   \n",
       "\n",
       "      risk_forecast_10m  \n",
       "0                   NaN  \n",
       "1                   NaN  \n",
       "2                   NaN  \n",
       "3                   NaN  \n",
       "4                   NaN  \n",
       "...                 ...  \n",
       "2253           0.542200  \n",
       "2254           0.819711  \n",
       "2255           0.830525  \n",
       "2256                NaN  \n",
       "2257                NaN  \n",
       "\n",
       "[2258 rows x 33 columns]"
      ]
     },
     "execution_count": 172,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat = sample_once_parallel() if 'sample_once_parallel' in globals() else sample_once()\n",
    "refresh_true_twap_and_gap(window_s=300)\n",
    "stamp_feeds_fresh_on_live(heartbeat_s=120, max_block_lag_s=120)\n",
    "ensure_labels_fixed_on_live(dev_thr=0.005, fused_thr=0.90, h10=10, h30=30, persist_version=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "47712000-f77f-4675-b31e-594c7ccabb60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import json, math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib\n",
    "matplotlib.use(\"Agg\")\n",
    "import matplotlib.pyplot as plt\n",
    "_SK = globals().get(\"_SK_OK\", True)\n",
    "try:\n",
    "    from sklearn.metrics import average_precision_score as AP\n",
    "except Exception:\n",
    "    _SK = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "id": "d833eb1e-d317-465c-addb-7e8b7f1ed41b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ART_DIR = (OUT / \"artifacts\")\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CAL_10M_JSON = ART_DIR / \"calibration_10m.json\"\n",
    "CAL_10M_PNG  = ART_DIR / \"calibration_10m.png\"\n",
    "CAL_30M_JSON = ART_DIR / \"calibration_30m.json\"\n",
    "CAL_30M_PNG  = ART_DIR / \"calibration_30m.png\"\n",
    "DET_AP_JSON  = ART_DIR / \"detector_pr_auc.json\"\n",
    "DET_AP_PNG   = ART_DIR / \"detector_pr_auc.png\"\n",
    "EXPLAIN_30M_JSON = OUT / \"explain_30m.json\"\n",
    "DRIFT_JSON  = ART_DIR / \"feature_drift.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "id": "a8567251-3756-4671-b453-ac8583fdcbb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _safe_np(a):\n",
    "    return np.asarray(a, dtype=float)\n",
    "def _ensure_labels_present(df: pd.DataFrame):\n",
    "    if df.empty:\n",
    "        return df\n",
    "    if \"y_10m\" not in df.columns or df[\"y_10m\"].nunique(dropna=True) < 2:\n",
    "        if \"ensure_labels_fixed_on_live\" in globals():\n",
    "            ensure_labels_fixed_on_live()\n",
    "        else:\n",
    "            df[\"y_10m\"] = _ensure_labels(df)\n",
    "            write_live(df)\n",
    "    if \"y_30m\" not in df.columns or df[\"y_30m\"].isna().all():\n",
    "        if \"ensure_labels_fixed_on_live\" in globals():\n",
    "            ensure_labels_fixed_on_live(fill_only=False)\n",
    "        else:\n",
    "            df[\"y_30m\"] = label_targets(\n",
    "                df, horizon=30, dev_thr=0.005,\n",
    "                fused_col=\"anom_fused\" if \"anom_fused\" in df.columns else None\n",
    "            ).astype(int)\n",
    "            write_live(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "c9b67b8d-d89b-47b5-82d5-169e7c306d53",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _calibration_bins(probs: np.ndarray, labels: np.ndarray, nbins: int = 10):\n",
    "    p = _safe_np(probs)\n",
    "    y = _safe_np(labels)\n",
    "    if p.size == 0 or y.size == 0:\n",
    "        return pd.DataFrame(columns=[\"bin\",\"p_mean\",\"y_mean\",\"n\"])\n",
    "    p = np.clip(p, 0.0, 1.0)\n",
    "    bins = np.linspace(0, 1, nbins+1)\n",
    "    idx = np.digitize(p, bins, right=False)  \n",
    "    idx[idx==nbins+1] = nbins\n",
    "    df = pd.DataFrame({\"p\": p, \"y\": y, \"bin\": idx})\n",
    "    out = df.groupby(\"bin\").agg(p_mean=(\"p\",\"mean\"), y_mean=(\"y\",\"mean\"), n=(\"y\",\"size\")).reset_index()\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "id": "d7f606a8-8cbd-40a7-b047-89e077fa12df",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _save_calibration_artifacts(horizon: int = 10, nbins: int = 10):\n",
    "    live = load_live()\n",
    "    if live.empty:\n",
    "        return None\n",
    "    if horizon == 10:\n",
    "        path = globals().get(\"FORECAST_10M_PARQUET\", OUT / \"forecast_10m.parquet\")\n",
    "        col  = \"risk_forecast_10m\"\n",
    "        ycol = \"y_10m\"\n",
    "        scorer = score_latest_10m\n",
    "    else:\n",
    "        path = globals().get(\"FORECAST_30M_PARQUET\", OUT / \"forecast_30m.parquet\")\n",
    "        col  = \"risk_forecast_30m\"\n",
    "        ycol = \"y_30m\"\n",
    "        scorer = score_latest_30m if \"score_latest_30m\" in globals() else None\n",
    "    _ensure_labels_present(live)\n",
    "    if Path(path).exists():\n",
    "        f = pd.read_parquet(path)\n",
    "    else:\n",
    "        if scorer is None:\n",
    "            return None\n",
    "        f = scorer(n_tail=200, write_parquet=False)\n",
    "    use = f.merge(live[[\"ts\",\"pool\",ycol]], on=[\"ts\",\"pool\"], how=\"left\")\n",
    "    use = use.dropna(subset=[col])\n",
    "    if ycol not in use.columns:\n",
    "        return None\n",
    "    y = pd.to_numeric(use[ycol], errors=\"coerce\").fillna(0).astype(int).to_numpy()\n",
    "    p = pd.to_numeric(use[col], errors=\"coerce\").fillna(0.0).to_numpy()\n",
    "    if (y.sum() == 0) or (np.unique(y).size < 2):\n",
    "        calib = pd.DataFrame(columns=[\"bin\",\"p_mean\",\"y_mean\",\"n\"])\n",
    "    else:\n",
    "        calib = _calibration_bins(p, y, nbins=nbins)\n",
    "    obj = {\n",
    "        \"horizon_min\": horizon,\n",
    "        \"bins\": calib.to_dict(orient=\"records\"),\n",
    "        \"counts\": {\"n\": int(len(use)), \"positives\": int(y.sum())}\n",
    "    }\n",
    "    jpath = CAL_10M_JSON if horizon==10 else CAL_30M_JSON\n",
    "    Path(jpath).write_text(json.dumps(obj, indent=2))\n",
    "    png = CAL_10M_PNG if horizon==10 else CAL_30M_PNG\n",
    "    plt.figure(figsize=(6,5))\n",
    "    xx = np.linspace(0,1,100)\n",
    "    plt.plot(xx, xx, linestyle=\"--\")\n",
    "    if not calib.empty:\n",
    "        plt.scatter(calib[\"p_mean\"], calib[\"y_mean\"], s=30)\n",
    "        for _, r in calib.iterrows():\n",
    "            plt.annotate(str(int(r[\"n\"])), (r[\"p_mean\"], r[\"y_mean\"]), xytext=(4,4), textcoords=\"offset points\", fontsize=8)\n",
    "    plt.xlim(0,1); plt.ylim(0,1)\n",
    "    plt.xlabel(\"Predicted probability\")\n",
    "    plt.ylabel(\"Observed frequency\")\n",
    "    plt.title(f\"Calibration ({horizon}m) — n={len(use)}, pos={y.sum()}\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(png, dpi=140)\n",
    "    plt.close()\n",
    "    return {\"json\": str(jpath), \"png\": str(png)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "id": "6939e258-3986-4489-9621-0a8138b70ce8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_calibration_artifacts():\n",
    "    out = {\"h10\": _save_calibration_artifacts(10), \"h30\": _save_calibration_artifacts(30)}\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "id": "a31374cb-597c-4062-af7c-0668211dd532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_detector_pr_auc(thr_abs_dev: float = 0.003):\n",
    "    df = load_live()\n",
    "    if df.empty: \n",
    "        return None\n",
    "    y = (df[\"dev\"].abs() >= float(thr_abs_dev)).astype(int).values\n",
    "    cols = [c for c in [\"z_if\",\"z_lof\",\"z_ocsvm\",\"z_cusum\",\"z_ae\",\"z_ae_seq\",\"anom_fused\"] if c in df.columns]\n",
    "    scores = {}\n",
    "    if _SK and y.sum() > 0 and y.sum() < len(y):\n",
    "        for c in cols:\n",
    "            s = pd.to_numeric(df[c], errors=\"coerce\").fillna(0.0).values\n",
    "            try:\n",
    "                scores[c] = float(AP(y, s))\n",
    "            except Exception:\n",
    "                scores[c] = float(\"nan\")\n",
    "    else:\n",
    "        for c in cols:\n",
    "            scores[c] = float(\"nan\")\n",
    "    winner = None\n",
    "    if scores:\n",
    "        winner = max(scores.items(), key=lambda kv: (kv[1] if not math.isnan(kv[1]) else -1, kv[0]==\"anom_fused\"))[0]\n",
    "    payload = {\"threshold_abs_dev\": thr_abs_dev, \"ap\": scores, \"winner\": winner}\n",
    "    DET_AP_JSON.write_text(json.dumps(payload, indent=2))\n",
    "    plt.figure(figsize=(7,4))\n",
    "    keys = list(scores.keys())\n",
    "    vals = [scores[k] if not (scores[k] is None or math.isnan(scores[k])) else 0.0 for k in keys]\n",
    "    plt.bar(range(len(keys)), vals)\n",
    "    plt.xticks(range(len(keys)), keys, rotation=20, ha=\"right\")\n",
    "    plt.ylim(0, 1)\n",
    "    plt.ylabel(\"PR-AUC\")\n",
    "    plt.title(\"Detector PR-AUC (|dev| ≥ {:.2f}%)\".format(thr_abs_dev*100))\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(DET_AP_PNG, dpi=140)\n",
    "    plt.close()\n",
    "    return {\"json\": str(DET_AP_JSON), \"png\": str(DET_AP_PNG), \"winner\": winner, \"scores\": scores}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "id": "f3ae580b-17b4-4b57-9755-ec3abd346c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "def explain_forecast_30m(feature_cols: list | None = None, n_repeats: int = 8) -> dict:\n",
    "    \"\"\"Permutation-importance for the 30m forecaster; mirrors your 10m explain.\"\"\"\n",
    "    if not globals().get(\"_SK_OK\", True):\n",
    "        payload = {\"ts\": _now_iso(), \"top_contributors\": [], \"all_features\": []}\n",
    "        EXPLAIN_30M_JSON.write_text(json.dumps(payload, indent=2))\n",
    "        return payload\n",
    "    df = load_live()\n",
    "    if df.empty:\n",
    "        raise ValueError(\"live_dataset is empty.\")\n",
    "    _ensure_labels_present(df)\n",
    "    if feature_cols is None:\n",
    "        feature_cols = [\n",
    "            \"dev\",\"dev_roll_std\",\"tvl_outflow_rate\",\"spot_twap_gap_bps\",\n",
    "            \"oracle_ratio\",\"anom_fused\",\"r0_delta\",\"r1_delta\",\n",
    "            \"event_severity_max_24h\",\"event_count_24h\"\n",
    "        ]\n",
    "    use_cols = [c for c in feature_cols if c in df.columns]\n",
    "    X = df[use_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0).to_numpy()\n",
    "    y = pd.to_numeric(df[\"y_30m\"], errors=\"coerce\").fillna(0).astype(int).to_numpy()\n",
    "    tr_idx, te_idx = _time_split_idx(df[\"ts\"], 0.70)\n",
    "    if te_idx.size == 0:\n",
    "        payload = {\"ts\": _now_iso(), \"top_contributors\": [], \"all_features\": []}\n",
    "        EXPLAIN_30M_JSON.write_text(json.dumps(payload, indent=2))\n",
    "        return payload\n",
    "    Xte, yte = X[te_idx], y[te_idx]\n",
    "    clf, calib = _load_forecaster_30m() if \"._load_forecaster_30m\" in str(globals().keys()) or \"_load_forecaster_30m\" in globals() else (None, None)\n",
    "    if clf is None and calib is None and \"train_forecaster_30m\" in globals():\n",
    "        _ = train_forecaster_30m(feature_cols=use_cols)\n",
    "        clf, calib = _load_forecaster_30m()\n",
    "    model = calib if calib is not None else clf\n",
    "    if model is None or len(np.unique(yte)) < 2:\n",
    "        payload = {\"ts\": _now_iso(), \"top_contributors\": [], \"all_features\": []}\n",
    "        EXPLAIN_30M_JSON.write_text(json.dumps(payload, indent=2))\n",
    "        return payload\n",
    "    from sklearn.inspection import permutation_importance\n",
    "    r = permutation_importance(model, Xte, yte, n_repeats=n_repeats, scoring=\"average_precision\", random_state=42)\n",
    "    imp = sorted(zip(use_cols, r.importances_mean, r.importances_std), key=lambda z: z[1], reverse=True)\n",
    "    top3 = [f\"{name} (+{mean:.4f}±{std:.4f} AP)\" for name, mean, std in imp[:3]]\n",
    "    payload = {\"ts\": _now_iso(), \"top_contributors\": top3,\n",
    "               \"all_features\": [{\"feature\": n, \"mean\": float(m), \"std\": float(s)} for n,m,s in imp]}\n",
    "    EXPLAIN_30M_JSON.write_text(json.dumps(payload, indent=2))\n",
    "    return payload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "877515e4-f87a-4006-a486-0aa691d5428a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _empirical_ks(xa: np.ndarray, xb: np.ndarray) -> float:\n",
    "    \"\"\"Minimal KS distance; no scipy dependency.\"\"\"\n",
    "    xa = xa[~np.isnan(xa)]; xb = xb[~np.isnan(xb)]\n",
    "    if xa.size < 5 or xb.size < 5:\n",
    "        return 0.0\n",
    "    xs = np.sort(np.unique(np.concatenate([xa, xb])))\n",
    "    if xs.size == 0:\n",
    "        return 0.0\n",
    "    def _cdf(x, v):\n",
    "        return (x <= v).sum() / x.size\n",
    "    ks = 0.0\n",
    "    for v in xs:\n",
    "        ks = max(ks, abs(_cdf(xa, v) - _cdf(xb, v)))\n",
    "    return float(ks)\n",
    "def _psi(base: np.ndarray, cur: np.ndarray, nbins: int = 10) -> float:\n",
    "    base = base[~np.isnan(base)]; cur = cur[~np.isnan(cur)]\n",
    "    if base.size < 10 or cur.size < 10:\n",
    "        return 0.0\n",
    "    qs = np.quantile(base, np.linspace(0,1,nbins+1))\n",
    "    qs[0] -= 1e-9; qs[-1] += 1e-9 \n",
    "    b = np.histogram(base, bins=qs)[0]; c = np.histogram(cur, bins=qs)[0]\n",
    "    b = b / max(b.sum(), 1); c = c / max(c.sum(), 1)\n",
    "    b = np.where(b==0, 1e-6, b)\n",
    "    c = np.where(c==0, 1e-6, c)\n",
    "    return float(np.sum((c - b) * np.log(c / b)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "dc9cda63-0f9c-4a2e-a6d4-962b2d37a076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_feature_drift(\n",
    "    features: list[str] | None = None,\n",
    "    ks_thr: float = 0.20,\n",
    "    psi_thr: float = 0.25\n",
    ") -> dict:\n",
    "    \"\"\"\n",
    "    Compare train(70%) vs recent(30%) distributions for selected features.\n",
    "    Returns a JSON-serializable dict with per-feature KS and PSI and a boolean 'drift'.\n",
    "    \"\"\"\n",
    "    df = load_live()\n",
    "    if df.empty:\n",
    "        return {\"drift\": False, \"reason\": \"no data\", \"metrics\": {}}\n",
    "    if features is None:\n",
    "        features = [\n",
    "            \"dev\",\"dev_roll_std\",\"tvl_outflow_rate\",\"spot_twap_gap_bps\",\n",
    "            \"oracle_ratio\",\"anom_fused\"\n",
    "        ]\n",
    "        features = [c for c in features if c in df.columns]\n",
    "    tr_idx, te_idx = _time_split_idx(df[\"ts\"], 0.70)\n",
    "    base = df.iloc[tr_idx, :].copy()\n",
    "    cur  = df.iloc[te_idx, :].copy()\n",
    "    metrics = {}\n",
    "    alert = False\n",
    "    for c in features:\n",
    "        a = pd.to_numeric(base[c], errors=\"coerce\").values\n",
    "        b = pd.to_numeric(cur[c],  errors=\"coerce\").values\n",
    "        ks  = _empirical_ks(a, b)\n",
    "        psi = _psi(a, b)\n",
    "        metrics[c] = {\"ks\": float(ks), \"psi\": float(psi)}\n",
    "        if (ks >= ks_thr) or (psi >= psi_thr):\n",
    "            alert = True\n",
    "    payload = {\"drift\": bool(alert), \"reason\": (\"feature drift\" if alert else \"ok\"),\n",
    "               \"thresholds\": {\"ks\": ks_thr, \"psi\": psi_thr},\n",
    "               \"metrics\": metrics, \"n_train\": int(len(base)), \"n_recent\": int(len(cur))}\n",
    "    DRIFT_JSON.write_text(json.dumps(payload, indent=2))\n",
    "    return payload\n",
    "def retrain_check() -> dict:\n",
    "    \"\"\"\n",
    "    Returns {should_retrain, reason, drift?}\n",
    "    - True if feature drift exceeds threshold, or if scheduled nightly condition is hit.\n",
    "    \"\"\"\n",
    "    drift = compute_feature_drift()\n",
    "    if drift.get(\"drift\", False):\n",
    "        return {\"should_retrain\": True, \"reason\": \"feature drift\", \"drift\": drift}\n",
    "    return {\"should_retrain\": True, \"reason\": \"scheduled nightly or drift threshold (PR-AUC drop) hit\", \"drift\": drift}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "837e05d2-c32b-4a8e-bc08-1b7929b88399",
   "metadata": {},
   "outputs": [],
   "source": [
    "def nightly_report_v2() -> dict:\n",
    "    \"\"\"\n",
    "    Nightly Markdown report:\n",
    "      - winner detector by PR-AUC + artifact saved\n",
    "      - forecast calibration (10m & 30m) + artifacts saved\n",
    "      - incidents summary (as before)\n",
    "    Also exports to PDF.\n",
    "    \"\"\"\n",
    "    df = load_live()\n",
    "    if df.empty:\n",
    "        return {\"ok\": False, \"markdown\": \"[report] no data\"}\n",
    "    det = save_detector_pr_auc(thr_abs_dev=0.003)\n",
    "    winner = (det or {}).get(\"winner\", \"anom_fused\")\n",
    "    cal_art = save_all_calibration_artifacts()\n",
    "    try:\n",
    "        recent = df.sort_values(\"ts\").groupby(\"pool\").tail(min(100, len(df)))\n",
    "        dec = decide_latest(n_tail=min(100, len(recent))) if \"decide_latest\" in globals() else pd.DataFrame()\n",
    "        reds = dec[dec[\"level\"] == \"red\"] if isinstance(dec, pd.DataFrame) else pd.DataFrame()\n",
    "    except Exception:\n",
    "        reds = pd.DataFrame()\n",
    "    md = []\n",
    "    md.append(\"# Nightly Model Report (v2)\")\n",
    "    md.append(f\"Generated: {_now_iso()}\\n\")\n",
    "    md.append(\"## Winner Detector Today\")\n",
    "    if det:\n",
    "        md.append(f\"- **Winner:** `{det.get('winner')}`  (PR-AUC: {det['scores'].get(det.get('winner'), float('nan')):.3f})\")\n",
    "        md.append(f\"- JSON: `{DET_AP_JSON.name}`  PNG: `{DET_AP_PNG.name}`\")\n",
    "    else:\n",
    "        md.append(\"- (no scores)\")\n",
    "    md.append(\"\\n## Forecast Calibration\")\n",
    "    if cal_art and cal_art.get(\"h10\"):\n",
    "        md.append(f\"- **10m** → JSON: `{Path(cal_art['h10']['json']).name}`  PNG: `{Path(cal_art['h10']['png']).name}`\")\n",
    "    if cal_art and cal_art.get(\"h30\"):\n",
    "        md.append(f\"- **30m** → JSON: `{Path(cal_art['h30']['json']).name}`  PNG: `{Path(cal_art['h30']['png']).name}`\")\n",
    "    drift = compute_feature_drift()\n",
    "    md.append(\"\\n## Feature Drift\")\n",
    "    md.append(f\"- drift: **{drift.get('drift')}**  reason: {drift.get('reason')}\")\n",
    "    md.append(f\"- JSON: `{DRIFT_JSON.name}`\")\n",
    "    md.append(\"\\n## Incidents (last window)\")\n",
    "    if isinstance(reds, pd.DataFrame) and not reds.empty:\n",
    "        for _, r in reds.iterrows():\n",
    "            an = r.get(\"anom_fused\", np.nan)\n",
    "            p10 = r.get(\"risk_forecast_10m\", np.nan)\n",
    "            md.append(f\"- {r['ts']} | {r['pool']} | fused={an:.2f} | p10={p10:.2f}\")\n",
    "    else:\n",
    "        md.append(\"- None\")\n",
    "    md_text = \"\\n\".join(md)\n",
    "    pdf_path = export_markdown_pdf(md_text, OUT / \"report.pdf\", title=\"Depeg Sentinel — Nightly Report v2\")\n",
    "    return {\"ok\": True, \"markdown\": md_text[:800] + \"...\", \"pdf\": str(pdf_path),\n",
    "            \"artifacts\": {\"detector_pr_auc\": {\"json\": str(DET_AP_JSON), \"png\": str(DET_AP_PNG)},\n",
    "                          \"calibration_10m\": cal_art.get(\"h10\") if cal_art else None,\n",
    "                          \"calibration_30m\": cal_art.get(\"h30\") if cal_art else None},\n",
    "            \"drift\": drift}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "id": "2d3c9e2b-d51a-46d7-99c6-b9a25e0eb5dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "nightly_report = nightly_report_v2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "id": "a251fd2c-dba3-4580-aa46-cc0a068012ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ok] Markdown PDF exported → C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\report.pdf\n",
      "{'ok': True, 'markdown': '# Nightly Model Report (v2)\\nGenerated: 2025-08-28T21:07:34+00:00\\n\\n## Winner Detector Today\\n- **Winner:** `z_if`  (PR-AUC: 0.704)\\n- JSON: `detector_pr_auc.json`  PNG: `detector_pr_auc.png`\\n\\n## Forecast Calibration\\n- **10m** → JSON: `calibration_10m.json`  PNG: `calibration_10m.png`\\n- **30m** → JSON: `calibration_30m.json`  PNG: `calibration_30m.png`\\n\\n## Feature Drift\\n- drift: **True**  reason: feature drift\\n- JSON: `feature_drift.json`\\n\\n## Incidents (last window)\\n- None...', 'pdf': 'C:\\\\Users\\\\aniru\\\\OneDrive\\\\Desktop\\\\ML tutorial\\\\DEFI Depeg sentinel\\\\report.pdf', 'artifacts': {'detector_pr_auc': {'json': 'C:\\\\Users\\\\aniru\\\\OneDrive\\\\Desktop\\\\ML tutorial\\\\DEFI Depeg sentinel\\\\artifacts\\\\detector_pr_auc.json', 'png': 'C:\\\\Users\\\\aniru\\\\OneDrive\\\\Desktop\\\\ML tutorial\\\\DEFI Depeg sentinel\\\\artifacts\\\\detector_pr_auc.png'}, 'calibration_10m': {'json': 'C:\\\\Users\\\\aniru\\\\OneDrive\\\\Desktop\\\\ML tutorial\\\\DEFI Depeg sentinel\\\\artifacts\\\\calibration_10m.json', 'png': 'C:\\\\Users\\\\aniru\\\\OneDrive\\\\Desktop\\\\ML tutorial\\\\DEFI Depeg sentinel\\\\artifacts\\\\calibration_10m.png'}, 'calibration_30m': {'json': 'C:\\\\Users\\\\aniru\\\\OneDrive\\\\Desktop\\\\ML tutorial\\\\DEFI Depeg sentinel\\\\artifacts\\\\calibration_30m.json', 'png': 'C:\\\\Users\\\\aniru\\\\OneDrive\\\\Desktop\\\\ML tutorial\\\\DEFI Depeg sentinel\\\\artifacts\\\\calibration_30m.png'}}, 'drift': {'drift': True, 'reason': 'feature drift', 'thresholds': {'ks': 0.2, 'psi': 0.25}, 'metrics': {'dev': {'ks': 0.25167652859960554, 'psi': 4.926066715649275}, 'dev_roll_std': {'ks': 0.33975357651112276, 'psi': 1.8948207496094331}, 'tvl_outflow_rate': {'ks': 0.14130372942641545, 'psi': 1.2076518269911787}, 'spot_twap_gap_bps': {'ks': 0.08002084012698985, 'psi': 0.10937781049813058}, 'oracle_ratio': {'ks': 0.25167652859960554, 'psi': 4.926066715649275}, 'anom_fused': {'ks': 0.3086872948898266, 'psi': 4.033216822393285}}, 'n_train': 1580, 'n_recent': 678}}\n",
      "C:\\Users\\aniru\\OneDrive\\Desktop\\ML tutorial\\DEFI Depeg sentinel\\explain_30m.json ['dev (+0.0000±0.0000 AP)', 'dev_roll_std (+0.0000±0.0000 AP)', 'tvl_outflow_rate (+0.0000±0.0000 AP)']\n",
      "{'drift': True, 'reason': 'feature drift', 'thresholds': {'ks': 0.2, 'psi': 0.25}, 'metrics': {'dev': {'ks': 0.25167652859960554, 'psi': 4.926066715649275}, 'dev_roll_std': {'ks': 0.33975357651112276, 'psi': 1.8948207496094331}, 'tvl_outflow_rate': {'ks': 0.14130372942641545, 'psi': 1.2076518269911787}, 'spot_twap_gap_bps': {'ks': 0.08002084012698985, 'psi': 0.10937781049813058}, 'oracle_ratio': {'ks': 0.25167652859960554, 'psi': 4.926066715649275}, 'anom_fused': {'ks': 0.3086872948898266, 'psi': 4.033216822393285}}, 'n_train': 1580, 'n_recent': 678}\n",
      "{'should_retrain': True, 'reason': 'feature drift', 'drift': {'drift': True, 'reason': 'feature drift', 'thresholds': {'ks': 0.2, 'psi': 0.25}, 'metrics': {'dev': {'ks': 0.25167652859960554, 'psi': 4.926066715649275}, 'dev_roll_std': {'ks': 0.33975357651112276, 'psi': 1.8948207496094331}, 'tvl_outflow_rate': {'ks': 0.14130372942641545, 'psi': 1.2076518269911787}, 'spot_twap_gap_bps': {'ks': 0.08002084012698985, 'psi': 0.10937781049813058}, 'oracle_ratio': {'ks': 0.25167652859960554, 'psi': 4.926066715649275}, 'anom_fused': {'ks': 0.3086872948898266, 'psi': 4.033216822393285}}, 'n_train': 1580, 'n_recent': 678}}\n"
     ]
    }
   ],
   "source": [
    "res_report = nightly_report()\n",
    "print(res_report)\n",
    "exp30 = explain_forecast_30m(n_repeats=8)\n",
    "print(EXPLAIN_30M_JSON, exp30.get(\"top_contributors\"))\n",
    "drift = compute_feature_drift()\n",
    "print(drift)\n",
    "print(retrain_check())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "3c94019d-05c0-4f70-ae83-6b545e2ad311",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    CANON_COLS\n",
    "except NameError:\n",
    "    CANON_COLS = []\n",
    "for _c in [\"neighbor_max_dev\",\"neighbor_avg_anom\",\"lead_lag_best\",\"corr_best\"]:\n",
    "    if _c not in CANON_COLS:\n",
    "        CANON_COLS.append(_c)\n",
    "import numpy as np, pandas as pd\n",
    "from collections import deque"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "id": "442ce55a-6a76-4c43-9c69-f7135f31127d",
   "metadata": {},
   "outputs": [],
   "source": [
    "_LAST_NET_WIN = globals().get(\"_LAST_NET_WIN\", 60)\n",
    "_LAST_MAX_LAG = globals().get(\"_LAST_MAX_LAG\", 10)\n",
    "_hist_dev  = globals().get(\"_hist_dev\", {})\n",
    "_hist_anom = globals().get(\"_hist_anom\", {})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "id": "788249f4-27fb-425a-b69d-b65caffb481b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _push_hist(d: dict, key: str, val: float, win: int = 60):\n",
    "    q = d.get(key)\n",
    "    if q is None:\n",
    "        q = deque(maxlen=win)\n",
    "        d[key] = q\n",
    "    q.append(float(val) if pd.notna(val) else 0.0)\n",
    "def _rolling_corr(x: np.ndarray, y: np.ndarray) -> float:\n",
    "    if len(x) < 3 or len(y) < 3 or len(x) != len(y):\n",
    "        return np.nan\n",
    "    x = np.asarray(x); y = np.asarray(y)\n",
    "    if x.std() == 0 or y.std() == 0:\n",
    "        return 0.0\n",
    "    return float(np.corrcoef(x, y)[0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "720d2249-df03-4011-ab77-9fe8f7953b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cross_corr_lag(x: np.ndarray, y: np.ndarray, max_lag: int = 10) -> tuple[float,int]:\n",
    "    if len(x) < 5 or len(y) < 5:\n",
    "        return np.nan, 0\n",
    "    x = np.asarray(x); y = np.asarray(y)\n",
    "    best, best_lag = -2.0, 0\n",
    "    for lag in range(-max_lag, max_lag+1):\n",
    "        if lag < 0:\n",
    "            xs, ys = x[:lag], y[-lag:]\n",
    "        elif lag > 0:\n",
    "            xs, ys = x[lag:], y[:-lag]\n",
    "        else:\n",
    "            xs, ys = x, y\n",
    "        if len(xs) < 5:\n",
    "            continue\n",
    "        c = _rolling_corr(xs, ys)\n",
    "        if np.isnan(c):\n",
    "            continue\n",
    "        if c > best:\n",
    "            best, best_lag = c, lag\n",
    "    return float(best), int(best_lag)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "id": "d6832e2d-3a18-492e-973a-d4592b2fc446",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_network_features(win: int = 60, max_lag: int = 10) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Compute neighbor_max_dev / neighbor_avg_anom and best lead/lag correlation per pool.\n",
    "    - Remembers `win` and `max_lag` in globals for audit strings.\n",
    "    - Updates last rows in live CSV.\n",
    "    \"\"\"\n",
    "    global _LAST_NET_WIN, _LAST_MAX_LAG\n",
    "    _LAST_NET_WIN = int(win)\n",
    "    _LAST_MAX_LAG = int(max_lag)\n",
    "    df = load_live().copy()\n",
    "    if df.empty:\n",
    "        print(\"[network] live is empty; skipping\")\n",
    "        return pd.DataFrame(columns=[\"ts\",\"pool\",\"neighbor_max_dev\",\"neighbor_avg_anom\",\"lead_lag_best\",\"corr_best\"])\n",
    "    df = df.sort_values(\"ts\")\n",
    "    for _, r in df.iterrows():\n",
    "        _push_hist(_hist_dev,  str(r.get(\"pool\")), float(r.get(\"dev\", 0.0)), win)\n",
    "        _push_hist(_hist_anom, str(r.get(\"pool\")), float(r.get(\"anom_fused\", 0.0)), win)\n",
    "    pools = list(df[\"pool\"].dropna().unique())\n",
    "    rows = []\n",
    "    for p in pools:\n",
    "        dev_p = list(_hist_dev.get(p, []))\n",
    "        neigh = [q for q in pools if q != p]\n",
    "        neigh_max_dev  = 0.0\n",
    "        neigh_avg_anom = 0.0\n",
    "        lead_lag_best  = 0\n",
    "        corr_best      = 0.0\n",
    "        used = 0\n",
    "        for q in neigh:\n",
    "            dev_q = list(_hist_dev.get(q, []))\n",
    "            an_q  = list(_hist_anom.get(q, []))\n",
    "            if dev_q:\n",
    "                neigh_max_dev = max(neigh_max_dev, float(np.nanmax(np.abs(dev_q))))\n",
    "            if an_q:\n",
    "                neigh_avg_anom += float(np.nanmean(an_q)); used += 1\n",
    "            if dev_p and dev_q:\n",
    "                c, lag = _cross_corr_lag(np.array(dev_p), np.array(dev_q), max_lag=max_lag)\n",
    "                if np.isfinite(c) and abs(c) > abs(corr_best):\n",
    "                    corr_best, lead_lag_best = float(c), int(lag)\n",
    "        if used > 0:\n",
    "            neigh_avg_anom /= used\n",
    "        rows.append({\n",
    "            \"pool\": p,\n",
    "            \"neighbor_max_dev\": float(neigh_max_dev),\n",
    "            \"neighbor_avg_anom\": float(neigh_avg_anom),\n",
    "            \"lead_lag_best\": int(lead_lag_best),\n",
    "            \"corr_best\": float(corr_best),\n",
    "        })\n",
    "    latest = df.groupby(\"pool\", as_index=False).tail(1).copy()\n",
    "    net = pd.DataFrame(rows)\n",
    "    latest = latest.drop(columns=[\"neighbor_max_dev\",\"neighbor_avg_anom\",\"lead_lag_best\",\"corr_best\"], errors=\"ignore\")\n",
    "    latest = latest.merge(net, on=\"pool\", how=\"left\")\n",
    "    live = load_live()\n",
    "    live[\"ts_str\"]   = _iso_str(live[\"ts\"])\n",
    "    latest[\"ts_str\"] = _iso_str(latest[\"ts\"])\n",
    "    live_idx = live.set_index([\"pool\",\"ts_str\"])\n",
    "    upd_idx  = latest.set_index([\"pool\",\"ts_str\"])[[\"neighbor_max_dev\",\"neighbor_avg_anom\",\"lead_lag_best\",\"corr_best\"]]\n",
    "    live_idx.update(upd_idx)\n",
    "    live_updated = live_idx.reset_index().drop(columns=[\"ts_str\"])\n",
    "    write_live(live_updated)\n",
    "    print(f\"[network] updated (win={win}, max_lag={max_lag})\")\n",
    "    return latest[[\"ts\",\"pool\",\"neighbor_max_dev\",\"neighbor_avg_anom\",\"lead_lag_best\",\"corr_best\"]].sort_values(\"pool\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "id": "7e48b039-0eac-43cb-b2e5-5e73738ab6d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import List, Sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "id": "180571ec-3656-4f17-af2b-a36161f8c9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "_NET_FEATS = [\"neighbor_max_dev\",\"neighbor_avg_anom\",\"lead_lag_best\",\"corr_best\"]\n",
    "def _default_feats_tabular() -> List[str]:\n",
    "    return [\n",
    "        \"dev\",\"dev_roll_std\",\"tvl_outflow_rate\",\"spot_twap_gap_bps\",\"oracle_ratio\",\n",
    "        \"anom_fused\",\"r0_delta\",\"r1_delta\",\"event_severity_max_24h\",\"event_count_24h\",\n",
    "        *_NET_FEATS,\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "id": "5924596a-0fff-4ca8-87dd-decda7f19aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_forecaster_10m(feature_cols: List[str] | None = None, label_col: str = \"y_10m\"):\n",
    "    if not globals().get(\"_XGB_OK\", False) or not globals().get(\"_SK_OK\", False):\n",
    "        raise RuntimeError(\"xgboost + scikit-learn required to train 10m forecaster.\")\n",
    "    try:\n",
    "        compute_network_features()\n",
    "    except Exception:\n",
    "        pass\n",
    "    df = load_live().copy()\n",
    "    if df.empty:\n",
    "        raise ValueError(\"live_dataset is empty. sample more rows first.\")\n",
    "    df = ensure_targets_all(df) if \"ensure_targets_all\" in globals() else df\n",
    "    if feature_cols is None:\n",
    "        feature_cols = _default_feats_tabular()\n",
    "    use_cols = [c for c in feature_cols if c in df.columns]\n",
    "    X = df[use_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0).to_numpy()\n",
    "    y = pd.to_numeric(df[label_col], errors=\"coerce\").fillna(0).astype(int).to_numpy()\n",
    "    ts = df[\"ts\"]; tr_idx, te_idx = _time_split_idx(ts, 0.70)\n",
    "    Xtr, Xte = X[tr_idx], X[te_idx]; ytr, yte = y[tr_idx], y[te_idx]\n",
    "    clf = xgb.XGBClassifier(\n",
    "        n_estimators=300, max_depth=4, learning_rate=0.06,\n",
    "        subsample=0.9, colsample_bytree=0.8,\n",
    "        objective=\"binary:logistic\", eval_metric=\"logloss\",\n",
    "        random_state=SEED, n_jobs=0\n",
    "    )\n",
    "    clf.fit(Xtr, ytr)\n",
    "    calib = None\n",
    "    unique, counts = np.unique(ytr, return_counts=True)\n",
    "    min_cls = min(counts) if len(counts)==2 else 0\n",
    "    if min_cls >= 3:\n",
    "        try:\n",
    "            calib = CalibratedClassifierCV(clf, method=\"isotonic\", cv=3).fit(Xtr, ytr)\n",
    "        except Exception:\n",
    "            calib = None\n",
    "    if calib is None and min_cls >= 2:\n",
    "        try:\n",
    "            calib = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=2).fit(Xtr, ytr)\n",
    "        except Exception:\n",
    "            calib = None\n",
    "    if len(np.unique(yte))>1:\n",
    "        try:\n",
    "            ap = average_precision_score(yte, (calib or clf).predict_proba(Xte)[:,1])\n",
    "            bs = float(brier_score_loss(yte, (calib or clf).predict_proba(Xte)[:,1]))\n",
    "        except Exception:\n",
    "            ap, bs = float(\"nan\"), float(\"nan\")\n",
    "    else:\n",
    "        ap, bs = float(\"nan\"), float(\"nan\")\n",
    "    print(f\"[forecast10m] AP={ap if pd.notna(ap) else float('nan'):.3f}  Brier={bs if pd.notna(bs) else float('nan'):.3f}  (with network feats)\")\n",
    "    dump(clf, OUT_MODEL / \"forecast_10m_xgb.joblib\")\n",
    "    if calib: dump(calib, OUT_MODEL / \"forecast_10m_calib.joblib\")\n",
    "    elif (OUT_MODEL / \"forecast_10m_calib.joblib\").exists():\n",
    "        (OUT_MODEL / \"forecast_10m_calib.joblib\").unlink()\n",
    "    tail = df.tail(6).copy()\n",
    "    X_tail = tail[use_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0).to_numpy()\n",
    "    tail[\"risk_forecast_10m\"] = (calib or clf).predict_proba(X_tail)[:,1]\n",
    "    return tail[[\"ts\",\"pool\",\"anom_fused\",\"risk_forecast_10m\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "id": "f3540b5e-7630-42ac-866f-99c5848f8d72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_latest_10m(n_tail: int = 60, feature_cols: Sequence[str] | None = None, write_parquet: bool = True) -> pd.DataFrame:\n",
    "    df = load_live()\n",
    "    if df.empty:\n",
    "        raise ValueError(\"live_dataset is empty. Run sample_once() first.\")\n",
    "    try:\n",
    "        compute_network_features()\n",
    "    except Exception:\n",
    "        pass\n",
    "    if feature_cols is None:\n",
    "        feature_cols = _default_feats_tabular()\n",
    "    use_cols = [c for c in feature_cols if c in df.columns]\n",
    "    X_tail = df.tail(n_tail)[use_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0).to_numpy()\n",
    "    clf, calib = _load_forecaster()  \n",
    "    if clf is None and calib is None:\n",
    "        _ = train_forecaster_10m(feature_cols=list(use_cols))\n",
    "        clf, calib = _load_forecaster()\n",
    "    model = calib if calib is not None else clf\n",
    "    p = model.predict_proba(X_tail)[:,1]\n",
    "    out = df.tail(n_tail).copy(); out[\"risk_forecast_10m\"] = p\n",
    "    out = out[[\"ts\",\"pool\",\"anom_fused\",\"risk_forecast_10m\"]]\n",
    "    if write_parquet:\n",
    "        try:\n",
    "            out.to_parquet(FORECAST_10M_PARQUET, index=False); print(f\"[forecast10m] wrote {FORECAST_10M_PARQUET}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] parquet write failed: {e}\")\n",
    "    return out.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "3884e88a-3127-4d33-a11b-aa60095e65eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_forecaster_30m(feature_cols: List[str] | None = None, label_col: str = \"y_30m\"):\n",
    "    if not globals().get(\"_XGB_OK\", False) or not globals().get(\"_SK_OK\", False):\n",
    "        raise RuntimeError(\"xgboost + scikit-learn required to train 30m forecaster.\")\n",
    "    try:\n",
    "        compute_network_features()\n",
    "    except Exception:\n",
    "        pass\n",
    "    df = load_live().copy()\n",
    "    if df.empty:\n",
    "        raise ValueError(\"live_dataset is empty. sample more rows first.\")\n",
    "    df = ensure_targets_all(df) if \"ensure_targets_all\" in globals() else df\n",
    "    if feature_cols is None:\n",
    "        feature_cols = _default_feats_tabular()\n",
    "    use_cols = [c for c in feature_cols if c in df.columns]\n",
    "    X = df[use_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0).to_numpy()\n",
    "    y = pd.to_numeric(df[label_col], errors=\"coerce\").fillna(0).astype(int).to_numpy()\n",
    "    ts = df[\"ts\"]; tr_idx, te_idx = _time_split_idx(ts, 0.70)\n",
    "    Xtr, Xte = X[tr_idx], X[te_idx]; ytr, yte = y[tr_idx], y[te_idx]\n",
    "    clf = xgb.XGBClassifier(\n",
    "        n_estimators=300, max_depth=4, learning_rate=0.06,\n",
    "        subsample=0.9, colsample_bytree=0.8,\n",
    "        objective=\"binary:logistic\", eval_metric=\"logloss\",\n",
    "        random_state=SEED, n_jobs=0\n",
    "    )\n",
    "    clf.fit(Xtr, ytr)\n",
    "    calib = None\n",
    "    unique, counts = np.unique(ytr, return_counts=True)\n",
    "    min_cls = min(counts) if len(counts)==2 else 0\n",
    "    if min_cls >= 3:\n",
    "        try:\n",
    "            calib = CalibratedClassifierCV(clf, method=\"isotonic\", cv=3).fit(Xtr, ytr)\n",
    "        except Exception:\n",
    "            calib = None\n",
    "    if calib is None and min_cls >= 2:\n",
    "        try:\n",
    "            calib = CalibratedClassifierCV(clf, method=\"sigmoid\", cv=2).fit(Xtr, ytr)\n",
    "        except Exception:\n",
    "            calib = None\n",
    "    if len(np.unique(yte))>1:\n",
    "        try:\n",
    "            ap = average_precision_score(yte, (calib or clf).predict_proba(Xte)[:,1])\n",
    "            bs = float(brier_score_loss(yte, (calib or clf).predict_proba(Xte)[:,1]))\n",
    "        except Exception:\n",
    "            ap, bs = float(\"nan\"), float(\"nan\")\n",
    "    else:\n",
    "        ap, bs = float(\"nan\"), float(\"nan\")\n",
    "    print(f\"[forecast30m] AP={ap if pd.notna(ap) else float('nan'):.3f}  Brier={bs if pd.notna(bs) else float('nan'):.3f}  (with network feats)\")\n",
    "    dump(clf, FORECAST_30M_PATH)\n",
    "    if calib: dump(calib, CALIB_30M_PATH)\n",
    "    elif CALIB_30M_PATH.exists():\n",
    "        CALIB_30M_PATH.unlink()\n",
    "    tail = df.tail(6).copy()\n",
    "    X_tail = tail[use_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0).to_numpy()\n",
    "    tail[\"risk_forecast_30m\"] = (calib or clf).predict_proba(X_tail)[:,1]\n",
    "    return tail[[\"ts\",\"pool\",\"anom_fused\",\"risk_forecast_30m\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "id": "9e402c16-023b-4553-9c4f-c2c997a30706",
   "metadata": {},
   "outputs": [],
   "source": [
    "def score_latest_30m(n_tail: int = 60, feature_cols: Sequence[str] | None = None, write_parquet: bool = True) -> pd.DataFrame:\n",
    "    df = load_live()\n",
    "    if df.empty:\n",
    "        raise ValueError(\"live_dataset is empty. Run sample_once() first.\")\n",
    "    try:\n",
    "        compute_network_features()\n",
    "    except Exception:\n",
    "        pass\n",
    "    if feature_cols is None:\n",
    "        feature_cols = _default_feats_tabular()\n",
    "    use_cols = [c for c in feature_cols if c in df.columns]\n",
    "    X_tail = df.tail(n_tail)[use_cols].replace([np.inf,-np.inf], np.nan).fillna(0.0).to_numpy()\n",
    "    clf, calib = _load_forecaster_30m()\n",
    "    if clf is None and calib is None:\n",
    "        _ = train_forecaster_30m(feature_cols=list(use_cols))\n",
    "        clf, calib = _load_forecaster_30m()\n",
    "    model = calib if calib is not None else clf\n",
    "    p = model.predict_proba(X_tail)[:,1]\n",
    "    out = df.tail(n_tail).copy(); out[\"risk_forecast_30m\"] = p\n",
    "    out = out[[\"ts\",\"pool\",\"anom_fused\",\"risk_forecast_30m\"]]\n",
    "    if write_parquet:\n",
    "        try:\n",
    "            out.to_parquet(FORECAST_30M_PARQUET, index=False); print(f\"[forecast30m] wrote {FORECAST_30M_PARQUET}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[warn] parquet write failed: {e}\")\n",
    "    return out.tail(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "id": "da23235c-1967-4dcf-80ed-89a57f155d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_analyst_note_v2() -> dict:\n",
    "    \"\"\"\n",
    "    Build Analyst Note v2 including:\n",
    "      - risk_now, 10m & 30m forecast\n",
    "      - top contributors\n",
    "      - propagation cue with (win, max_lag) for audit\n",
    "    \"\"\"\n",
    "    import json\n",
    "    tail = load_live().sort_values(\"ts\").groupby(\"pool\").tail(1)\n",
    "    if tail.empty:\n",
    "        return {\"ok\": False, \"reason\": \"no data\"}\n",
    "    top3 = []\n",
    "    try:\n",
    "        if EXPLAIN_JSON.exists():\n",
    "            explain = json.loads(EXPLAIN_JSON.read_text())\n",
    "        else:\n",
    "            explain = explain_forecast_10m()\n",
    "        top3 = (explain or {}).get(\"top_contributors\", [])[:3]\n",
    "    except Exception:\n",
    "        pass\n",
    "    cue = None\n",
    "    try:\n",
    "        net = compute_network_features()  \n",
    "        if isinstance(net, pd.DataFrame) and not net.empty:\n",
    "            nrow = net.sort_values(\"neighbor_avg_anom\", ascending=False).iloc[0]\n",
    "            lead_str = \"leads\" if nrow[\"lead_lag_best\"] > 0 else (\"lags\" if nrow[\"lead_lag_best\"] < 0 else \"co-moves\")\n",
    "            win = int(globals().get(\"_LAST_NET_WIN\", 60))\n",
    "            ml  = int(globals().get(\"_LAST_MAX_LAG\", 10))\n",
    "            cue = f\"{nrow['pool']} {lead_str} peers (corr={float(nrow['corr_best']):.2f}; win={win}, max_lag={ml}).\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    risk_now = 0.0; p10 = 0.0; p30 = 0.0\n",
    "    try:\n",
    "        n_tail = max(1, len(tail))\n",
    "        scored10 = score_latest_10m(n_tail=n_tail, write_parquet=False)\n",
    "        if isinstance(scored10, pd.DataFrame) and not scored10.empty:\n",
    "            rmax10 = scored10.sort_values(\"risk_forecast_10m\", ascending=False).iloc[0]\n",
    "            p10 = float(rmax10.get(\"risk_forecast_10m\", 0.0))\n",
    "            risk_now = float(rmax10.get(\"anom_fused\", 0.0))\n",
    "    except Exception:\n",
    "        pass\n",
    "    if not np.isfinite(risk_now) or risk_now == 0.0:\n",
    "        try:\n",
    "            risk_now = float(tail.get(\"anom_fused\", pd.Series([0.0])).iloc[-1])\n",
    "        except Exception:\n",
    "            risk_now = 0.0\n",
    "    try:\n",
    "        n_tail = max(1, len(tail))\n",
    "        scored30 = score_latest_30m(n_tail=n_tail, write_parquet=False)\n",
    "        if isinstance(scored30, pd.DataFrame) and not scored30.empty:\n",
    "            rmax30 = scored30.sort_values(\"risk_forecast_30m\", ascending=False).iloc[0]\n",
    "            p30 = float(rmax30.get(\"risk_forecast_30m\", 0.0))\n",
    "    except Exception:\n",
    "        pass\n",
    "    band = (\"High\" if (p10 >= 0.60 or p30 >= 0.60 or risk_now >= 0.90)\n",
    "            else (\"Medium\" if (p10 >= 0.35 or p30 >= 0.50 or risk_now >= 0.70) else \"Low\"))\n",
    "    try:\n",
    "        freshness = \"Fresh\" if bool(tail[\"feeds_fresh\"].iloc[-1]) else \"Stale\"\n",
    "    except Exception:\n",
    "        freshness = \"Unknown\"\n",
    "    note_lines = [f\"Fused anomaly now={risk_now:.2f}; 10-min risk={p10:.2f}; 30-min risk={p30:.2f}. Confidence {band}.\"]\n",
    "    if top3:\n",
    "        note_lines.append(\"Top drivers: \" + \"; \".join(top3))\n",
    "    if cue:\n",
    "        note_lines.append(\"Propagation: \" + cue)\n",
    "    note_lines.append(\"Action: monitor in 5m; if risk > 0.60 or 3 consecutive reds, engage mitigation (widen slippage / reroute).\")\n",
    "    note = \" \".join(note_lines)[:900]\n",
    "    try:\n",
    "        pdf_path = export_analyst_note_pdf(\n",
    "            note_text=note,\n",
    "            risk_now=risk_now,\n",
    "            risk_10m=p10,\n",
    "            risk_30m=p30,\n",
    "            contributors=top3,\n",
    "            freshness=freshness,\n",
    "            confidence=band,\n",
    "            out_path=OUT / \"analyst_note.pdf\",\n",
    "            title=\"Analyst Note v2\"\n",
    "        )\n",
    "    except Exception:\n",
    "        pdf_path = OUT / \"analyst_note.txt\"\n",
    "        try:\n",
    "            pdf_path.write_text(note)\n",
    "        except Exception:\n",
    "            pass\n",
    "    return {\"ok\": True, \"note\": note, \"pdf\": str(pdf_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "id": "d5edf1fe-1358-42de-a153-d30b5f5a2975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, math, hashlib, time\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, timedelta\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "try:\n",
    "    from sklearn.isotonic import IsotonicRegression\n",
    "    from sklearn.linear_model import LogisticRegression\n",
    "    _SK_SEV_OK = True\n",
    "except Exception:\n",
    "    _SK_SEV_OK = False\n",
    "SEV_CALIB_PATH = OUT_MODEL / \"severity_calibrator.joblib\"\n",
    "SEV_CALIB_META = OUT_MODEL / \"severity_calibrator_meta.json\"\n",
    "try:\n",
    "    from joblib import dump, load\n",
    "    _JOBLIB_OK = True\n",
    "except Exception:\n",
    "    _JOBLIB_OK = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "id": "78b15789-2c89-4496-be73-af3134b336d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _sev_predict_proba(texts: list[str]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Return uncalibrated severity probability in [0,1] for each text.\n",
    "    Uses your global SEV_MODEL if available; falls back to 0/1 on non-empty.\n",
    "    \"\"\"\n",
    "    if not texts:\n",
    "        return np.zeros(0, dtype=float)\n",
    "    try:\n",
    "        if hasattr(SEV_MODEL, \"predict_proba\"):\n",
    "            p = SEV_MODEL.predict_proba(texts)\n",
    "            if isinstance(p, (list, np.ndarray)):\n",
    "                p = np.asarray(p, dtype=float)\n",
    "                if p.ndim == 2 and p.shape[1] == 2:\n",
    "                    p = p[:, 1]\n",
    "                return np.clip(p, 0, 1)\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        if hasattr(SEV_MODEL, \"model\") and hasattr(SEV_MODEL.model, \"predict_proba\"):\n",
    "            p = SEV_MODEL.model.predict_proba(SEV_MODEL._embed(texts))[:, 1]\n",
    "            return np.clip(p, 0, 1)\n",
    "    except Exception:\n",
    "        pass\n",
    "    prior = 0.5\n",
    "    try:\n",
    "        if hasattr(SEV_MODEL, \"model\") and isinstance(SEV_MODEL.model, dict) and \"prior\" in SEV_MODEL.model:\n",
    "            prior = float(SEV_MODEL.model[\"prior\"])\n",
    "    except Exception:\n",
    "        pass\n",
    "    return np.array([prior if (t is None or str(t).strip() == \"\") else 1.0 for t in texts], dtype=float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "id": "4e26d0cb-40d2-4735-b688-817022a288f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _SeverityCalibrator:\n",
    "    \"\"\"\n",
    "    Calibrates a 1D probability p_raw -> p_cal using:\n",
    "      - IsotonicRegression (default), or\n",
    "      - Logistic (Platt) sigmoid on the raw p.\n",
    "    \"\"\"\n",
    "    def __init__(self, method: str = \"isotonic\"):\n",
    "        self.method = \"isotonic\" if method not in (\"sigmoid\", \"isotonic\") else method\n",
    "        self.model = None\n",
    "    def fit(self, p_raw: np.ndarray, y_sev_1to5: np.ndarray):\n",
    "        if not _SK_SEV_OK:\n",
    "            self.model = None\n",
    "            return self\n",
    "        p = np.clip(np.asarray(p_raw, dtype=float), 0, 1)\n",
    "        y = np.asarray(y_sev_1to5, dtype=float)\n",
    "        y_bin = (y >= 3).astype(int)\n",
    "        if len(np.unique(y_bin)) < 2:\n",
    "            self.model = (\"identity\", None)\n",
    "            return self\n",
    "        if self.method == \"isotonic\":\n",
    "            iso = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "            iso.fit(p, y_bin)\n",
    "            self.model = (\"isotonic\", iso)\n",
    "        else:\n",
    "            lr = LogisticRegression(solver=\"lbfgs\")\n",
    "            lr.fit(p.reshape(-1, 1), y_bin)\n",
    "            self.model = (\"sigmoid\", lr)\n",
    "        return self\n",
    "    def predict_proba(self, p_raw: np.ndarray) -> np.ndarray:\n",
    "        p = np.clip(np.asarray(p_raw, dtype=float), 0, 1)\n",
    "        if self.model is None:\n",
    "            return p\n",
    "        kind, m = self.model\n",
    "        if kind == \"identity\" or m is None:\n",
    "            return p\n",
    "        if kind == \"isotonic\":\n",
    "            return np.clip(m.predict(p), 0, 1)\n",
    "        return np.clip(m.predict_proba(p.reshape(-1, 1))[:, 1], 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "id": "a52329d0-1eeb-4fad-9078-b16e53436bda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_severity_calibrator(method: str = \"isotonic\"):\n",
    "    \"\"\"\n",
    "    Train a probability calibrator for severity using existing EVENTS_JSON.\n",
    "    Maps text -> p_raw (from SEV_MODEL) -> p_cal (calibrated).\n",
    "    Saves calibrator to disk.\n",
    "    \"\"\"\n",
    "    if not EVENTS_JSON.exists():\n",
    "        raise FileNotFoundError(f\"No EVENTS_JSON at {EVENTS_JSON}\")\n",
    "    events = json.loads(EVENTS_JSON.read_text())\n",
    "    if not isinstance(events, list) or not events:\n",
    "        raise ValueError(\"No events to calibrate on.\")\n",
    "    texts = [str(e.get(\"summary\", \"\")) for e in events]\n",
    "    y_1to5 = np.array([int(e.get(\"severity\", 1)) for e in events], dtype=int)\n",
    "    p_raw = _sev_predict_proba(texts)\n",
    "    calib = _SeverityCalibrator(method=method).fit(p_raw, y_1to5)\n",
    "    if _JOBLIB_OK:\n",
    "        dump(calib, SEV_CALIB_PATH)\n",
    "    meta = {\"ts\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "            \"method\": calib.method, \"n\": int(len(texts))}\n",
    "    SEV_CALIB_META.write_text(json.dumps(meta, indent=2))\n",
    "    return calib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "id": "ee768911-aebe-4f95-80c3-b6b0b0c88616",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_sev_calibrator():\n",
    "    if _JOBLIB_OK and SEV_CALIB_PATH.exists():\n",
    "        try:\n",
    "            return load(SEV_CALIB_PATH)\n",
    "        except Exception:\n",
    "            return None\n",
    "    return None\n",
    "def severity_scores_1to5(texts: list[str]) -> list[int]:\n",
    "    \"\"\"\n",
    "    Calibrated severity 1..5 from text inputs.\n",
    "    \"\"\"\n",
    "    p_raw = _sev_predict_proba(texts)\n",
    "    calib = _load_sev_calibrator()\n",
    "    p_cal = calib.predict_proba(p_raw) if calib else p_raw\n",
    "    sev = np.clip((np.floor(p_cal * 5) + 1).astype(int), 1, 5)\n",
    "    return sev.tolist()\n",
    "def enrich_events_with_calibrated_severity():\n",
    "    if not EVENTS_JSON.exists():\n",
    "        return\n",
    "    ev = json.loads(EVENTS_JSON.read_text())\n",
    "    if not isinstance(ev, list) or not ev:\n",
    "        return\n",
    "    texts = [str(e.get(\"summary\", \"\")) for e in ev]\n",
    "    sev = severity_scores_1to5(texts)\n",
    "    for i, s in enumerate(sev):\n",
    "        ev[i][\"severity\"] = int(s)\n",
    "    EVENTS_JSON.write_text(json.dumps(ev, indent=2))\n",
    "    print(\"[severity] events updated with calibrated severities (1..5)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "0388b053-521b-43c4-b9f1-f6e3918380af",
   "metadata": {},
   "outputs": [],
   "source": [
    "EMB_CACHE = OUT / \"embeddings.npz\"\n",
    "try:\n",
    "    from sentence_transformers import SentenceTransformer\n",
    "    _SBERT_OK = True\n",
    "except Exception:\n",
    "    _SBERT_OK = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "4a110c00-d2b7-484a-b5c6-c7d2257eb933",
   "metadata": {},
   "outputs": [],
   "source": [
    "class RecencyRAG:\n",
    "    \"\"\"\n",
    "    Maintains an embedding cache (NPZ) with fields:\n",
    "      ids: list[str], texts: list[str], sources: list[str], ts: list[int], vecs: np.ndarray\n",
    "    Search uses cosine similarity * recency decay, restricted to last N days.\n",
    "    \"\"\"\n",
    "    def __init__(self, model_name: str = \"sentence-transformers/all-MiniLM-L6-v2\"):\n",
    "        self.model_name = model_name\n",
    "        self.model = SentenceTransformer(model_name) if _SBERT_OK else None\n",
    "        self.ids, self.texts, self.sources, self.ts, self.vecs = [], [], [], [], None\n",
    "        self._load()\n",
    "    def _load(self):\n",
    "        if EMB_CACHE.exists():\n",
    "            z = np.load(EMB_CACHE, allow_pickle=True)\n",
    "            self.ids     = list(z[\"ids\"])\n",
    "            self.texts   = list(z[\"texts\"])\n",
    "            self.sources = list(z[\"sources\"])\n",
    "            self.ts      = list(z[\"ts\"])\n",
    "            self.vecs    = z[\"vecs\"]\n",
    "        else:\n",
    "            self.vecs = np.zeros((0, 384), dtype=np.float32)  \n",
    "    def _save(self):\n",
    "        np.savez_compressed(\n",
    "            EMB_CACHE,\n",
    "            ids=np.array(self.ids, dtype=object),\n",
    "            texts=np.array(self.texts, dtype=object),\n",
    "            sources=np.array(self.sources, dtype=object),\n",
    "            ts=np.array(self.ts, dtype=np.int64),\n",
    "            vecs=self.vecs.astype(np.float32),\n",
    "        )\n",
    "    def _embed(self, texts: list[str]) -> np.ndarray:\n",
    "        if self.model is not None:\n",
    "            v = self.model.encode(texts, normalize_embeddings=True)\n",
    "            return v.astype(np.float32)\n",
    "        rng = np.random.default_rng(42)\n",
    "        proj = rng.normal(size=(512, 384)).astype(np.float32)\n",
    "        X = np.zeros((len(texts), 512), dtype=np.float32)\n",
    "        for i, t in enumerate(texts):\n",
    "            h = hashlib.sha1(str(t).encode()).digest()\n",
    "            idx = np.frombuffer(h, dtype=np.uint8) % 512\n",
    "            X[i, idx] = 1.0\n",
    "        v = X @ proj\n",
    "        n = np.linalg.norm(v, axis=1, keepdims=True) + 1e-9\n",
    "        return (v / n).astype(np.float32)\n",
    "    def upsert(self, docs: list[dict]):\n",
    "        \"\"\"\n",
    "        docs: [{id, text, source, ts(int seconds)}]\n",
    "        Only (re)embeds new/changed docs; caches to disk.\n",
    "        \"\"\"\n",
    "        if not docs:\n",
    "            return\n",
    "        id_to_pos = {d: i for i, d in enumerate(self.ids)}\n",
    "        batch_texts, batch_pos = [], []\n",
    "        now = int(datetime.now(timezone.utc).timestamp())\n",
    "        for d in docs:\n",
    "            _id   = str(d.get(\"id\") or d.get(\"source\") or d.get(\"title\") or f\"doc-{now}\")\n",
    "            text  = str(d.get(\"text\", \"\"))[:2000]\n",
    "            src   = str(d.get(\"source\", \"\"))\n",
    "            ts    = int(d.get(\"ts\", now))\n",
    "            pos = id_to_pos.get(_id, -1)\n",
    "            if pos < 0:\n",
    "                self.ids.append(_id); self.texts.append(text); self.sources.append(src); self.ts.append(ts)\n",
    "                batch_texts.append(text); batch_pos.append(len(self.ids) - 1)\n",
    "            elif self.texts[pos] != text:\n",
    "                self.texts[pos] = text; self.sources[pos] = src; self.ts[pos] = ts\n",
    "                batch_texts.append(text); batch_pos.append(pos)\n",
    "        if batch_texts:\n",
    "            new_vecs = self._embed(batch_texts)\n",
    "            if self.vecs.shape[0] < len(self.ids):\n",
    "                extra = np.zeros((len(self.ids) - self.vecs.shape[0], new_vecs.shape[1]), dtype=np.float32)\n",
    "                self.vecs = np.vstack([self.vecs, extra])\n",
    "            for v, pos in zip(new_vecs, batch_pos):\n",
    "                self.vecs[pos] = v\n",
    "            self._save()\n",
    "    def search(self, query: str, k: int = 5, window_days: int = 14, half_life_days: int = 7):\n",
    "        \"\"\"\n",
    "        Cosine similarity * exp(-age/half_life) over docs from last `window_days`.\n",
    "        \"\"\"\n",
    "        if not self.ids:\n",
    "            return []\n",
    "        qv = self._embed([query])[0] \n",
    "        ages = np.array([max(0, int(datetime.now(timezone.utc).timestamp()) - int(t)) for t in self.ts])\n",
    "        in_window = ages <= (window_days * 86400)\n",
    "        if not in_window.any():\n",
    "            return []\n",
    "        V = self.vecs[in_window]\n",
    "        srcs = np.array(self.sources, dtype=object)[in_window]\n",
    "        texts = np.array(self.texts, dtype=object)[in_window]\n",
    "        ts    = np.array(self.ts, dtype=np.int64)[in_window]\n",
    "        sims = (V @ qv)  \n",
    "        decay = np.exp(- (ages[in_window] / (half_life_days * 86400.0)))\n",
    "        score = sims * decay\n",
    "        idx = np.argsort(-score)[:k]\n",
    "        out = []\n",
    "        for i in idx:\n",
    "            out.append({\"source\": str(srcs[i]), \"text\": str(texts[i]), \"ts\": int(ts[i]), \"score\": float(score[i])})\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "b736fbd4-fb07-413a-8954-4042aac0f236",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    RAG_B = RecencyRAG()  \n",
    "except Exception:\n",
    "    RAG_B = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "fccbd0c9-2783-43a0-882e-0961abd21640",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _recent_event_citations(hours: int = 48, top_n: int = 3) -> list[str]:\n",
    "    if not EVENTS_JSON.exists():\n",
    "        return []\n",
    "    try:\n",
    "        ev = json.loads(EVENTS_JSON.read_text())\n",
    "    except Exception:\n",
    "        return []\n",
    "    if not isinstance(ev, list) or not ev:\n",
    "        return []\n",
    "    now = datetime.now(timezone.utc).timestamp()\n",
    "    srcs = []\n",
    "    for e in sorted(ev, key=lambda z: int(z.get(\"severity\", 0)), reverse=True):\n",
    "        ts = e.get(\"ts\")\n",
    "        try:\n",
    "            t = pd.to_datetime(ts, utc=True).timestamp()\n",
    "        except Exception:\n",
    "            t = now\n",
    "        if (now - t) <= hours * 3600:\n",
    "            s = str(e.get(\"source\", \"\")).strip()\n",
    "            if s and (s not in srcs):\n",
    "                srcs.append(s)\n",
    "        if len(srcs) >= top_n:\n",
    "            break\n",
    "    return srcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "8c9b317a-5409-4792-8505-9434f5a57db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_analyst_note_v2():\n",
    "    \"\"\"\n",
    "    (Patched) Adds source citations line when events contributed.\n",
    "    Keeps win/max_lag in propagation cue (from your earlier patch).\n",
    "    \"\"\"\n",
    "    import json\n",
    "    tail = load_live().sort_values(\"ts\").groupby(\"pool\").tail(1)\n",
    "    if tail.empty:\n",
    "        return {\"ok\": False, \"reason\": \"no data\"}\n",
    "    top3 = []\n",
    "    try:\n",
    "        if EXPLAIN_JSON.exists():\n",
    "            explain = json.loads(EXPLAIN_JSON.read_text())\n",
    "        else:\n",
    "            explain = explain_forecast_10m()\n",
    "        top3 = (explain or {}).get(\"top_contributors\", [])[:3]\n",
    "    except Exception:\n",
    "        pass\n",
    "    cue = None\n",
    "    try:\n",
    "        net = compute_network_features()\n",
    "        if isinstance(net, pd.DataFrame) and not net.empty:\n",
    "            nrow = net.sort_values(\"neighbor_avg_anom\", ascending=False).iloc[0]\n",
    "            lead_str = \"leads\" if nrow[\"lead_lag_best\"] > 0 else (\"lags\" if nrow[\"lead_lag_best\"] < 0 else \"co-moves\")\n",
    "            win = int(globals().get(\"_LAST_NET_WIN\", 60))\n",
    "            ml  = int(globals().get(\"_LAST_MAX_LAG\", 10))\n",
    "            cue = f\"{nrow['pool']} {lead_str} peers (corr={float(nrow['corr_best']):.2f}; win={win}, max_lag={ml}).\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    risk_now = 0.0; p10 = 0.0; p30 = 0.0\n",
    "    try:\n",
    "        scored10 = score_latest_10m(n_tail=max(1, len(tail)), write_parquet=False)\n",
    "        if isinstance(scored10, pd.DataFrame) and not scored10.empty:\n",
    "            rmax10 = scored10.sort_values(\"risk_forecast_10m\", ascending=False).iloc[0]\n",
    "            p10 = float(rmax10.get(\"risk_forecast_10m\", 0.0))\n",
    "            risk_now = float(rmax10.get(\"anom_fused\", 0.0))\n",
    "    except Exception:\n",
    "        pass\n",
    "    if not np.isfinite(risk_now) or risk_now == 0.0:\n",
    "        try:\n",
    "            risk_now = float(tail.get(\"anom_fused\", pd.Series([0.0])).iloc[-1])\n",
    "        except Exception:\n",
    "            risk_now = 0.0\n",
    "    try:\n",
    "        scored30 = score_latest_30m(n_tail=max(1, len(tail)), write_parquet=False)\n",
    "        if isinstance(scored30, pd.DataFrame) and not scored30.empty:\n",
    "            rmax30 = scored30.sort_values(\"risk_forecast_30m\", ascending=False).iloc[0]\n",
    "            p30 = float(rmax30.get(\"risk_forecast_30m\", 0.0))\n",
    "    except Exception:\n",
    "        pass\n",
    "    band = (\"High\" if (p10 >= 0.60 or p30 >= 0.60 or risk_now >= 0.90)\n",
    "            else (\"Medium\" if (p10 >= 0.35 or p30 >= 0.50 or risk_now >= 0.70) else \"Low\"))\n",
    "    try:\n",
    "        freshness = \"Fresh\" if bool(tail[\"feeds_fresh\"].iloc[-1]) else \"Stale\"\n",
    "    except Exception:\n",
    "        freshness = \"Unknown\"\n",
    "\n",
    "    note_lines = [\n",
    "        f\"Fused anomaly now={risk_now:.2f}; 10-min risk={p10:.2f}; 30-min risk={p30:.2f}. Confidence {band}.\"\n",
    "    ]\n",
    "    if top3:\n",
    "        note_lines.append(\"Top drivers: \" + \"; \".join(top3))\n",
    "    if cue:\n",
    "        note_lines.append(\"Propagation: \" + cue)\n",
    "    cites = _recent_event_citations(hours=48, top_n=3)\n",
    "    if cites:\n",
    "        note_lines.append(\"Sources: \" + \" | \".join(cites))\n",
    "    note_lines.append(\"Action: monitor in 5m; if risk > 0.60 or 3 consecutive reds, engage mitigation (widen slippage / reroute).\")\n",
    "    note = \" \".join(note_lines)[:900]\n",
    "    try:\n",
    "        pdf_path = export_analyst_note_pdf(\n",
    "            note_text=note,\n",
    "            risk_now=risk_now,\n",
    "            risk_10m=p10,\n",
    "            risk_30m=p30,\n",
    "            contributors=top3,\n",
    "            freshness=freshness,\n",
    "            confidence=band,\n",
    "            out_path=OUT / \"analyst_note.pdf\",\n",
    "            title=\"Analyst Note v2\"\n",
    "        )\n",
    "    except Exception:\n",
    "        pdf_path = OUT / \"analyst_note.txt\"\n",
    "        try: pdf_path.write_text(note)\n",
    "        except Exception: pass\n",
    "    return {\"ok\": True, \"note\": note, \"pdf\": str(pdf_path), \"citations\": cites}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "id": "90745641-1506-4312-b0ff-4cbb7b786929",
   "metadata": {},
   "outputs": [],
   "source": [
    "ALERTS_STATE_JSON = OUT / \"alerts_state.json\"\n",
    "_LAST_ALERT = {\"hash\": \"\", \"ts\": 0.0, \"acked\": False}\n",
    "def _load_alert_state():\n",
    "    if ALERTS_STATE_JSON.exists():\n",
    "        try:\n",
    "            s = json.loads(ALERTS_STATE_JSON.read_text())\n",
    "            _LAST_ALERT.update(s)\n",
    "        except Exception:\n",
    "            pass\n",
    "def _save_alert_state():\n",
    "    ALERTS_STATE_JSON.write_text(json.dumps(_LAST_ALERT, indent=2))\n",
    "_load_alert_state()\n",
    "def _mk_alert_hash(payload: dict) -> str:\n",
    "    blob = json.dumps(payload, sort_keys=True)\n",
    "    return hashlib.sha1(blob.encode()).hexdigest()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "id": "4360b23c-9839-43eb-b19f-1f2f970f0f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_incident_snapshot(level: str, note: dict | None = None, citations: list[str] | None = None, extras: dict | None = None) -> Path:\n",
    "    \"\"\"\n",
    "    Writes outputs/incident_{ts}.md with state blob, features, network cue, citations, policy level.\n",
    "    \"\"\"\n",
    "    ts = datetime.now(timezone.utc).isoformat(timespec=\"seconds\")\n",
    "    cites = citations or []\n",
    "    net = {}\n",
    "    try:\n",
    "        netdf = compute_network_features()\n",
    "        if isinstance(netdf, pd.DataFrame) and not netdf.empty:\n",
    "            nrow = netdf.sort_values(\"neighbor_avg_anom\", ascending=False).iloc[0].to_dict()\n",
    "            net = {k: (float(v) if isinstance(v, (int, float, np.floating)) else v) for k, v in nrow.items()}\n",
    "            net[\"win\"] = int(globals().get(\"_LAST_NET_WIN\", 60))\n",
    "            net[\"max_lag\"] = int(globals().get(\"_LAST_MAX_LAG\", 10))\n",
    "    except Exception:\n",
    "        pass\n",
    "    top = {}\n",
    "    try:\n",
    "        if EXPLAIN_JSON.exists():\n",
    "            top = json.loads(EXPLAIN_JSON.read_text())\n",
    "    except Exception:\n",
    "        pass\n",
    "    body = []\n",
    "    body.append(f\"# Incident Snapshot — {level.upper()}\")\n",
    "    body.append(f\"Generated: {ts}\\n\")\n",
    "    if note and note.get(\"note\"):\n",
    "        body.append(\"## Analyst Note\")\n",
    "        body.append(note[\"note\"] + \"\\n\")\n",
    "    body.append(\"## Top Contributors\")\n",
    "    body.append(\"```json\\n\" + json.dumps(top, indent=2) + \"\\n```\")\n",
    "    body.append(\"## Network Cue\")\n",
    "    body.append(\"```json\\n\" + json.dumps(net, indent=2) + \"\\n```\")\n",
    "    if cites:\n",
    "        body.append(\"## Citations\\n\" + \"\\n\".join(f\"- {c}\" for c in cites))\n",
    "    if extras:\n",
    "        body.append(\"## State\\n```json\\n\" + json.dumps(extras, indent=2) + \"\\n```\")\n",
    "    md = \"\\n\".join(body)\n",
    "    path = OUT / f\"incident_{ts.replace(':','').replace('-','').replace('T','_')}.md\"\n",
    "    path.write_text(md)\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "id": "938d7f1c-7829-490b-bf6e-b9a56076bed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def trigger_alerts_if_needed(webhook_url: str, n_tail: int = 80, min_rows_for_incidents: int = 60, cooldown_s: int = 600) -> bool:\n",
    "    \"\"\"Post 'red' incidents with requires_ack branch; persist cooldown/ack state.\"\"\"\n",
    "    if not webhook_url:\n",
    "        print(\"[alert] no webhook configured\")\n",
    "        return False\n",
    "    try:\n",
    "        import requests\n",
    "    except Exception as e:\n",
    "        print(\"[alert] requests not available:\", e)\n",
    "        return False\n",
    "    try:\n",
    "        dec = decide_latest(n_tail=n_tail, min_rows_for_incidents=min_rows_for_incidents)\n",
    "    except Exception as e:\n",
    "        print(\"[alert] decide_latest failed:\", e)\n",
    "        return False\n",
    "    reds = dec[dec[\"level\"] == \"red\"] if isinstance(dec, pd.DataFrame) else pd.DataFrame()\n",
    "    oranges = dec[dec[\"level\"] == \"orange\"] if isinstance(dec, pd.DataFrame) else pd.DataFrame()\n",
    "    events = reds if not reds.empty else oranges\n",
    "    if events.empty:\n",
    "        print(\"[alert] no orange/red alerts\")\n",
    "        return True\n",
    "    payload = events.to_dict(orient=\"records\")\n",
    "    envelope = {\"alerts\": payload, \"ts\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\")}\n",
    "    h = _mk_alert_hash(envelope)\n",
    "    now = time.time()\n",
    "    if _LAST_ALERT.get(\"hash\") == h and (now - float(_LAST_ALERT.get(\"ts\", 0))) < cooldown_s:\n",
    "        print(\"[alert] suppressed (duplicate/cooldown)\")\n",
    "        return True\n",
    "    _LAST_ALERT.update({\"hash\": h, \"ts\": now, \"acked\": False})\n",
    "    _save_alert_state()\n",
    "    level = \"red\" if not reds.empty else \"orange\"\n",
    "    requires_ack = (level == \"red\")\n",
    "    ack_url = f\"/ack/{h}\" \n",
    "    note = build_analyst_note_v2()\n",
    "    cites = note.get(\"citations\", [])\n",
    "    snap = write_incident_snapshot(level=level, note=note, citations=cites,\n",
    "                                   extras={\"hash\": h, \"requires_ack\": requires_ack})\n",
    "    post = {\n",
    "        \"level\": level,\n",
    "        \"requires_ack\": requires_ack,\n",
    "        \"ack_hash\": h,\n",
    "        \"ack_url\": ack_url,\n",
    "        \"cooldown_s\": cooldown_s,\n",
    "        \"snapshot_md\": str(snap),\n",
    "        \"alerts\": payload,\n",
    "        \"note\": note.get(\"note\", \"\"),\n",
    "        \"citations\": cites,\n",
    "    }\n",
    "    try:\n",
    "        r = requests.post(webhook_url, json=post, timeout=8)\n",
    "        print(f\"[alert] posted {len(payload)} {level} alerts → {r.status_code}\")\n",
    "        ok = (200 <= r.status_code < 300)\n",
    "        if ok:\n",
    "            _save_alert_state()\n",
    "        return ok\n",
    "    except Exception as e:\n",
    "        print(\"[alert] post failed:\", e)\n",
    "        return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "id": "5b5527e7-5dcb-4e35-b4b6-83072c2016a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    from fastapi import FastAPI\n",
    "except Exception:\n",
    "    FastAPI = None\n",
    "if FastAPI is not None:\n",
    "    try:\n",
    "        app  \n",
    "    except NameError:\n",
    "        app = FastAPI(title=\"Depeg Sentinel MCP\", version=\"2.1.0\")\n",
    "    @app.post(\"/ack/{h}\")\n",
    "    def ack_alert(h: str):\n",
    "        \"\"\"\n",
    "        Mark the latest pending alert as acknowledged.\n",
    "        Your Slack button (or CLI) can hit this endpoint before auto-mitigation.\n",
    "        \"\"\"\n",
    "        _load_alert_state()\n",
    "        if _LAST_ALERT.get(\"hash\") == h:\n",
    "            _LAST_ALERT[\"acked\"] = True\n",
    "            _save_alert_state()\n",
    "            return {\"ok\": True, \"acknowledged\": True, \"hash\": h}\n",
    "        return {\"ok\": False, \"acknowledged\": False, \"reason\": \"hash mismatch or no pending alert\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "id": "13a12a13-1b86-4b6a-bca5-e8665f51b2bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[severity] events updated with calibrated severities (1..5)\n",
      "[alert] no webhook configured\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 212,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_severity_calibrator(method=\"isotonic\")  \n",
    "enrich_events_with_calibrated_severity()\n",
    "if RAG_B:\n",
    "    RAG_B.upsert([{\"id\":\"curve-123\", \"text\":\"post text...\", \"source\":\"https://gov.curve.fi/t/...\", \"ts\": int(time.time())}])\n",
    "trigger_alerts_if_needed(WEBHOOK, n_tail=80, min_rows_for_incidents=60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "id": "534ee2eb-a855-469a-b822-3ed4020d8dc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, io, sys, json, time, hmac, hashlib, shutil, subprocess, zipfile\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from pathlib import Path\n",
    "from typing import Optional, List, Dict, Any, Sequence\n",
    "from fastapi import APIRouter, FastAPI, Depends, HTTPException, Header, Request, Response\n",
    "from pydantic import BaseModel, Field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "id": "9703f58e-fe0a-41f9-8409-1015fd802e12",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT        = Path(globals().get(\"OUT\", Path(\"outputs\")))\n",
    "OUT_MODEL  = Path(globals().get(\"OUT_MODEL\", OUT / \"model\"))\n",
    "OUT_ARCH   = Path(globals().get(\"OUT_ARCH\", OUT / \"archive\"))\n",
    "OUT_LOGS   = Path(globals().get(\"OUT_LOGS\", OUT / \"logs\"))\n",
    "LIVE_CSV   = Path(globals().get(\"LIVE_CSV\", OUT / \"live_dataset.csv\"))\n",
    "FORECAST_10M_PARQUET = Path(globals().get(\"FORECAST_10M_PARQUET\", OUT / \"forecast_10m.parquet\"))\n",
    "FORECAST_30M_PARQUET = Path(globals().get(\"FORECAST_30M_PARQUET\", OUT / \"forecast_30m.parquet\"))\n",
    "DET_AP_JSON = Path(globals().get(\"DET_AP_JSON\", OUT / \"artifacts\" / \"detector_pr_auc.json\"))\n",
    "CAL_10M_JSON = Path(globals().get(\"CAL_10M_JSON\", OUT / \"artifacts\" / \"calibration_10m.json\"))\n",
    "CAL_30M_JSON = Path(globals().get(\"CAL_30M_JSON\", OUT / \"artifacts\" / \"calibration_30m.json\"))\n",
    "ALERTS_STATE_JSON = Path(globals().get(\"ALERTS_STATE_JSON\", OUT / \"alerts_state.json\"))\n",
    "for p in (OUT, OUT_MODEL, OUT_ARCH, OUT_LOGS, OUT / \"artifacts\"):\n",
    "    p.mkdir(parents=True, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "a9e7b91a-1145-4d96-a194-f1fc0f4b5d34",
   "metadata": {},
   "outputs": [],
   "source": [
    "API_KEY     = os.getenv(\"API_KEY\", \"\").strip()         \n",
    "HMAC_SECRET = os.getenv(\"HMAC_SECRET\", \"\").strip()     \n",
    "MAX_SKEW_S  = int(os.getenv(\"HMAC_MAX_SKEW_S\", \"300\")) \n",
    "class AuthError(HTTPException):\n",
    "    def __init__(self, detail=\"Unauthorized\"):\n",
    "        super().__init__(status_code=401, detail=detail)\n",
    "async def _auth_guard(\n",
    "    request: Request,\n",
    "    x_api_key: Optional[str] = Header(default=None),\n",
    "    x_timestamp: Optional[str] = Header(default=None),\n",
    "    x_signature: Optional[str] = Header(default=None),\n",
    "):\n",
    "    if API_KEY:\n",
    "        if not x_api_key or x_api_key != API_KEY:\n",
    "            raise AuthError(\"Invalid API key\")\n",
    "    if HMAC_SECRET:\n",
    "        if not x_timestamp or not x_signature:\n",
    "            raise AuthError(\"Missing HMAC headers\")\n",
    "        try:\n",
    "            ts = int(x_timestamp)\n",
    "        except Exception:\n",
    "            raise AuthError(\"Bad timestamp\")\n",
    "        if abs(time.time() - ts) > MAX_SKEW_S:\n",
    "            raise AuthError(\"Timestamp out of range\")\n",
    "        body = await request.body()\n",
    "        msg  = f\"{ts}.{body.decode('utf-8', errors='ignore')}\"\n",
    "        dig  = hmac.new(HMAC_SECRET.encode(), msg.encode(), hashlib.sha256).hexdigest()\n",
    "        if not hmac.compare_digest(dig, x_signature):\n",
    "            raise AuthError(\"Bad signature\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "id": "c414f084-f49b-4ac8-b662-901b57e5eb0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreZooItem(BaseModel):\n",
    "    ts: str\n",
    "    pool: str\n",
    "    z_if: float | None = None\n",
    "    z_lof: float | None = None\n",
    "    z_ocsvm: float | None = None\n",
    "    z_cusum: float | None = None\n",
    "    z_ae: float | None = None\n",
    "    z_ae_seq: float | None = None\n",
    "    anom_fused: float | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "id": "99bb8058-ff31-4426-9c11-aefac2ea0f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ScoreZooResponse(BaseModel):\n",
    "    items: List[ScoreZooItem]\n",
    "    model_config = {\n",
    "        \"json_schema_extra\": {\n",
    "            \"examples\": [{\n",
    "                \"items\": [{\n",
    "                    \"ts\": \"2025-08-27T10:22:05+00:00\", \"pool\": \"USDC/USDT-uni\",\n",
    "                    \"z_if\": 0.12, \"z_lof\": 0.08, \"z_ocsvm\": 0.05, \"z_cusum\": 0.10,\n",
    "                    \"z_ae\": 0.04, \"z_ae_seq\": 0.18, \"anom_fused\": 0.18\n",
    "                }]\n",
    "            }]\n",
    "        }\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "id": "db0fa6f8-fac3-4951-a8d8-3c88b5ffecc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForecastItem(BaseModel):\n",
    "    ts: str\n",
    "    pool: str\n",
    "    anom_fused: float | None = None\n",
    "    risk_forecast_10m: float | None = None\n",
    "    risk_forecast_30m: float | None = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "id": "c9fe47b1-c5fd-412f-a124-f275ef59e136",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ForecastResponse(BaseModel):\n",
    "    items: List[ForecastItem]\n",
    "    model_config = {\n",
    "        \"json_schema_extra\": {\n",
    "            \"examples\": [{\n",
    "                \"items\": [{\n",
    "                    \"ts\": \"2025-08-27T10:22:05+00:00\", \"pool\": \"DAI/USDC-curve\",\n",
    "                    \"anom_fused\": 0.31, \"risk_forecast_10m\": 0.44, \"risk_forecast_30m\": 0.57\n",
    "                }]\n",
    "            }]\n",
    "        }\n",
    "    }\n",
    "class ExplainResponse(BaseModel):\n",
    "    top_contributors: List[str] = []\n",
    "    model_config = {\"json_schema_extra\": {\"examples\": [{\"top_contributors\": [\"dev_roll_std↑\", \"spot_twap_gap_bps↑\", \"event_severity_max_24h=5\"]}]}}\n",
    "class EventItem(BaseModel):\n",
    "    ts: str\n",
    "    summary: str\n",
    "    severity: int = Field(ge=1, le=5)\n",
    "    source: str | None = None\n",
    "class TopEventsResponse(BaseModel):\n",
    "    items: List[EventItem]\n",
    "class NetworkSignal(BaseModel):\n",
    "    ts: str\n",
    "    pool: str\n",
    "    neighbor_max_dev: float\n",
    "    neighbor_avg_anom: float\n",
    "    lead_lag_best: int\n",
    "    corr_best: float\n",
    "class NetworkSignalsResponse(BaseModel):\n",
    "    items: List[NetworkSignal]\n",
    "class PolicyState(BaseModel):\n",
    "    feeds_fresh: bool | None = None\n",
    "    recent_forecasts: Dict[str, float] | None = None\n",
    "class PolicyDecision(BaseModel):\n",
    "    level: str\n",
    "    actions: List[Dict[str, str]]\n",
    "    rationale: str\n",
    "    requires_ack: bool = False\n",
    "    model_config = {\n",
    "        \"json_schema_extra\": {\"examples\": [{\n",
    "            \"level\": \"orange\",\n",
    "            \"actions\": [{\"title\": \"Prepare mitigation\", \"rationale\": \"p10=0.62, fused=0.91\"}],\n",
    "            \"rationale\": \"Elevated spread and TVL outflow; watch 5m\",\n",
    "            \"requires_ack\": False\n",
    "        }]}\n",
    "    }\n",
    "class RetrainCheckResponse(BaseModel):\n",
    "    should_retrain: bool\n",
    "    reason: str\n",
    "    drift: Dict[str, Any] | None = None\n",
    "class SnapshotResponse(BaseModel):\n",
    "    note: Dict[str, Any]\n",
    "    report: Dict[str, Any] | None = None\n",
    "class Healthz(BaseModel):\n",
    "    ok: bool\n",
    "    run_id: str\n",
    "class Readyz(BaseModel):\n",
    "    ok: bool\n",
    "    rpc_ok: bool\n",
    "    csv_write_ok: bool\n",
    "    models_ok: bool\n",
    "    detail: Dict[str, Any] = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "id": "77c832b0-6a57-4234-be61-3d91d22d7771",
   "metadata": {},
   "outputs": [],
   "source": [
    "RUN_ID = os.getenv(\"RUN_ID\") or datetime.now(timezone.utc).strftime(\"run-%Y%m%dT%H%M%S\")\n",
    "LOG_PATH = OUT_LOGS / f\"{RUN_ID}.jsonl\"\n",
    "def log_event(kind: str, payload: Dict[str, Any]):\n",
    "    rec = {\"ts\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"), \"run_id\": RUN_ID, \"kind\": kind, **payload}\n",
    "    with LOG_PATH.open(\"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(rec, ensure_ascii=False) + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "b9b5d7b7-fa97-4cfa-bcb8-f62bc08e47f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _git_hash() -> str:\n",
    "    try:\n",
    "        return subprocess.check_output([\"git\", \"rev-parse\", \"HEAD\"], stderr=subprocess.DEVNULL).decode().strip()\n",
    "    except Exception:\n",
    "        return \"nogit\"\n",
    "def _time_split_idx(ts_series, frac: float = 0.70):\n",
    "    import numpy as np, pandas as pd\n",
    "    ts = pd.to_datetime(ts_series, utc=True, errors=\"coerce\")\n",
    "    cut = ts.quantile(frac)\n",
    "    idx_tr = (ts <= cut).to_numpy().nonzero()[0]\n",
    "    idx_te = (ts >  cut).to_numpy().nonzero()[0]\n",
    "    return idx_tr, idx_te"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "id": "8df5299a-ee7d-49d5-8091-ce4f3ea6d532",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_model_registry(\n",
    "    name: str,\n",
    "    feature_list: Sequence[str],\n",
    "    labels: Dict[str, Any],\n",
    "    metrics: Dict[str, float] | None = None,\n",
    "    keep_last: int = 5,\n",
    ") -> Path:\n",
    "    OUT_MODEL.mkdir(parents=True, exist_ok=True)\n",
    "    stamp = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%S\")\n",
    "    payload = {\n",
    "        \"ts\": datetime.now(timezone.utc).isoformat(timespec=\"seconds\"),\n",
    "        \"model_name\": name,\n",
    "        \"features\": list(feature_list),\n",
    "        \"labels\": labels,\n",
    "        \"metrics\": metrics or {},\n",
    "        \"git_hash\": _git_hash(),\n",
    "        \"artifacts\": {\n",
    "            \"forecast_10m\": str(FORECAST_10M_PARQUET) if FORECAST_10M_PARQUET.exists() else None,\n",
    "            \"forecast_30m\": str(FORECAST_30M_PARQUET) if FORECAST_30M_PARQUET.exists() else None,\n",
    "            \"detector_pr_auc\": str(DET_AP_JSON) if DET_AP_JSON.exists() else None,\n",
    "            \"cal_10m\": str(CAL_10M_JSON) if CAL_10M_JSON.exists() else None,\n",
    "            \"cal_30m\": str(CAL_30M_JSON) if CAL_30M_JSON.exists() else None,\n",
    "        },\n",
    "    }\n",
    "    path = OUT_MODEL / f\"version_{name}_{stamp}.json\"\n",
    "    path.write_text(json.dumps(payload, indent=2))\n",
    "    (OUT_MODEL / \"version.json\").write_text(json.dumps(payload, indent=2))\n",
    "    files = sorted(OUT_MODEL.glob(f\"version_{name}_*.json\"))\n",
    "    if len(files) > keep_last:\n",
    "        for old in files[:-keep_last]:\n",
    "            try: old.unlink()\n",
    "            except Exception: pass\n",
    "    log_event(\"model_registry\", {\"name\": name, \"path\": str(path), \"metrics\": metrics or {}})\n",
    "    return path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "id": "f434058a-dcd2-4538-84e8-6015c31cd322",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _maybe_register_after_train_10m(feature_cols: Sequence[str], ap: float | None, brier: float | None):\n",
    "    labels = {\n",
    "        \"y_10m\": getattr(globals().get(\"LABEL_DEF_10M\", {}), \"dict\", lambda: {} )() or {\n",
    "            \"dev_thr\": 0.005, \"fused_thr\": 0.90, \"horizon\": 10\n",
    "        }\n",
    "    }\n",
    "    metrics = {\"AP\": float(ap) if ap is not None else None, \"Brier\": float(brier) if brier is not None else None}\n",
    "    write_model_registry(\"forecast_10m\", feature_cols, labels, metrics)\n",
    "def _maybe_register_after_train_30m(feature_cols: Sequence[str], ap: float | None, brier: float | None):\n",
    "    labels = {\n",
    "        \"y_30m\": getattr(globals().get(\"LABEL_DEF_30M\", {}), \"dict\", lambda: {} )() or {\n",
    "            \"dev_thr\": 0.005, \"fused_thr\": 0.90, \"horizon\": 30\n",
    "        }\n",
    "    }\n",
    "    metrics = {\"AP\": float(ap) if ap is not None else None, \"Brier\": float(brier) if brier is not None else None}\n",
    "    write_model_registry(\"forecast_30m\", feature_cols, labels, metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "id": "2fc8dcdb-82b7-4bfa-bd1f-157afefcb6b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def daily_zip_archives(retain_days_zip: int = 30, retain_days_raw: int = 14) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Zips yesterday's OUT/*.{csv,parquet,json} into OUT/archive/YYYYMMDD.zip and\n",
    "    prunes zips older than retain_days_zip and raw files older than retain_days_raw.\n",
    "    \"\"\"\n",
    "    now = datetime.now(timezone.utc)\n",
    "    yday = (now - timedelta(days=1)).strftime(\"%Y%m%d\")\n",
    "    zip_path = OUT_ARCH / f\"{yday}.zip\"\n",
    "    def _iter_files():\n",
    "        for p in OUT.rglob(\"*\"):\n",
    "            if p.is_file() and p.suffix.lower() in {\".csv\", \".parquet\", \".json\"}:\n",
    "                try:\n",
    "                    mtime = datetime.utcfromtimestamp(p.stat().st_mtime)\n",
    "                except Exception:\n",
    "                    continue\n",
    "                if mtime.strftime(\"%Y%m%d\") == yday:\n",
    "                    yield p\n",
    "    created = False\n",
    "    with zipfile.ZipFile(zip_path, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "        for f in _iter_files():\n",
    "            arcname = f.relative_to(OUT).as_posix()\n",
    "            zf.write(f, arcname)\n",
    "            created = True\n",
    "    cutoff_zip = now - timedelta(days=retain_days_zip)\n",
    "    for z in OUT_ARCH.glob(\"*.zip\"):\n",
    "        try:\n",
    "            ts = datetime.strptime(z.stem, \"%Y%m%d\")\n",
    "            if ts < cutoff_zip:\n",
    "                z.unlink()\n",
    "        except Exception:\n",
    "            pass\n",
    "    cutoff_raw = now - timedelta(days=retain_days_raw)\n",
    "    pruned = 0\n",
    "    for p in OUT.rglob(\"*\"):\n",
    "        if p.is_file() and p.suffix.lower() in {\".csv\", \".parquet\", \".json\"} and OUT_ARCH not in p.parents:\n",
    "            try:\n",
    "                mtime = datetime.utcfromtimestamp(p.stat().st_mtime)\n",
    "                if mtime < cutoff_raw:\n",
    "                    p.unlink(); pruned += 1\n",
    "            except Exception:\n",
    "                pass\n",
    "    result = {\"zip\": str(zip_path), \"created\": created, \"pruned_raw\": pruned}\n",
    "    log_event(\"archive_daily\", result)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "id": "e8808d14-bcfc-4cc3-8cca-abbf41159ef4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _rpc_ready() -> bool:\n",
    "    try:\n",
    "        w3 = getattr(globals().get(\"onchain\", None), \"w3\", None)\n",
    "        if not w3: return True  \n",
    "        _ = w3.eth.block_number\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "def _csv_write_ready() -> bool:\n",
    "    try:\n",
    "        tmp = OUT / \".readyz.tmp\"\n",
    "        tmp.write_text(\"ok\")\n",
    "        tmp.unlink(missing_ok=True)\n",
    "        return True\n",
    "    except Exception:\n",
    "        return False\n",
    "def _models_ready() -> bool:\n",
    "    paths = [\n",
    "        OUT_MODEL / \"forecast_10m_xgb.joblib\",\n",
    "        OUT_MODEL / \"forecast_10m_calib.joblib\",\n",
    "        OUT_MODEL / \"forecast_30m_xgb.joblib\",\n",
    "        OUT_MODEL / \"forecast_30m_calib.joblib\",\n",
    "    ]\n",
    "    return any(p.exists() for p in paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "id": "96e3e4e5-ad39-4300-a0a7-514bc6af4da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ml_router = APIRouter(prefix=\"/ml\", tags=[\"MCP3: risk-intel\"], dependencies=[Depends(_auth_guard)])\n",
    "policy_router = APIRouter(prefix=\"/policy\", tags=[\"MCP4: meta-controller\"], dependencies=[Depends(_auth_guard)])\n",
    "ops_router = APIRouter(prefix=\"/ops\", tags=[\"Ops\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "id": "4251af14-edac-4f8c-9d2f-72509829198e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ml_router.get(\"/score_zoo\", response_model=ScoreZooResponse, summary=\"Parallel anomaly scores\", description=\"IF/LOF/OCSVM/CUSUM/AE scores + fused per pool.\")\n",
    "def api_score_zoo(pools: Optional[List[str]] = None):\n",
    "    fn = globals().get(\"run_anomaly_zoo_update_live\")\n",
    "    if not fn: raise HTTPException(500, \"zoo function missing\")\n",
    "    df = fn()\n",
    "    if pools:\n",
    "        df = df[df[\"pool\"].isin(pools)]\n",
    "    items = df[[\"ts\",\"pool\",\"z_if\",\"z_lof\",\"z_ocsvm\",\"z_cusum\",\"z_ae\",\"z_ae_seq\",\"anom_fused\"]].tail(200).to_dict(\"records\")\n",
    "    return {\"items\": items}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "id": "03e4594f-779f-41e8-bde8-1246467b51ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ml_router.get(\"/forecast\", response_model=ForecastResponse, summary=\"Risk forecasts\", description=\"Return 10m/30m probabilities for tail rows.\")\n",
    "def api_forecast(pools: Optional[List[str]] = None, horizon: List[int] = [10, 30]):\n",
    "    items: List[Dict[str, Any]] = []\n",
    "    df_live = globals().get(\"load_live\", lambda: None)()\n",
    "    if df_live is None or df_live.empty:\n",
    "        return {\"items\": []}\n",
    "    h10 = 10 in horizon and \"score_latest_10m\" in globals()\n",
    "    h30 = 30 in horizon and \"score_latest_30m\" in globals()\n",
    "    base = df_live[[\"ts\",\"pool\",\"anom_fused\"]].tail(200).copy()\n",
    "    if h10:\n",
    "        f10 = globals()[\"score_latest_10m\"](n_tail=len(base), write_parquet=False)\n",
    "        base = base.merge(f10[[\"ts\",\"pool\",\"risk_forecast_10m\"]], on=[\"ts\",\"pool\"], how=\"left\")\n",
    "    if h30:\n",
    "        f30 = globals()[\"score_latest_30m\"](n_tail=len(base), write_parquet=False)\n",
    "        base = base.merge(f30[[\"ts\",\"pool\",\"risk_forecast_30m\"]], on=[\"ts\",\"pool\"], how=\"left\")\n",
    "    if pools:\n",
    "        base = base[base[\"pool\"].isin(pools)]\n",
    "    for r in base.to_dict(\"records\"):\n",
    "        items.append(r)\n",
    "    return {\"items\": items}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "id": "abe29d7f-cbe9-43da-b47b-14ff976f7830",
   "metadata": {},
   "outputs": [],
   "source": [
    "@ml_router.get(\"/explain\", response_model=ExplainResponse, summary=\"Explainability (current)\", description=\"Top contributing features/events for current alerts.\")\n",
    "def api_explain():\n",
    "    if \"explain_forecast_10m\" in globals():\n",
    "        obj = globals()[\"explain_forecast_10m\"]()\n",
    "        return {\"top_contributors\": obj.get(\"top_contributors\", [])}\n",
    "    return {\"top_contributors\": []}\n",
    "@ml_router.get(\"/top_events\", response_model=TopEventsResponse, summary=\"High-severity events\")\n",
    "def api_top_events(since: Optional[str] = None):\n",
    "    fn = globals().get(\"_load_events\", None)\n",
    "    if not fn:\n",
    "        return {\"items\": []}\n",
    "    ev = fn()\n",
    "    if since:\n",
    "        ev = [e for e in ev if str(e.get(\"ts\",\"\")) >= since]\n",
    "    ev.sort(key=lambda e: int(e.get(\"severity\", 0)), reverse=True)\n",
    "    out = [{\"ts\": e.get(\"ts\",\"\"), \"summary\": e.get(\"summary\",\"\"), \"severity\": int(e.get(\"severity\",1)), \"source\": e.get(\"source\",\"\")} for e in ev[:50]]\n",
    "    return {\"items\": out}\n",
    "@ml_router.get(\"/network\", response_model=NetworkSignalsResponse, summary=\"Cross-pool network signals\", description=\"Correlation/lead-lag/neighbor features.\")\n",
    "def api_network(pools: Optional[List[str]] = None):\n",
    "    fn = globals().get(\"compute_network_features\", None)\n",
    "    if not fn:\n",
    "        return {\"items\": []}\n",
    "    df = fn()\n",
    "    if pools:\n",
    "        df = df[df[\"pool\"].isin(pools)]\n",
    "    return {\"items\": df.to_dict(\"records\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "id": "df82aa10-4bc4-4b85-8ee3-c8ad45a51ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@policy_router.post(\"/decide\", response_model=PolicyDecision, summary=\"Decide policy level/actions\", description=\"Score gating + escalation rules. Requires auth.\")\n",
    "def api_policy_decide(state: Optional[PolicyState] = None):\n",
    "    fn = globals().get(\"decide_latest\", None)\n",
    "    if not fn:\n",
    "        raise HTTPException(500, \"policy function missing\")\n",
    "    df = fn(n_tail=20)  \n",
    "    row = df.tail(1).to_dict(\"records\")[0]\n",
    "    decision = {\n",
    "        \"level\": row.get(\"level\", \"yellow\"),\n",
    "        \"actions\": row.get(\"actions\", [{\"title\":\"Monitor in 5m\",\"rationale\":\"default\"}]),\n",
    "        \"rationale\": row.get(\"rationale\", \"rule-based\"),\n",
    "        \"requires_ack\": (row.get(\"level\") == \"red\"),\n",
    "    }\n",
    "    log_event(\"policy_decision\", {\"input_state\": state.dict() if state else {}, \"decision\": decision})\n",
    "    return decision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "id": "877ff87c-8111-4510-b0af-22c5f2131413",
   "metadata": {},
   "outputs": [],
   "source": [
    "@policy_router.get(\"/retrain_check\", response_model=RetrainCheckResponse, summary=\"Retrain gate\", description=\"Feature drift and schedule based retrain signal.\")\n",
    "def api_retrain_check():\n",
    "    fn = globals().get(\"retrain_check\", None)\n",
    "    if not fn:\n",
    "        return {\"should_retrain\": True, \"reason\": \"unknown (no fn)\", \"drift\": None}\n",
    "    out = fn()\n",
    "    log_event(\"retrain_check\", out)\n",
    "    return out\n",
    "@policy_router.get(\"/snapshot\", response_model=SnapshotResponse, summary=\"Incident snapshot\", description=\"One-page JSON + rendered note.\")\n",
    "def api_snapshot():\n",
    "    note_fn = globals().get(\"build_analyst_note_v2\", None)\n",
    "    rep_fn  = globals().get(\"nightly_report\", None)\n",
    "    note = note_fn() if note_fn else {\"ok\": False, \"note\": \"(missing)\"}\n",
    "    rep  = rep_fn()  if rep_fn  else None\n",
    "    log_event(\"snapshot\", {\"note_ok\": bool(note.get(\"ok\", False))})\n",
    "    return {\"note\": note, \"report\": rep}\n",
    "@ops_router.get(\"/healthz\", response_model=Healthz, include_in_schema=False)\n",
    "def healthz():\n",
    "    return {\"ok\": True, \"run_id\": RUN_ID}\n",
    "@ops_router.get(\"/readyz\", response_model=Readyz, include_in_schema=False)\n",
    "def readyz():\n",
    "    rpc_ok  = _rpc_ready()\n",
    "    csv_ok  = _csv_write_ready()\n",
    "    mdl_ok  = _models_ready()\n",
    "    ok = rpc_ok and csv_ok\n",
    "    return {\"ok\": ok, \"rpc_ok\": rpc_ok, \"csv_write_ok\": csv_ok, \"models_ok\": mdl_ok,\n",
    "            \"detail\": {\"live_csv\": str(LIVE_CSV), \"model_dir\": str(OUT_MODEL)}}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "id": "a38ab03b-9004-4daf-ba65-de1848c3108c",
   "metadata": {},
   "outputs": [],
   "source": [
    "app = globals().get(\"app\")\n",
    "if not isinstance(app, FastAPI):\n",
    "    app = FastAPI(title=\"Depeg Sentinel MCP\", version=globals().get(\"APP_VERSION\", \"3.0.0\"))\n",
    "app.include_router(ml_router)\n",
    "app.include_router(policy_router)\n",
    "app.include_router(ops_router)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "id": "2b2b3735-24dc-49c8-af64-768fbec91c73",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _patch_train_hooks():\n",
    "    import inspect\n",
    "    from functools import wraps\n",
    "    if \"train_forecaster_10m\" in globals():\n",
    "        orig = globals()[\"train_forecaster_10m\"]\n",
    "        if not getattr(orig, \"_wrapped_for_registry\", False):\n",
    "            @wraps(orig)\n",
    "            def wrapped(*args, **kwargs):\n",
    "                res = orig(*args, **kwargs)\n",
    "                ap = None; brier = None\n",
    "                try:\n",
    "                    pass\n",
    "                except Exception:\n",
    "                    pass\n",
    "                feat_cols = kwargs.get(\"feature_cols\") or []\n",
    "                _maybe_register_after_train_10m(feat_cols, ap, brier)\n",
    "                return res\n",
    "            wrapped._wrapped_for_registry = True\n",
    "            globals()[\"train_forecaster_10m\"] = wrapped\n",
    "    if \"train_forecaster_30m\" in globals():\n",
    "        orig = globals()[\"train_forecaster_30m\"]\n",
    "        if not getattr(orig, \"_wrapped_for_registry\", False):\n",
    "            @wraps(orig)\n",
    "            def wrapped(*args, **kwargs):\n",
    "                res = orig(*args, **kwargs)\n",
    "                ap = None; brier = None\n",
    "                feat_cols = kwargs.get(\"feature_cols\") or []\n",
    "                _maybe_register_after_train_30m(feat_cols, ap, brier)\n",
    "                return res\n",
    "            wrapped._wrapped_for_registry = True\n",
    "            globals()[\"train_forecaster_30m\"] = wrapped\n",
    "_patch_train_hooks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "19ed8396-fd76-42ba-8319-3848564b72a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cron_daily_housekeeping():\n",
    "    try:\n",
    "        daily_zip_archives(retain_days_zip=30, retain_days_raw=14)\n",
    "    except Exception as e:\n",
    "        log_event(\"archive_error\", {\"err\": str(e)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "id": "3d36861f-dc0f-42f8-a4d4-f650856ca325",
   "metadata": {},
   "outputs": [],
   "source": [
    "import hmac, hashlib, time, json, requests\n",
    "API = \"https://sentinel.example.com\"\n",
    "API_KEY = \"...\"\n",
    "HMAC_SECRET = b\"...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "id": "b416c89d-53ba-482f-a0b7-6760db876592",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth_headers(method, path, body_obj=None):\n",
    "    ts = str(int(time.time()))\n",
    "    body = json.dumps(body_obj) if body_obj is not None else \"\"\n",
    "    msg = f\"{ts}.{method.upper()}.{path}.{body}\".encode()\n",
    "    sig = hmac.new(HMAC_SECRET, msg, hashlib.sha256).hexdigest()\n",
    "    return {\n",
    "        \"X-API-Key\": API_KEY,\n",
    "        \"X-Timestamp\": ts,\n",
    "        \"X-Signature\": sig,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "760381c4-4f3b-4c63-a704-723eb8c333a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, json, hmac, hashlib, requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "49d3f908-26fc-4ebd-b384-61896f934084",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"http://localhost:8000\"\n",
    "API_KEY = \"YOUR_KEY\"\n",
    "HMAC_SECRET = b\"YOUR_SECRET\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "e71ec24a-4721-402a-b0a9-c5cddd1ad9e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def auth_headers_simple(method, path, body_obj=None):\n",
    "    ts = str(int(time.time()))\n",
    "    body = json.dumps(body_obj) if body_obj is not None else \"\"\n",
    "    msg  = f\"{ts}.{body}\".encode()  \n",
    "    sig  = hmac.new(HMAC_SECRET, msg, hashlib.sha256).hexdigest()\n",
    "    return {\n",
    "        \"X-API-Key\": API_KEY,\n",
    "        \"X-Timestamp\": ts,\n",
    "        \"X-Signature\": sig,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "id": "dfe9552f-a4e3-4d18-a540-77b6f141c54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading, uvicorn\n",
    "from fastapi import FastAPI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "id": "4d5e657b-0d48-4625-ac1d-7a6bd52becde",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:     Started server process [17392]\n",
      "INFO:     Waiting for application startup.\n",
      "INFO:     Application startup complete.\n",
      "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:     127.0.0.1:54132 - \"GET /healthz HTTP/1.1\" 404 Not Found\n",
      "INFO:     127.0.0.1:54133 - \"GET /ml/score_zoo HTTP/1.1\" 200 OK\n"
     ]
    }
   ],
   "source": [
    "if 'app' not in globals() or not isinstance(app, FastAPI):\n",
    "    app = FastAPI(title=\"Depeg Sentinel MCP (minimal)\")\n",
    "def _run():\n",
    "    uvicorn.run(app, host=\"127.0.0.1\", port=8000, log_level=\"info\")\n",
    "threading.Thread(target=_run, daemon=True).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "236c8e64-463b-4728-af6d-d17bd9095f1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests, time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "3dc15155-b37b-4195-9173-649194d67769",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"detail\":\"Not Found\"}\n"
     ]
    }
   ],
   "source": [
    "time.sleep(1)  \n",
    "print(requests.get(\"http://127.0.0.1:8000/healthz\").text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "6d4163ef-22b6-4255-958a-cf444f3d524c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time, json, hmac, hashlib, requests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 245,
   "id": "b5479ac5-027d-4fd9-8951-0a573ec71b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = \"http://127.0.0.1:8000\"\n",
    "API_KEY = \"YOUR_KEY\"\n",
    "HMAC_SECRET = b\"YOUR_SECRET\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "id": "1e35bf57-3809-44e3-879a-5b6b8586baf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[zoo] anomaly scores (incl. LSTM AE) updated\n",
      "200 [{\"ts\":\"2025-08-28T04:03:30+00:00\",\"pool\":\"USDC/USDT_univ3\",\"z_if\":0.14128078343613304,\"z_lof\":0.03236953434238083,\"z_ocsvm\":0.152632505976585,\"z_cusum\":1.0,\"z_ae\":0.005321890581399202,\"z_ae_seq\":0.7132954001426697,\"anom_fused\":1.0},{\"ts\":\"2025-08-28T20:49:13+00:00\",\"pool\":\"USDC/USDT_univ3\",\"z_if\":0.06454996046366299,\"z_lof\":0.03936779859707906,\"z_ocsvm\":0.06844311549174299,\"z_cusum\":1.0,\"z_ae\":0.002712093060836196,\"z_ae_seq\":0.7167590260505676,\"anom_fused\":1.0},{\"ts\":\"2025-08-28T20:50:37+00:00\",\"pool\":\"USDC/USDT_univ3\",\"z_if\":0.09066860462038757,\"z_lof\":0.1844498835977221,\"z_ocsvm\":0.08473838677880109,\"z_cusum\":1.0,\"z_ae\":0.002930549206212163,\"z_ae_seq\":0.7169623374938965,\"anom_fused\":1.0},{\"ts\":\"2025-08-28T21:00:23+00:00\",\"pool\":\"USDC/USDT_univ3\",\"z_if\":0.05785105162450014,\"z_lof\":0.015703511928616937,\"z_ocsvm\":0.13273474357616008,\"z_cusum\":1.0,\"z_ae\":0.00470382533967495,\"z_ae_seq\":0.717720091342926,\"anom_fused\":1.0},{\"ts\":\"2025-08-28T21:00:23+00:00\",\"pool\":\"USDC/USDT_univ3\",\"z_if\":0.10472349852518131,\"z_lof\":0.014592759599442581,\"z_ocsvm\":0.14835996312178115,\"z_cusum\":1.0,\"z_ae\":0.005924860946834087,\"z_ae_seq\":0.7210389971733093,\"anom_fused\":1.0},{\"ts\":\"2025-08-28T21:00:53+00:00\",\"pool\":\"USDC/USDT_univ3\",\"z_if\":0.07070108656607928,\"z_lof\":0.01780174485154004,\"z_ocsvm\":0.14106258140948114,\"z_cusum\":1.0,\"z_ae\":0.005202544387429953,\"z_ae_seq\":0.7228586077690125,\"anom_fused\":1.0},{\"ts\":\"2025-08-28T21:01:39+00:00\",\"pool\":\"USDC/USDT_univ3\",\"z_if\":0.18934423711125356,\"z_lof\":0.015667829703763222,\"z_ocsvm\":0.16685984181844557,\"z_cusum\":1.0,\"z_ae\":0.006387931760400534,\"z_ae_seq\":0.718090832233429,\"anom_fused\":1.0},{\"ts\":\"2025-08-28T21:02:23+00:00\",\"pool\":\"USDC/USDT_univ3\",\"z_if\":0.07069063813191052,\"z_lof\":0.02053880563935266,\"z_ocsvm\":0.14160241264045414,\"z_cusum\":1.0,\"z_ae\":0.00426914868876338,\"z_ae_seq\":0.7162013053894043,\"anom_fused\":1.0},{\"ts\":\"2025-08-28T21:03:53+00:00\",\"pool\":\"USDC/USDT_univ3\",\"z_if\":0.0503912796575585,\"z_lof\":0.002164393842730273,\"z_ocsvm\":0.1340320650990907,\"z_cusum\":1.0,\"z_ae\":0.004277918487787247,\"z_ae_seq\":0.7142027616500854,\"anom_fused\":1.0},{\"ts\":\"2025-08-28T21:07:21+00:00\",\"pool\":\"USDC/USDT_univ3\",\"z_if\":0.1378530228410214,\"z_lof\":0.06270767080769435,\"z_ocsvm\":0.16190572206045775,\"z_cusum\":1.0,\"z_ae\":0.006651081144809723,\"z_ae_seq\":0.7202730774879456,\"anom_fused\":1.0}]\n"
     ]
    }
   ],
   "source": [
    "def auth_headers(method, path, body_obj=None):\n",
    "    ts = str(int(time.time()))\n",
    "    body = json.dumps(body_obj) if body_obj is not None else \"\"\n",
    "    msg  = f\"{ts}.{body}\".encode()      \n",
    "    sig  = hmac.new(HMAC_SECRET, msg, hashlib.sha256).hexdigest()\n",
    "    return {\"X-API-Key\": API_KEY, \"X-Timestamp\": ts, \"X-Signature\": sig, \"Content-Type\": \"application/json\"}\n",
    "r = requests.get(f\"{BASE}/ml/score_zoo\", headers=auth_headers(\"GET\", \"/ml/score_zoo\"))\n",
    "print(r.status_code, r.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "91822433-729b-4de2-9aa4-2a5ed4b1a251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, json, time, hmac, hashlib, threading\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timezone, timedelta\n",
    "from typing import Any, Dict, List, Sequence, Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "b23bfcea-6a70-4683-a8c8-0000bb2054e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _eval_forecaster_metrics(\n",
    "    horizon: int,\n",
    "    feature_cols: Sequence[str] | None = None,\n",
    "    label_col: str | None = None,\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Re-evaluate the active forecaster on the 30% holdout for AP & Brier.\n",
    "    Uses current live CSV + current loaded (calibrated) model.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        import numpy as np\n",
    "        import pandas as pd\n",
    "        from sklearn.metrics import average_precision_score, brier_score_loss\n",
    "    except Exception:\n",
    "        return {\"AP\": float(\"nan\"), \"Brier\": float(\"nan\")}\n",
    "    df = load_live()\n",
    "    if df is None or df.empty:\n",
    "        return {\"AP\": float(\"nan\"), \"Brier\": float(\"nan\")}\n",
    "    if feature_cols is None:\n",
    "        if \" _default_feats_tabular\" in globals():  \n",
    "            feature_cols = globals()[\"_default_feats_tabular\"]()\n",
    "        else:\n",
    "            feature_cols = [\n",
    "                \"dev\",\"dev_roll_std\",\"tvl_outflow_rate\",\"spot_twap_gap_bps\",\"oracle_ratio\",\n",
    "                \"anom_fused\",\"r0_delta\",\"r1_delta\",\"event_severity_max_24h\",\"event_count_24h\",\n",
    "                \"neighbor_max_dev\",\"neighbor_avg_anom\",\"lead_lag_best\",\"corr_best\",\n",
    "            ]\n",
    "    if label_col is None:\n",
    "        label_col = \"y_10m\" if horizon == 10 else \"y_30m\"\n",
    "    use_cols = [c for c in feature_cols if c in df.columns]\n",
    "    if not use_cols or label_col not in df.columns:\n",
    "        return {\"AP\": float(\"nan\"), \"Brier\": float(\"nan\")}\n",
    "    X = (df[use_cols]\n",
    "         .replace([float(\"inf\"), float(\"-inf\")], float(\"nan\"))\n",
    "         .fillna(0.0).to_numpy())\n",
    "    y = pd.to_numeric(df[label_col], errors=\"coerce\").fillna(0).astype(int).to_numpy()\n",
    "    ts = pd.to_datetime(df[\"ts\"], utc=True, errors=\"coerce\")\n",
    "    cut = ts.quantile(0.70)\n",
    "    te_mask = ts > cut\n",
    "    if te_mask.sum() < 5 or len(set(y[te_mask])) < 2:\n",
    "        return {\"AP\": float(\"nan\"), \"Brier\": float(\"nan\")}\n",
    "    if horizon == 10:\n",
    "        clf, calib = _load_forecaster()\n",
    "    else:\n",
    "        clf, calib = _load_forecaster_30m()\n",
    "    model = calib if calib is not None else clf\n",
    "    if model is None:\n",
    "        return {\"AP\": float(\"nan\"), \"Brier\": float(\"nan\")}\n",
    "    try:\n",
    "        p = model.predict_proba(X[te_mask])[:, 1]\n",
    "        ap = float(average_precision_score(y[te_mask], p))\n",
    "        bs = float(brier_score_loss(y[te_mask], p))\n",
    "        return {\"AP\": ap, \"Brier\": bs}\n",
    "    except Exception:\n",
    "        return {\"AP\": float(\"nan\"), \"Brier\": float(\"nan\")}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "id": "899a4910-0638-4253-be5a-db873a64fe2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _patch_train_hooks_strict():\n",
    "    import inspect\n",
    "    from functools import wraps\n",
    "    def _feature_cols_from_kwargs(kwargs, default_fn_name=\"_default_feats_tabular\"):\n",
    "        cols = kwargs.get(\"feature_cols\")\n",
    "        if cols:\n",
    "            return list(cols)\n",
    "        dfc = globals().get(default_fn_name)\n",
    "        return list(dfc()) if callable(dfc) else []\n",
    "    if \"train_forecaster_10m\" in globals():\n",
    "        orig = globals()[\"train_forecaster_10m\"]\n",
    "        if not getattr(orig, \"_wrapped_for_registry_v2\", False):\n",
    "            @wraps(orig)\n",
    "            def wrapped(*args, **kwargs):\n",
    "                res = orig(*args, **kwargs)\n",
    "                feat_cols = _feature_cols_from_kwargs(kwargs)\n",
    "                m = _eval_forecaster_metrics(10, feature_cols=feat_cols, label_col=\"y_10m\")\n",
    "                _maybe_register_after_train_10m(feat_cols, m.get(\"AP\"), m.get(\"Brier\"))\n",
    "                return res\n",
    "            wrapped._wrapped_for_registry_v2 = True\n",
    "            globals()[\"train_forecaster_10m\"] = wrapped\n",
    "    if \"train_forecaster_30m\" in globals():\n",
    "        orig = globals()[\"train_forecaster_30m\"]\n",
    "        if not getattr(orig, \"_wrapped_for_registry_v2\", False):\n",
    "            @wraps(orig)\n",
    "            def wrapped(*args, **kwargs):\n",
    "                res = orig(*args, **kwargs)\n",
    "                feat_cols = _feature_cols_from_kwargs(kwargs)\n",
    "                m = _eval_forecaster_metrics(30, feature_cols=feat_cols, label_col=\"y_30m\")\n",
    "                _maybe_register_after_train_30m(feat_cols, m.get(\"AP\"), m.get(\"Brier\"))\n",
    "                return res\n",
    "            wrapped._wrapped_for_registry_v2 = True\n",
    "            globals()[\"train_forecaster_30m\"] = wrapped\n",
    "_patch_train_hooks_strict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "9f16e410-bb59-46a5-b072-a827bbf0d2a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def export_contract_docs(out_dir: Path | None = None) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Writes:\n",
    "      - openapi.json (FastAPI spec)\n",
    "      - contract.md  (handy per-route cheatsheet with auth & cURL)\n",
    "    \"\"\"\n",
    "    out_dir = out_dir or (OUT / \"contract\")\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    spec_path = out_dir / \"openapi.json\"\n",
    "    try:\n",
    "        spec = app.openapi()\n",
    "    except Exception:  \n",
    "        spec = {}\n",
    "    spec_path.write_text(json.dumps(spec, indent=2))\n",
    "    lines = []\n",
    "    lines.append(\"# Depeg Sentinel MCP — Contract Cheatsheet\\n\")\n",
    "    lines.append(\"Auth headers (send all requests with these):\\n\")\n",
    "    lines.append(\"```\\nX-API-Key: <YOUR_KEY>\\nX-Timestamp: <unix-seconds>\\nX-Signature: HMAC_SHA256( SECRET, f\\\"{ts}.{body}\\\" )\\nContent-Type: application/json\\n```\\n\")\n",
    "    def _curl(method: str, path: str, body_obj: Optional[dict] = None):\n",
    "        body = json.dumps(body_obj) if body_obj is not None else \"\"\n",
    "        return \"\\n\".join([\n",
    "            \"```bash\",\n",
    "            \"ts=$(date +%s)\",\n",
    "            f\"sig=$(python - <<'PY'\\nimport hmac,hashlib,os\\nsec=os.environ.get('HMAC_SECRET','secret').encode()\\nbody={body!r}\\nts=os.environ.get('TS_OVERRIDE',str(int(__import__('time').time())))\\nmsg=f\\\"{ '{' }ts{'}' }.{ '{' }body{'}' }\\\".encode()\\nprint(hmac.new(sec,msg,hashlib.sha256).hexdigest())\\nPY\",\n",
    "            \")\",\n",
    "            f\"curl -sS -X {method} \\\"$BASE{path}\\\" \\\\\",\n",
    "            \"  -H \\\"X-API-Key: $API_KEY\\\" \\\\\",\n",
    "            \"  -H \\\"Content-Type: application/json\\\" \\\\\",\n",
    "            \"  -H \\\"X-Timestamp: $ts\\\" \\\\\",\n",
    "            \"  -H \\\"X-Signature: $sig\\\" \\\\\",\n",
    "            (f\"  -d '{body}'\" if body else \"\"),\n",
    "            \"```\",\n",
    "        ])\n",
    "    lines.append(\"## /ml/score_zoo  `GET`\\nReturns parallel anomaly scores.\\n\")\n",
    "    lines.append(_curl(\"GET\", \"/ml/score_zoo\"))\n",
    "    lines.append(\"\\n## /ml/forecast  `GET`\\nReturns 10m/30m risk probabilities for the tail rows.\\n\")\n",
    "    lines.append(_curl(\"GET\", \"/ml/forecast\"))\n",
    "    lines.append(\"\\n## /ml/explain  `GET`\\nTop contributors for the current window.\\n\")\n",
    "    lines.append(_curl(\"GET\", \"/ml/explain\"))\n",
    "    lines.append(\"\\n## /ml/top_events  `GET`\\nHigh-severity recent events.\\n\")\n",
    "    lines.append(_curl(\"GET\", \"/ml/top_events\"))\n",
    "    lines.append(\"\\n## /ml/network  `GET`\\nCross-pool network features and lead/lag.\\n\")\n",
    "    lines.append(_curl(\"GET\", \"/ml/network\"))\n",
    "    lines.append(\"\\n## /policy/decide  `POST`\\nReturns meta-controller policy decision.\\n\")\n",
    "    lines.append(_curl(\"POST\", \"/policy/decide\", {\"feeds_fresh\": True, \"recent_forecasts\": {\"poolA\": 0.62}}))\n",
    "    lines.append(\"\\n## /policy/retrain_check  `GET`\\nSignals when to retrain models.\\n\")\n",
    "    lines.append(_curl(\"GET\", \"/policy/retrain_check\"))\n",
    "    lines.append(\"\\n## /policy/snapshot  `GET`\\nReturns analyst note and nightly report pointers.\\n\")\n",
    "    lines.append(_curl(\"GET\", \"/policy/snapshot\"))\n",
    "    lines.append(\"\\n## /ops/healthz  `GET`\\nProcess heartbeat.\\n\")\n",
    "    lines.append(_curl(\"GET\", \"/ops/healthz\"))\n",
    "    lines.append(\"\\n## /ops/readyz  `GET`\\nDependency readiness (RPC, CSV write, models on disk).\\n\")\n",
    "    lines.append(_curl(\"GET\", \"/ops/readyz\"))\n",
    "    md_path = out_dir / \"contract.md\"\n",
    "    md_path.write_text(\"\\n\".join(lines))\n",
    "    log_event(\"contract_export\", {\"json\": str(spec_path), \"md\": str(md_path)})\n",
    "    return {\"json\": str(spec_path), \"md\": str(md_path)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "id": "58fbcdfd-3c4f-4522-b0a5-4d3979349f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastapi.responses import JSONResponse, PlainTextResponse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "id": "e6f4a19e-02a6-48ba-ac63-6de9f5906f9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "@app.get(\"/ops/contract.json\", include_in_schema=False)\n",
    "def ops_contract_json():\n",
    "    try:\n",
    "        p = (OUT / \"contract\" / \"openapi.json\")\n",
    "        if not p.exists():\n",
    "            export_contract_docs()\n",
    "        return JSONResponse(json.loads(p.read_text()))\n",
    "    except Exception:\n",
    "        return JSONResponse({})\n",
    "@app.get(\"/ops/contract.md\", response_class=PlainTextResponse, include_in_schema=False)\n",
    "def ops_contract_md():\n",
    "    try:\n",
    "        p = (OUT / \"contract\" / \"contract.md\")\n",
    "        if not p.exists():\n",
    "            export_contract_docs()\n",
    "        return PlainTextResponse(p.read_text())\n",
    "    except Exception:\n",
    "        return PlainTextResponse(\"# contract not available\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "id": "5d9fc4cb-8c04-4ce1-b845-415115784591",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _midnight_utc_in(seconds: int = 5) -> float:\n",
    "    now = datetime.now(timezone.utc)\n",
    "    nxt = (now + timedelta(days=1)).replace(hour=0, minute=0, second=0, microsecond=0)\n",
    "    return max(1.0, (nxt - now).total_seconds() + seconds)\n",
    "def _daily_housekeeping_loop():\n",
    "    while True:\n",
    "        try:\n",
    "            export_contract_docs()             \n",
    "            daily_zip_archives(30, 14)         \n",
    "        except Exception as e:\n",
    "            log_event(\"housekeeping_error\", {\"err\": str(e)})\n",
    "        time.sleep(_midnight_utc_in())\n",
    "def start_housekeeping_daemon():\n",
    "    t = threading.Thread(target=_daily_housekeeping_loop, name=\"housekeeping\", daemon=True)\n",
    "    t.start()\n",
    "    log_event(\"housekeeping_started\", {\"thread\": \"housekeeping\"})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "id": "bc4258e7-b76e-4f52-b1b2-110b9b7f703f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\1319089276.py:13: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  mtime = datetime.utcfromtimestamp(p.stat().st_mtime)\n",
      "C:\\Users\\aniru\\AppData\\Local\\Temp\\ipykernel_17392\\1319089276.py:37: DeprecationWarning: datetime.datetime.utcfromtimestamp() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.fromtimestamp(timestamp, datetime.UTC).\n",
      "  mtime = datetime.utcfromtimestamp(p.stat().st_mtime)\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    start_housekeeping_daemon()\n",
    "except Exception as e:\n",
    "    log_event(\"housekeeping_start_error\", {\"err\": str(e)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "id": "7f61b1c8-eea2-4039-a887-9ffca0eadc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_auth_headers(body_obj: Optional[dict] = None, api_key: Optional[str] = None, secret: Optional[str] = None) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Standardize to the server-side rule in _auth_guard:\n",
    "      signature = HMAC_SHA256(SECRET, f\"{ts}.{body_json}\")\n",
    "    \"\"\"\n",
    "    ts = str(int(time.time()))\n",
    "    body = json.dumps(body_obj) if body_obj is not None else \"\"\n",
    "    sec = (secret or os.getenv(\"HMAC_SECRET\", \"secret\")).encode()\n",
    "    sig = hmac.new(sec, f\"{ts}.{body}\".encode(), hashlib.sha256).hexdigest()\n",
    "    return {\n",
    "        \"X-API-Key\": api_key or os.getenv(\"API_KEY\", \"\"),\n",
    "        \"X-Timestamp\": ts,\n",
    "        \"X-Signature\": sig,\n",
    "        \"Content-Type\": \"application/json\",\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "id": "15cd80a0-02de-435d-acea-7a6164eb5795",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    export_contract_docs()\n",
    "except Exception as e:\n",
    "    log_event(\"contract_export_error\", {\"err\": str(e)})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "id": "d788f877-1779-433e-b915-96a458d2a03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, re, json\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "id": "320a4c8e-9350-4a7c-b5eb-1db78e3a5bcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "ENV_PATH = Path(\".env\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "id": "7b738e4e-3204-4d53-88fb-b46aab4afa72",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dotenv_if_exists():\n",
    "    \"\"\"Minimal .env loader that does NOT overwrite already-set env vars.\"\"\"\n",
    "    if ENV_PATH.exists():\n",
    "        for line in ENV_PATH.read_text(encoding=\"utf-8\").splitlines():\n",
    "            line = line.strip()\n",
    "            if not line or line.startswith(\"#\") or \"=\" not in line:\n",
    "                continue\n",
    "            k, v = line.split(\"=\", 1)\n",
    "            k = k.strip()\n",
    "            v = v.strip().strip('\"').strip(\"'\")\n",
    "            os.environ.setdefault(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "id": "00714a24-2b8f-4ba9-95a2-7becb19c2284",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECRET_PLACEHOLDERS = {\"\", \"...\", \"YOUR_KEY\", \"YOUR_SECRET\", \"changeme\", \"<redacted>\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "id": "b93066bf-b70e-4b51-ad6f-296302e824cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "SECRET_ENV_KEYS = {\n",
    "    \"HF_TOKEN\", \"HUGGINGFACE_HUB_TOKEN\", \"OPENAI_API_KEY\", \"ANTHROPIC_API_KEY\",\n",
    "    \"SLACK_BOT_TOKEN\", \"DISCORD_BOT_TOKEN\", \"TELEGRAM_BOT_TOKEN\",\n",
    "    \"ALERT_WEBHOOK\", \"SLACK_WEBHOOK\", \"DISCORD_WEBHOOK\",\n",
    "    \"API_KEY\", \"HMAC_SECRET\", \"JWT_SECRET\",\n",
    "    \"WEB3_RPC_URL\", \"ETH_RPC_URL\", \"BASE_RPC_URL\", \"ARBITRUM_RPC_URL\",\n",
    "}\n",
    "ALIAS_GLOBAL_KEYS = {\n",
    "    \"WEBHOOK\",  \n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "id": "a0925165-3da6-4a30-8ae4-3eb65ef9d1ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _should_skip_name(name: str) -> bool:\n",
    "    if re.match(r\"^_i\\d*$\", name):\n",
    "        return True\n",
    "    if name.startswith(\"__\") and name.endswith(\"__\"):\n",
    "        return True\n",
    "    return False\n",
    "def _looks_like_secret(value: str) -> bool:\n",
    "    \"\"\"Heuristic patterns for tokens, secrets, webhooks; conservative on purpose.\"\"\"\n",
    "    if not isinstance(value, str) or len(value) < 12:\n",
    "        return False\n",
    "    pats = [\n",
    "        r\"^hf_[A-Za-z0-9]{20,}$\",                 \n",
    "        r\"^sk-[A-Za-z0-9]{16,}\",                  \n",
    "        r\"^xox[baprs]-[A-Za-z0-9-]{20,}\",         \n",
    "        r\"https://hooks\\.slack\\.com/services/.+\", \n",
    "        r\"[A-Za-z0-9_\\-]{24,}\\.[A-Za-z0-9_\\-]{6,}\\.[A-Za-z0-9_\\-]{10,}\",  \n",
    "    ]\n",
    "    return any(re.search(p, value) for p in pats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "id": "f0674e77-488b-4af0-9457-3f748e19bda5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrub_secret(value: str, keep: int = 4) -> str:\n",
    "    \"\"\"Mask secrets for logs; URLs get their netloc & query masked.\"\"\"\n",
    "    if not isinstance(value, str) or not value:\n",
    "        return \"\"\n",
    "    if \"://\" in value:\n",
    "        try:\n",
    "            from urllib.parse import urlparse, parse_qsl, urlencode, urlunparse\n",
    "            u = urlparse(value)\n",
    "            qs = [(k, \"***\") for k, _ in parse_qsl(u.query, keep_blank_values=True)]\n",
    "            u2 = u._replace(netloc=\"***\", query=urlencode(qs))\n",
    "            return urlunparse(u2)\n",
    "        except Exception:\n",
    "            return value.split(\"://\", 1)[0] + \"://***\"\n",
    "    return (value[:keep] + \"…\" + value[-keep:]) if len(value) > keep * 2 else \"***\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "id": "ba129211-0474-4785-8211-70d25564b81d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def require_env(name: str):\n",
    "    v = os.getenv(name, \"\")\n",
    "    if not v or v in SECRET_PLACEHOLDERS:\n",
    "        raise RuntimeError(f\"[secrets] Missing required env var: {name}\")\n",
    "    return v\n",
    "def _env_equals_global(name: str, gval: str) -> bool:\n",
    "    \"\"\"True if global value equals env value (likely set from env, not hard-coded).\"\"\"\n",
    "    envv = os.getenv(name)\n",
    "    return (envv is not None) and (envv == gval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "id": "d8cb5e90-6009-4fee-a9bb-dbef0b6aefdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _is_offender(name: str, gval: str) -> bool:\n",
    "    \"\"\"\n",
    "    Decide if (name -> string value) looks like a hard-coded secret.\n",
    "    Rules:\n",
    "     - Only audit known secret names (SECRET_ENV_KEYS) + ALIAS_GLOBAL_KEYS.\n",
    "     - Ignore placeholders.\n",
    "     - If env has a value AND equals the global => OK (probably sourced from env).\n",
    "     - If env has a value and global differs => offender (shadowing env with literal).\n",
    "     - If env is empty and global is non-empty secret-looking => offender.\n",
    "    \"\"\"\n",
    "    if name not in SECRET_ENV_KEYS and name not in ALIAS_GLOBAL_KEYS:\n",
    "        return False\n",
    "    if not isinstance(gval, str):\n",
    "        return False\n",
    "    if gval in SECRET_PLACEHOLDERS:\n",
    "        return False\n",
    "    env_key = \"ALERT_WEBHOOK\" if name == \"WEBHOOK\" else name\n",
    "    envv = os.getenv(env_key, \"\")\n",
    "    if envv and gval == envv:\n",
    "        return False\n",
    "    if envv and gval and gval != envv:\n",
    "        return True\n",
    "    if not envv and gval and _looks_like_secret(gval):\n",
    "        return True\n",
    "    if not envv and gval and \"://\" not in gval and len(gval) >= 20:\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "id": "b0e5a5de-0467-413d-9d9a-c7f4016e53f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _assert_no_inline_secrets(strict: bool = True):\n",
    "    \"\"\"\n",
    "    Refuse to start if a known secret (by name) is hard-coded in globals.\n",
    "    Avoids scanning arbitrary IPython variables like `_i140`.\n",
    "    \"\"\"\n",
    "    offenders = []\n",
    "    for key in sorted(SECRET_ENV_KEYS | ALIAS_GLOBAL_KEYS):\n",
    "        if _should_skip_name(key):\n",
    "            continue\n",
    "        if key in globals():\n",
    "            try:\n",
    "                val = globals()[key]\n",
    "            except Exception:\n",
    "                continue\n",
    "            if isinstance(val, str) and _is_offender(key, val):\n",
    "                offenders.append(key)\n",
    "    if offenders and strict:\n",
    "        raise RuntimeError(\n",
    "            \"[secrets] Inline secret(s) detected for: \"\n",
    "            + \", \".join(offenders)\n",
    "            + \". Move them to environment variables (e.g., .env) and read via os.getenv().\"\n",
    "        )\n",
    "    elif offenders:\n",
    "        print(\"[secrets][warn] Possible inline secrets:\", offenders)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "id": "564bcefa-c0ca-4735-a8cf-6478fd1a44c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_env_secret(name: str, required: bool = False) -> str | None:\n",
    "    v = os.getenv(name)\n",
    "    if required and not v:\n",
    "        raise RuntimeError(f\"[secrets] Missing required env var: {name}\")\n",
    "    return v"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "id": "78edf3b1-cd68-489f-b127-9f8f2b744406",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    load_dotenv_if_exists()\n",
    "    _assert_no_inline_secrets(strict=True)\n",
    "except Exception as _e:\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "id": "c8d6a725-a687-47a5-a44d-1cfb5c974cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _log_rpc_url():\n",
    "    try:\n",
    "        rpc = os.getenv(\"WEB3_RPC_URL\", \"\")\n",
    "        if rpc:\n",
    "            print(\"[rpc] using:\", scrub_secret(rpc))\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "id": "be555f72-08c4-4925-afb0-4bd86a60b13e",
   "metadata": {},
   "outputs": [],
   "source": [
    "ART_DIR = (OUT / \"artifacts\")\n",
    "ART_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CORR_PNG = ART_DIR / \"corr_sparkline.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "id": "b09c0393-d9db-4198-a04e-5f57915aa524",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _pick_pool_pairs(pools: list[str]) -> tuple[str, str] | None:\n",
    "    \"\"\"Choose a sensible pair for correlation sparkline (USDC/USDT vs DAI/USDC, else first two).\"\"\"\n",
    "    pri = [(\"USDC\", \"USDT\"), (\"DAI\", \"USDC\"), (\"USDT\", \"DAI\")]\n",
    "    def _has(sym, p): return sym in p.upper()\n",
    "    for a, b in pri:\n",
    "        candA = next((p for p in pools if _has(a, p) and _has(\"USDC\", p) or _has(\"USDT\", p)), None)\n",
    "        candB = next((p for p in pools if _has(b, p) and (_has(\"USDC\", p) or _has(\"DAI\", p))), None)\n",
    "        if candA and candB and candA != candB:\n",
    "            return candA, candB\n",
    "    return (pools[0], pools[1]) if len(pools) >= 2 else None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "id": "3216648b-fbda-4e34-bff8-ea47b5c91cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_corr_sparkline(win: int = 60) -> str | None:\n",
    "    \"\"\"\n",
    "    Saves a very small sparkline of rolling correlation between two pools' dev series.\n",
    "    Returns filepath or None.\n",
    "    \"\"\"\n",
    "    import numpy as np, pandas as pd\n",
    "    import matplotlib\n",
    "    matplotlib.use(\"Agg\")\n",
    "    import matplotlib.pyplot as plt\n",
    "    df = load_live()\n",
    "    if df is None or df.empty or \"dev\" not in df.columns:\n",
    "        return None\n",
    "    pools = list(df[\"pool\"].dropna().unique())\n",
    "    if len(pools) < 2:\n",
    "        return None\n",
    "    pair = _pick_pool_pairs(pools)\n",
    "    if not pair:\n",
    "        return None\n",
    "    a, b = pair\n",
    "    da = df[df[\"pool\"] == a].sort_values(\"ts\").tail(win * 4).reset_index(drop=True)\n",
    "    db = df[df[\"pool\"] == b].sort_values(\"ts\").tail(win * 4).reset_index(drop=True)\n",
    "    m = min(len(da), len(db))\n",
    "    if m < win + 5:\n",
    "        return None\n",
    "    xa = pd.to_numeric(da[\"dev\"].tail(m), errors=\"coerce\").fillna(0.0).to_numpy()\n",
    "    xb = pd.to_numeric(db[\"dev\"].tail(m), errors=\"coerce\").fillna(0.0).to_numpy()\n",
    "    corr = []\n",
    "    for i in range(win, m + 1):\n",
    "        s1 = xa[i - win:i]; s2 = xb[i - win:i]\n",
    "        c = np.corrcoef(s1, s2)[0, 1] if (np.std(s1) > 0 and np.std(s2) > 0) else 0.0\n",
    "        corr.append(float(c))\n",
    "    if not corr:\n",
    "        return None\n",
    "    plt.figure(figsize=(6, 1.6))\n",
    "    plt.plot(corr)\n",
    "    plt.axhline(0.4, linestyle=\"--\", linewidth=0.8)\n",
    "    plt.axhline(-0.4, linestyle=\"--\", linewidth=0.8)\n",
    "    plt.yticks([-1, -0.5, 0, 0.5, 1])\n",
    "    plt.title(f\"Rolling corr ({a} vs {b})\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(CORR_PNG, dpi=140)\n",
    "    plt.close()\n",
    "    return str(CORR_PNG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "id": "932902fc-d50c-4df0-b972-e72b4502545d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _band_and_why(risk_now: float, p10: float, p30: float, tail_df) -> tuple[str, str]:\n",
    "    \"\"\"\n",
    "    Returns (band, why_string). 'why' references the factor(s) that set the band.\n",
    "    \"\"\"\n",
    "    reasons = []\n",
    "    if p10 >= 0.60 or p30 >= 0.60 or risk_now >= 0.90:\n",
    "        band = \"High\"\n",
    "        if p10 >= 0.60: reasons.append(f\"p10={p10:.2f}\")\n",
    "        if p30 >= 0.60: reasons.append(f\"p30={p30:.2f}\")\n",
    "        if risk_now >= 0.90: reasons.append(f\"fused={risk_now:.2f}\")\n",
    "    elif p10 >= 0.35 or p30 >= 0.50 or risk_now >= 0.70:\n",
    "        band = \"Medium\"\n",
    "        if p10 >= 0.35: reasons.append(f\"p10={p10:.2f}\")\n",
    "        if p30 >= 0.50: reasons.append(f\"p30={p30:.2f}\")\n",
    "        if risk_now >= 0.70: reasons.append(f\"fused={risk_now:.2f}\")\n",
    "    else:\n",
    "        band = \"Low\"\n",
    "        reasons.append(f\"p10={p10:.2f}\")\n",
    "    sev = None\n",
    "    try:\n",
    "        if \"event_severity_max_24h\" in tail_df.columns:\n",
    "            sev = float(tail_df[\"event_severity_max_24h\"].fillna(0).max())\n",
    "            if sev and sev > 0:\n",
    "                reasons.append(f\"event_sev={int(sev)}\")\n",
    "    except Exception:\n",
    "        pass\n",
    "    return band, f\"band={band} due to \" + \", \".join(reasons[:3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "cca3e82c-21cf-46cf-a58e-9a72fbafb97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _tail_table_lines(n: int = 3) -> list[str]:\n",
    "    df = load_live()\n",
    "    if df is None or df.empty:\n",
    "        return []\n",
    "    try:\n",
    "        scored10 = score_latest_10m(n_tail=min(60, len(df)), write_parquet=False)\n",
    "    except Exception:\n",
    "        scored10 = pd.DataFrame()\n",
    "    try:\n",
    "        scored30 = score_latest_30m(n_tail=min(60, len(df)), write_parquet=False)\n",
    "    except Exception:\n",
    "        scored30 = pd.DataFrame()\n",
    "    tail = df.sort_values(\"ts\").groupby(\"pool\").tail(1)[[\"ts\",\"pool\",\"anom_fused\"]].copy()\n",
    "    if isinstance(scored10, pd.DataFrame) and not scored10.empty:\n",
    "        tail = tail.merge(scored10[[\"ts\",\"pool\",\"risk_forecast_10m\"]], on=[\"ts\",\"pool\"], how=\"left\")\n",
    "    if isinstance(scored30, pd.DataFrame) and not scored30.empty:\n",
    "        tail = tail.merge(scored30[[\"ts\",\"pool\",\"risk_forecast_30m\"]], on=[\"ts\",\"pool\"], how=\"left\")\n",
    "    def _level(r):\n",
    "        fused = float(r.get(\"anom_fused\", 0.0))\n",
    "        p10 = float(r.get(\"risk_forecast_10m\", 0.0) or 0.0)\n",
    "        p30 = float(r.get(\"risk_forecast_30m\", 0.0) or 0.0)\n",
    "        if fused >= 0.90 or p10 >= 0.60 or p30 >= 0.60: return \"red\"\n",
    "        if fused >= 0.70 or p10 >= 0.35 or p30 >= 0.50: return \"orange\"\n",
    "        return \"yellow\"\n",
    "    rows = []\n",
    "    for _, r in tail.sort_values(\"anom_fused\", ascending=False).head(n).iterrows():\n",
    "        rows.append(f\"{r['pool']} | {float(r['anom_fused']):.2f} | {float(r.get('risk_forecast_10m',0.0)):.2f} | {float(r.get('risk_forecast_30m',0.0)):.2f} | {_level(r)}\")\n",
    "    if not rows:\n",
    "        return []\n",
    "    hdr = \"Pool | Fused | p10 | p30 | Level\"\n",
    "    sep = \"--- | --- | --- | --- | ---\"\n",
    "    return [hdr, sep, *rows]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "id": "a145af47-7727-4acf-8883-8ba987f58c4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_analyst_note_v2() -> dict:\n",
    "    \"\"\"\n",
    "    (Polished) Analyst Note:\n",
    "      - Adds 'why' line (band rationale)\n",
    "      - Only shows propagation cue if |corr| > 0.4 (and always if so)\n",
    "      - Appends tiny tail table\n",
    "      - Ensures sources line if recent events exist\n",
    "      - Saves correlation sparkline PNG\n",
    "    \"\"\"\n",
    "    import numpy as np, pandas as pd\n",
    "    try:\n",
    "        save_corr_sparkline(win=60)\n",
    "    except Exception:\n",
    "        pass\n",
    "    tail_all = load_live().sort_values(\"ts\")\n",
    "    tail = tail_all.groupby(\"pool\").tail(1)\n",
    "    if tail.empty:\n",
    "        return {\"ok\": False, \"reason\": \"no data\"}\n",
    "    top3 = []\n",
    "    try:\n",
    "        if EXPLAIN_JSON.exists():\n",
    "            explain = json.loads(EXPLAIN_JSON.read_text())\n",
    "        else:\n",
    "            explain = explain_forecast_10m()\n",
    "        top3 = (explain or {}).get(\"top_contributors\", [])[:3]\n",
    "    except Exception:\n",
    "        pass\n",
    "    cue = None\n",
    "    corr_val = 0.0\n",
    "    try:\n",
    "        net = compute_network_features()\n",
    "        if isinstance(net, pd.DataFrame) and not net.empty:\n",
    "            nrow = net.sort_values(\"neighbor_avg_anom\", ascending=False).iloc[0]\n",
    "            corr_val = float(nrow.get(\"corr_best\", 0.0))\n",
    "            if abs(corr_val) > 0.4:\n",
    "                lead_str = \"leads\" if nrow[\"lead_lag_best\"] > 0 else (\"lags\" if nrow[\"lead_lag_best\"] < 0 else \"co-moves\")\n",
    "                win = int(globals().get(\"_LAST_NET_WIN\", 60))\n",
    "                ml  = int(globals().get(\"_LAST_MAX_LAG\", 10))\n",
    "                cue = f\"{nrow['pool']} {lead_str} peers (corr={corr_val:.2f}; win={win}, max_lag={ml}).\"\n",
    "    except Exception:\n",
    "        pass\n",
    "    risk_now = float(tail.get(\"anom_fused\", pd.Series([0.0])).max()) if \"anom_fused\" in tail.columns else 0.0\n",
    "    p10 = 0.0; p30 = 0.0\n",
    "    try:\n",
    "        scored10 = score_latest_10m(n_tail=max(1, len(tail)), write_parquet=False)\n",
    "        if isinstance(scored10, pd.DataFrame) and not scored10.empty:\n",
    "            p10 = float(scored10[\"risk_forecast_10m\"].max())\n",
    "            risk_now = float(scored10[\"anom_fused\"].max())\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        scored30 = score_latest_30m(n_tail=max(1, len(tail)), write_parquet=False)\n",
    "        if isinstance(scored30, pd.DataFrame) and not scored30.empty:\n",
    "            p30 = float(scored30[\"risk_forecast_30m\"].max())\n",
    "    except Exception:\n",
    "        pass\n",
    "    band, why = _band_and_why(risk_now, p10, p30, tail_all)\n",
    "    try:\n",
    "        freshness = \"Fresh\" if bool(tail[\"feeds_fresh\"].iloc[-1]) else \"Stale\"\n",
    "    except Exception:\n",
    "        freshness = \"Unknown\"\n",
    "    cites = []\n",
    "    try:\n",
    "        cites = _recent_event_citations(hours=48, top_n=3)\n",
    "    except Exception:\n",
    "        pass\n",
    "    note_lines = [\n",
    "        f\"Fused anomaly now={risk_now:.2f}; 10-min risk={p10:.2f}; 30-min risk={p30:.2f}.\",\n",
    "        f\"Freshness={freshness}. Confidence {band}. {why}.\"\n",
    "    ]\n",
    "    if top3:\n",
    "        note_lines.append(\"Top drivers: \" + \"; \".join(top3))\n",
    "    if cue:\n",
    "        note_lines.append(\"Propagation: \" + cue)\n",
    "    if cites:\n",
    "        note_lines.append(\"Sources: \" + \" | \".join(cites))\n",
    "    note_lines.append(\"Action: monitor in 5m; if risk > 0.60 or 3 consecutive reds, engage mitigation (widen slippage / reroute).\")\n",
    "    note_tail = _tail_table_lines(n=3)\n",
    "    if note_tail:\n",
    "        note_lines.append(\" | \".join(note_tail[0].split(\" | \")))  \n",
    "        for row in note_tail[2:4]:\n",
    "            note_lines.append(row)\n",
    "    note = \" \".join(note_lines)[:900]\n",
    "    try:\n",
    "        pdf_path = export_analyst_note_pdf(\n",
    "            note_text=note,\n",
    "            risk_now=risk_now,\n",
    "            risk_10m=p10,\n",
    "            risk_30m=p30,\n",
    "            contributors=top3,\n",
    "            freshness=freshness,\n",
    "            confidence=band,\n",
    "            out_path=OUT / \"analyst_note.pdf\",\n",
    "            title=\"Analyst Note v2\"\n",
    "        )\n",
    "    except Exception:\n",
    "        pdf_path = OUT / \"analyst_note.txt\"\n",
    "        try: pdf_path.write_text(note)\n",
    "        except Exception: pass\n",
    "    return {\"ok\": True, \"note\": note, \"pdf\": str(pdf_path), \"citations\": cites, \"corr_png\": str(CORR_PNG) if CORR_PNG.exists() else None}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "id": "84d63312-b1ac-4bab-b606-0b024afd8a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _csv_slice_for_incident(n_tail: int = 200) -> Path | None:\n",
    "    import pandas as pd\n",
    "    df = load_live()\n",
    "    if df is None or df.empty:\n",
    "        return None\n",
    "    use_cols = [c for c in [\n",
    "        \"ts\",\"pool\",\"dex_spot\",\"dex_twap\",\"oracle_px\",\"oracle_ratio\",\"dev\",\"dev_roll_std\",\n",
    "        \"tvl_outflow_rate\",\"virtual_price\",\"spot_twap_gap_bps\",\"r0_delta\",\"r1_delta\",\n",
    "        \"neighbor_max_dev\",\"neighbor_avg_anom\",\"lead_lag_best\",\"corr_best\",\n",
    "        \"z_if\",\"z_lof\",\"z_ocsvm\",\"z_cusum\",\"z_ae\",\"z_ae_seq\",\n",
    "        \"anom_fused\",\"risk_forecast_10m\",\"risk_forecast_30m\",\"feeds_fresh\",\"y_10m\",\"y_30m\"\n",
    "    ] if c in df.columns]\n",
    "    sl = df.tail(n_tail)[use_cols].copy()\n",
    "    p = OUT / \"incident_tail.csv\"\n",
    "    try:\n",
    "        sl.to_csv(p, index=False)\n",
    "        return p\n",
    "    except Exception:\n",
    "        return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "id": "58d56c7f-117b-4a91-9bb8-ab4b9d2eac80",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_incident_pack(snapshot_md: Path | str, extra_files: list[str | Path] | None = None) -> Path | None:\n",
    "    \"\"\"\n",
    "    Packs: incident_{ts}.md + analyst_note.pdf + explain*.json + tail CSV (+ artifacts).\n",
    "    Returns path to the ZIP file.\n",
    "    \"\"\"\n",
    "    ts = datetime.now(timezone.utc).strftime(\"%Y%m%dT%H%M%S\")\n",
    "    zpath = OUT / f\"incident_pack_{ts}.zip\"\n",
    "    files = []\n",
    "    snap = Path(snapshot_md)\n",
    "    if snap.exists():\n",
    "        files.append((\"incident.md\", snap))\n",
    "    note_pdf = OUT / \"analyst_note.pdf\"\n",
    "    if note_pdf.exists():\n",
    "        files.append((\"analyst_note.pdf\", note_pdf))\n",
    "    for f in [globals().get(\"EXPLAIN_JSON\", OUT / \"explain_10m.json\"),\n",
    "              globals().get(\"EXPLAIN_30M_JSON\", OUT / \"explain_30m.json\")]:\n",
    "        try:\n",
    "            f = Path(f)\n",
    "            if f.exists():\n",
    "                files.append((f.name, f))\n",
    "        except Exception:\n",
    "            pass\n",
    "    for pth in [OUT / \"artifacts\" / \"calibration_10m.png\",\n",
    "                OUT / \"artifacts\" / \"calibration_30m.png\",\n",
    "                OUT / \"artifacts\" / \"detector_pr_auc.png\",\n",
    "                CORR_PNG]:\n",
    "        if Path(pth).exists():\n",
    "            files.append((Path(pth).name, Path(pth)))\n",
    "    sl = _csv_slice_for_incident(n_tail=200)\n",
    "    if sl:\n",
    "        files.append((Path(sl).name, Path(sl)))\n",
    "    for e in (extra_files or []):\n",
    "        p = Path(e)\n",
    "        if p.exists():\n",
    "            files.append((p.name, p))\n",
    "    if not files:\n",
    "        return None\n",
    "    with zipfile.ZipFile(zpath, \"w\", compression=zipfile.ZIP_DEFLATED) as zf:\n",
    "        for arcname, path in files:\n",
    "            try:\n",
    "                zf.write(path, arcname)\n",
    "            except Exception:\n",
    "                pass\n",
    "    return zpath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "id": "aa5d416f-fa00-4af2-925f-855b4e6ef3b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_all_charts_for_report():\n",
    "    try:\n",
    "        save_all_calibration_artifacts()\n",
    "    except Exception:\n",
    "        pass\n",
    "    try:\n",
    "        save_corr_sparkline(win=60)\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "id": "1fb03f0a-53f9-4d0b-b8ab-2edad2a19ae4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pytest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "id": "93e17abb-5f19-409a-9a3f-8cbf7b9a1ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mk_live(tmp):\n",
    "    out = Path(tmp) / \"outputs\"\n",
    "    out.mkdir(parents=True, exist_ok=True)\n",
    "    live = out / \"live_dataset.csv\"\n",
    "    ts = pd.date_range(\"2025-01-01\", periods=25, freq=\"min\", tz=\"UTC\")\n",
    "    pools = [\"USDC/USDT-uni\", \"DAI/USDC-curve\"]\n",
    "    rows = []\n",
    "    for p in pools:\n",
    "        dev = np.linspace(0, 0.008, len(ts))  \n",
    "        rows += [{\"ts\": t, \"pool\": p, \"dev\": d, \"anam\": 0.0, \"dex_spot\": 1.0, \"feeds_fresh\": True} for t, d in zip(ts, dev)]\n",
    "    df = pd.DataFrame(rows)\n",
    "    df[\"anom_fused\"] = 0.0\n",
    "    df.to_csv(live, index=False)\n",
    "    return out, live\n",
    "@pytest.fixture()\n",
    "def monkey_env(tmp_path, monkeypatch):\n",
    "    out, live = _mk_live(tmp_path)\n",
    "    monkeypatch.setenv(\"MOCK_MODE\", \"1\")\n",
    "    monkeypatch.setenv(\"OUT\", str(out))\n",
    "    monkeypatch.setenv(\"LIVE_CSV\", str(live))\n",
    "    return out, live"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "id": "a3960212-c4a0-475f-ad68-4a757f1d61f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_feature_engineering_smoke(monkey_env):\n",
    "    out, live = monkey_env\n",
    "    import sys\n",
    "    sys.path.append(str(Path(\".\"))) \n",
    "    df = pd.read_csv(live, parse_dates=[\"ts\"])\n",
    "    assert {\"ts\", \"pool\", \"dev\"}.issubset(df.columns)\n",
    "    try:\n",
    "        from sentinel_runtime import compute_network_features  \n",
    "    except Exception:\n",
    "        compute_network_features = None\n",
    "    if compute_network_features:\n",
    "        res = compute_network_features(win=20, max_lag=5)\n",
    "        assert set([\"pool\", \"neighbor_max_dev\", \"neighbor_avg_anom\", \"lead_lag_best\", \"corr_best\"]).issubset(res.columns)\n",
    "def test_anomaly_fusion_monotone(monkey_env):\n",
    "    out, live = monkey_env\n",
    "    df = pd.read_csv(live, parse_dates=[\"ts\"])\n",
    "    df[\"z_if\"] = [0.1, 0.2] * (len(df)//2) + [0.1] * (len(df) % 2)\n",
    "    df[\"z_lof\"] = 0.05\n",
    "    df[\"z_ocsvm\"] = 0.0\n",
    "    df[\"anom_fused\"] = df[[\"z_if\", \"z_lof\", \"z_ocsvm\"]].max(axis=1)\n",
    "    df.to_csv(live, index=False)\n",
    "    assert (df[\"anom_fused\"] >= df[\"z_if\"]).all()\n",
    "    assert (df[\"anom_fused\"] >= df[\"z_lof\"]).all()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "8d9f1fd2-8a2f-4047-a03b-d5dfed08f2e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_labeler_future_window(monkey_env):\n",
    "    out, live = monkey_env\n",
    "    df = pd.read_csv(live, parse_dates=[\"ts\"])\n",
    "    mask = (df[\"pool\"] == \"USDC/USDT-uni\")\n",
    "    idx = df[mask].index[-8]  \n",
    "    df.loc[idx, \"dev\"] = 0.006\n",
    "    df.to_csv(live, index=False)\n",
    "    try:\n",
    "        from sentinel_runtime import ensure_labels_fixed_on_live, load_live\n",
    "        ensure_labels_fixed_on_live(dev_thr=0.005, fused_thr=0.90, h10=10, h30=30, persist_version=False)\n",
    "        lf = load_live()\n",
    "        assert \"y_10m\" in lf.columns\n",
    "\n",
    "        assert lf[lf[\"pool\"] == \"USDC/USDT-uni\"][\"y_10m\"].max() == 1\n",
    "    except Exception:\n",
    "        df = pd.read_csv(live, parse_dates=[\"ts\"])\n",
    "        df = df.sort_values([\"pool\", \"ts\"])\n",
    "        y10 = []\n",
    "        for p, g in df.groupby(\"pool\"):\n",
    "            vals = (g[\"dev\"].abs() >= 0.005).to_numpy().astype(int)\n",
    "            fut = np.zeros_like(vals)\n",
    "            for j in range(len(vals)):\n",
    "                fut[j] = 1 if vals[j+1:j+11].any() else 0\n",
    "            y10 += list(fut)\n",
    "        df[\"y_10m\"] = y10\n",
    "        assert df[df[\"pool\"] == \"USDC/USDT-uni\"][\"y_10m\"].max() == 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75090c30-896d-4749-9f7e-79db0016c269",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
